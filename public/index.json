
[{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"In this article I will go through the process to setup a AI agent\nIntro # In this article I will go through the process to setup a AI agent\nReferences # https://github.com/john-adeojo/graph_websearch_agent ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/aiagent/","section":"Drafts","summary":"","title":"aiagent","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/airflow/","section":"Tags","summary":"","title":"Airflow","type":"tags"},{"content":"In this article i\u0026rsquo;ll describe the procedure to create a Airflow environment running docker\nIntro # In this article i\u0026rsquo;ll describe the procedure to create a Airflow environment running in docker supporting several BigData Operators.\nI\u0026rsquo;ll explain how it works, how to setup and try to document my personal thoughts on the software compared to other solutions.\nIntroduction # Airflow is a python platform to create, schedule and monitor workflows.\n{% blockquote Official Website Definition%}\nUse airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n{% endblockquote %}\nPoC architecture # For this PoC i\u0026rsquo;m considering the following architecture:\n1 docker : Scheduler 1 docker : Worker 1 docker : Flower 1 docker : DB 1 docker : Webserver 1 docker : Metrics exporter For a production environment, it should be taken into account a better HA-Solution.\nDockers Setup # TODO\nLaunch the Environment # TODO\nCreate a initial Workflow # TODO\nOperations # TODO\nTests # TODO\nConclusion # TODO\n","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/airflowpoc/","section":"Drafts","summary":"","title":"Airflow PoC","type":"drafts"},{"content":"TBD\nIntro # In this article we will go through the process to setup an algolia index. Also setup hexo plugin so that each new post be indexed there. In the end implement a python script for search that could be used in a bot.\nWhat is Algolia # Why bother building an Algolia index? That\u0026rsquo;s simple:\nEnhance search relevance: Deliver highly relevant search results tailored to your data and user needs. Optimize search performance: Experience fast and efficient search queries, even with large volumes of data. Personalization: Create personalized search experiences for each user\nAlgolia sets some upper limits to its services to ensure stability and performance for all users.\nYour free allowance on Algolia Build Your first 1,000,000 records are free, and every month you\u0026rsquo;ll receive 10,000 search requests and 10,000 recommend requests. You can learn more about Build\u0026rsquo;s service limits\nhttps://www.algolia.com/pricing/\nUp to 10k search requests / month 1m records included AI features to test\nFree, Free/pay-as-you-go, Annual plans\nAlso supports configuring budgets\nHow it works # You start by defining you Index data. You specify your searchable attributes from your data (eg: title, cast.name) Specify Ranking and sorting (eg: popularity) Search:\nIt supports A/B testing and Personalization\nYou can apply typo tolerance eg: min number of characters in word to accept matches\nLanguage support, ignore-plurals, Synonymous, segmentation and decompounding, special characters. word proximity\nStop words: List of words that should be considered as optional\nPagination configuration, Highlight, snippeting, prefixing Deduplication and Grouping\nReplicas\nAlgolia has one ranking formula per index. Every index has a unique sorting strategy but you can’t change it at query time. This is because pre-sorting during indexing instead of at query time leads to a considerable performance boost.\nThe key use of replica indices is they let you provide different rankings for the same data.\nAlgolia lets you automatically replicate the content of one index (the primary) onto other indices (standard and virtual replicas) and synchronize content changes.\nLogging and Monitoring\nDictionaries\nYou can configure words to be ignored, plurals\nAnalytics\nYou can can an overview of the users that search, total searches, results, geo,\nA/B testing\nYou can create a replica of your index and launch an A/B test and check analytics to understand if the new option provides better results befiore promoting to all users.\nEnhance\nYou can customize the search experience with rules, such as promoting an item or hiding one. Remove words and redirect users to another page\nDynamic Re-ranking allow the results and search conversions to be incorporated on the model continuously\nPersonalization\nIt also allows personalization, which relies com capturing user activity via events that need to be sent.\nRecommendation\nAlgolia provides recommendations based on the data in your index. One needs to choose the type of recommendations and send events using Insights API . You can have:\nComplementary recommendations Alternative recommendations Trending items Trending facets value Looking similar You can also apply rules on trained models Local Setup # Let\u0026rsquo;s create a local Vue app to test the index search.\nnpx create-instantsearch-app@latest instantsearch-app \\ --name \u0026#39;instantsearch-app\u0026#39; \\ --template \u0026#39;Vue InstantSearch with Vue 3\u0026#39; \\ --app-id \u0026#39;APP_ID\u0026#39; \\ --api-key \u0026#39;API_KEY\u0026#39; \\ --index-name \u0026#39;movie\u0026#39; \\ --attributes-to-display \u0026#39;title\u0026#39; \\ --no-interactive \\ --image-attribute \u0026#39;backdrop_path\u0026#39; Replace APP_ID and API_KEY for the values provided upon registering your account.\nYou can also select the start guide and choose other libraries\nThen start the application with\nyarn start And go to http://localhost:3000/\nIntegration in Hexo # Conclusion # TBD\nReferences # https://dashboard.algolia.com https://github.com/thom4parisot/hexo-algolia https://github.com/LouisBarranqueiro/hexo-algoliasearch https://www.algolia.com/developers/code-exchange/integrate-hexo-with-algolia/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/algolia/","section":"Drafts","summary":"","title":"algolia","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/analytics/","section":"Tags","summary":"","title":"Analytics","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/bigdata/","section":"Tags","summary":"","title":"BigData","type":"tags"},{"content":"TBD\nIntro # TBD\nReferences # https://buildpacks.io/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/buildpacks/","section":"Drafts","summary":"","title":"buildpacks","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/category/","section":"Tags","summary":"","title":"Category","type":"tags"},{"content":"TBD\nIntro # What is a Task Queue # Task queues are used as a mechanism to distribute work across threads or machines.\nA task queue’s input is a unit of work called a task. Dedicated worker processes constantly monitor task queues for new work to perform.\nCelery communicates via messages, usually using a broker to mediate between clients and workers. To initiate a task the client adds a message to the queue, the broker then delivers that message to a worker.\nA Celery system can consist of multiple workers and brokers, giving way to high availability and horizontal scaling.\nSupports # Brokers # RabbitMQ Redis Amazon SQS Concurrency # prefork (multiprocessing) Eventlet, gevent thread (multithreaded) solo (single threaded) Result Stores # AMQP, Redis Memcached, SQLAlchemy, Django ORM Apache Cassandra, Elasticsearch, Riak MongoDB, CouchDB, Couchbase, ArangoDB Amazon DynamoDB, Amazon S3 Microsoft Azure Block Blob, Microsoft Azure Cosmos DB Google Cloud Storage File system Serialization # pickle, json, yaml, msgpack zlib, bzip2 compression Cryptographic message signing Features # Monitoring # A stream of monitoring events is emitted by workers and is used by built-in and external tools to tell you what your cluster is doing – in real-time.\nScheduling # You can specify the time to run a task in seconds or a datetime, or you can use periodic tasks for recurring events based on a simple interval, or Crontab expressions supporting minute, hour, day of week, day of month, and month of year.\nWork-flows # Simple and complex work-flows can be composed using a set of powerful primitives we call the “canvas”, including grouping, chaining, chunking, and more.\nResource Leak Protection # The --max-tasks-per-child option is used for user tasks leaking resources, like memory or file descriptors, that are simply out of your control.\nTime \u0026amp; Rate Limits # You can control how many tasks can be executed per second/minute/hour, or how long a task can be allowed to run, and this can be set as a default, for a specific worker or individually for each task type.\nUser Components # Each worker component can be customized, and additional components can be defined by the user. The worker is built up using “bootsteps” — a dependency graph enabling fine grained control of the worker’s internals.\nInstall # To install using pip\npip install -U Celery References # https://docs.celeryq.dev/en/stable/getting-started/index.html ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/celery/","section":"Drafts","summary":"","title":"celery","type":"drafts"},{"content":"In this article I will go through the process to quickly setup an API on top of a sqlite DB managed by dbt and integrate a chart frontend component in React\nIntro # In this article I will go through the process to quickly setup an API on top of a sqlite DB managed by dbt and integrate a chart frontend component in React\nRequirements # You will need to setup DBT for the backend process You will need to have sqlite module for DBT. In case you use Ubuntu use the following command to install python3 -m pip install dbt-sqlite You will need to have Node so setup the frontend You will need to have sqlite for the database Project Details # Our database will have information regarding products and it\u0026rsquo;s costs during a given timeline.\nWe will use dbt to prepare the view for our backend We will setup CRUD operations for our API We will setup a Node applications to render some charts Model:\nTable : Products Table : Incidents Table : Costs This are our raw tables.\nOur frontend application will render charts based on selected product ID\nMonthly Costs of Product (BarChart) Costs per Category (Piechart) Monthly Incidents per Products (Line) You can clone the following git repo which holds this project setup at:\nhttp://github.com/myrepo DBT Setup # Lets start by initializing a DBT project and creating.\ndbt init make sure to use sqlite as the database source.\nNext let\u0026rsquo;s add you sqlite configuration on the following file ~/.dbt/profiles.yml\nchartjs: outputs: dev: type: sqlite threads: 1 database: chartjs schema: \u0026#39;main\u0026#39; schemas_and_paths: main: \u0026#39;\u0026lt;path_to_the_project\u0026gt;/chartjs/data/etl.db\u0026#39; schema_directory: \u0026#39;\u0026lt;path_to_the_project\u0026gt;/chartjs/data\u0026#39; Make sure to adjust the path to you database file and run dbt debug to make sure the connection is working correctly. You can execute sqlite data/etl.db to create a blank database\nNow let\u0026rsquo;s create a products table. I\u0026rsquo;ve added a sample seed file that you can use as reference.\ndbt seed --select \u0026#34;products\u0026#34; NOTE: If you get an error similar to AttributeError: 'SQLiteCredentials' object has no attribute 'host' please check this thread on how to fix it.\nYou can validate that you have now data with the following command\nsqlite3 -cmd \u0026#34;select * FROM products\u0026#34; data/etl.db Now let\u0026rsquo;s create a simplified materialized view for our API which would only require:\nIdentifier for our Product Category Cost TimeColumn Create View for our API # Create the following file products/schema.yml with the content\nversion: 2 sources: - name: costs tables: - name: costs And the following script products/costs.sql with the content\n{{ config(materialized=\u0026#39;table\u0026#39;) }} TODO Create Nodejs API with CRUD components # We would use the express server to quickly setup this\nmkdir server cd server npm init It will create a package.json file lets add the required packages\nnpm install express npm install sqlite3 npm install sequelize And create the following file app.js with the content\nconst express = require(\u0026#39;express\u0026#39;); const bodyParser = require(\u0026#39;body-parser\u0026#39;); const { Sequelize, Model, DataTypes } = require(\u0026#39;sequelize\u0026#39;); const app = express(); const port = 3000; // Create Sequelize instance const sequelize = new Sequelize({ dialect: \u0026#39;sqlite\u0026#39;, storage: \u0026#39;../chartjs/data/etl.db\u0026#39; }); // Define User model class User extends Model {} User.init({ name: DataTypes.STRING, email: DataTypes.STRING, password: DataTypes.STRING }, { sequelize, modelName: \u0026#39;user\u0026#39; }); // Sync models with database sequelize.sync(); // Middleware for parsing request body app.use(bodyParser.urlencoded({ extended: false })); app.use(bodyParser.json()); // CRUD routes for User model app.get(\u0026#39;/users\u0026#39;, async (req, res) =\u0026gt; { const users = await User.findAll(); res.json(users); }); app.get(\u0026#39;/users/:id\u0026#39;, async (req, res) =\u0026gt; { const user = await User.findByPk(req.params.id); res.json(user); }); app.post(\u0026#39;/users\u0026#39;, async (req, res) =\u0026gt; { const user = await User.create(req.body); res.json(user); }); app.put(\u0026#39;/users/:id\u0026#39;, async (req, res) =\u0026gt; { const user = await User.findByPk(req.params.id); if (user) { await user.update(req.body); res.json(user); } else { res.status(404).json({ message: \u0026#39;User not found\u0026#39; }); } }); app.delete(\u0026#39;/users/:id\u0026#39;, async (req, res) =\u0026gt; { const user = await User.findByPk(req.params.id); if (user) { await user.destroy(); res.json({ message: \u0026#39;User deleted\u0026#39; }); } else { res.status(404).json({ message: \u0026#39;User not found\u0026#39; }); } }); // Start server app.listen(port, () =\u0026gt; { console.log(`Server listening on port ${port}`); }); One can execute the following command to test\nnode app.js I also provide a sample postman script if you would like to test the several endpoint to the git repo.\nYou can test the API on the following URL http://localhost:3000/\nReferences # https://docs.getdbt.com/docs/core/connect-data-platform/sqlite-setup https://github.com/codeforkjeff/dbt-sqlite/issues/47 ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/chartsapi/","section":"Drafts","summary":"","title":"Charts with Node Express and DBT","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/ci/cd/","section":"Tags","summary":"","title":"CI/CD","type":"tags"},{"content":" Intro # Testing # Conclusion # References # https://github.com/citusdata/citus https://dl.acm.org/doi/pdf/10.1145/3448016.3457551 https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/concepts-columnar ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/citus/","section":"Drafts","summary":"","title":"citus","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/cloudfoundry/","section":"Tags","summary":"","title":"CloudFoundry","type":"tags"},{"content":"TBD\nIntro # Klarojs\nReferences # https://github.com/klaro-org/klaro-js https://klaro.org/docs/tutorials/google_tag_manager https://klaro.org/docs/getting-started ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/consentbanner/","section":"Drafts","summary":"","title":"consentbanner","type":"drafts"},{"content":"TBD\nIntro # TBD\nReferences # https://couchdb.apache.org/fauxton-visual-guide/index.html#getting-started https://docs.couchdb.org/en/stable/intro/index.html ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/couchdb/","section":"Drafts","summary":"","title":"couchdb","type":"drafts"},{"content":"In this article we will go through a quick start guide for Dagstaer an orchestrator designed to maintain data assets.\nIntro # Dagster is an orchestrator that\u0026rsquo;s designed for developing and maintaining data assets, such as tables, data sets, machine learning models, and reports.\nYou declare functions that you want to run and the data assets that those functions produce or update. Dagster then helps you run your functions at the right time and keep your assets up-to-date.\nDagster is designed to be used at every stage of the data development life-cycle, including local development, unit tests, integration tests, staging environments, and production.\nFeatures # Dagster models pipelines in terms of the data assets they produce and consume, which, by default, brings order and observability to your data platform. Assets in Dagster can model data produced by any system, such as dbt models, Snowflake tables, or even CSV files.\nDagster’s asset-centric approach to building data pipelines makes it easy to:\nUnderstand asset production: Everyone can comprehend data lineage and asset relationships without building the pipeline themselves. Verify asset currency: Easily identify reasons for outdated assets, such as upstream data delays or code errors. Diagnose data quality issues: Implement data quality checks in pipelines and receive automatic notifications for issues. Standardize best practices: Software-defined Assets (SDAs) unify data teams, promoting collaboration and best practices like domain-specific languages and continuous integration. Simplify debugging: Debugging tools are asset-specific, allowing quick identification and resolution of issues, with minimal re-execution required Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.\nAdditionally, Dagster is accompanied by a sleek, modern, web-based UI\nInstall # Start by creating a virtual environment\npython -m venv venv source venv/bin/activate Install the required packages\npip install dagster pip install --upgrade pip Create dagster project\ndagster project from-example --example tutorial cd tutorial/ pip install -e \u0026#34;.[dev]\u0026#34; pip install dagster-webserver Startup the service\ndagster dev You can access the Dagster UI on the following URL:\nhttp://127.0.0.1:3000 Topics # Writing your first asset Build an asset graph Scheduling External services Data Quality Partition Testing Adding new Python dependencies Env variables and secrets unit testing References # https://docs.dagster.io https://dagster.io/community https://github.com/dagster-io/dagster https://docs.dagster.io/getting-started/quickstart https://courses.dagster.io/ https://docs.dagster.io/tutorial https://docs.dagster.io/concepts https://docs.dagster.io/guides ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/dagster/","section":"Drafts","summary":"","title":"dagster","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/dashboards/","section":"Tags","summary":"","title":"Dashboards","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/data/","section":"Tags","summary":"","title":"Data","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"Data Engineering","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/data-enginering/","section":"Tags","summary":"","title":"Data Enginering","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/data-services/","section":"Tags","summary":"","title":"Data Services","type":"tags"},{"content":"TBD\nIntro # TBD\nConclusion # References # https://www.datawars.io/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/datawars/","section":"Drafts","summary":"","title":"datawars","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/dbt/","section":"Tags","summary":"","title":"DBT","type":"tags"},{"content":"TBD\nIntro # In this article I\u0026rsquo;ll be following dbt testing approach with great expectations integration\nReferences # https://docs.getdbt.com/reference/commands/test https://docs.getdbt.com/docs/build/data-tests https://zoltanctoth.medium.com/boost-your-dbt-tests-using-great-expectations-in-dbt-1c2d33d53fb3 ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/dbt-tests/","section":"Drafts","summary":"","title":"dbt-tests","type":"drafts"},{"content":"TBD\nIntro # TBD\nReferences # https://github.com/decaporg/decap-cms https://decapcms.org/docs/examples/ https://github.com/DemoMacro/hexo-boilerplate-netlify-cms ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/decapcms/","section":"Drafts","summary":"","title":"decapcms","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/development/","section":"Tags","summary":"","title":"Development","type":"tags"},{"content":"TBD\nIntro # In this article we are going through the quick start of diesel\nWhat is Diesel # Diesel is a Safe, Extensible ORM and Query Builder for Rust\nORM Definition # Object–relational mapping (ORM, O/RM, and O/R mapping tool) in computer science is a programming technique for converting data between a relational database and the heap of an object-oriented programming language.\nRequirements # References # https://diesel.rs/guides/getting-started https://github.com/diesel-rs/diesel/tree/2.2.x/examples/sqlite/getting_started_step_1 https://diesel.rs https://github.com/diesel-rs/diesel/tree/2.2.x/examples/sqlite/getting_started_step_1 https://blog.logrocket.com/real-time-chat-app-rust-react/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/diesel/","section":"Drafts","summary":"","title":"diesel","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/distributed-systems/","section":"Tags","summary":"","title":"Distributed Systems","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/","section":"Drafts","summary":"","title":"Drafts","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/duckdb/","section":"Tags","summary":"","title":"DuckDB","type":"tags"},{"content":"TBD\nIntro # Short description of the article\nReferences # https://motherduck.com/blog/duckdb-python-e2e-data-engineering-project-part-1/ https://github.com/mehd-io/pypi-duck-flow ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/duckdbprojecteng/","section":"Drafts","summary":"","title":"duckdbprojecteng","type":"drafts"},{"content":"TBD\nIntro # References # https://www.linkedin.com/pulse/enhancing-duckdb-unix-pipe-integration-introducing-shellfs-conover-f0jwe ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/duckdbshellfs/","section":"Drafts","summary":"","title":"duckdbshellfs","type":"drafts"},{"content":"TBD\nIntro # TBD\nReferences # https://www.edgedb.com/blog/edgedb-3-0 ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/edgedb3/","section":"Drafts","summary":"","title":"edgedb","type":"drafts"},{"content":"TBD\nIntro # TBD\nReferences # https://www.youtube.com/watch?v=ML743nrkMHw ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/electron/","section":"Drafts","summary":"","title":"electron","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/etl/","section":"Tags","summary":"","title":"ETL","type":"tags"},{"content":" Example # This is a simple example\n","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/example/","section":"Drafts","summary":"","title":"example","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/feedback/","section":"Tags","summary":"","title":"Feedback","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/frontend/","section":"Tags","summary":"","title":"Frontend","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/frontend-development/","section":"Tags","summary":"","title":"Frontend Development","type":"tags"},{"content":"TBD\nIntro # In this article I will go though how to setup GA4 consent to hexo\nReferences # ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/ga4consent/","section":"Drafts","summary":"","title":"ga4consent","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/gdpr/","section":"Tags","summary":"","title":"GDPR","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/google-analytics/","section":"Tags","summary":"","title":"Google Analytics","type":"tags"},{"content":"TBD\nIntro # In this article we will go through the process of creating 2 APIs one to manage Products and other to manage Inventory and glue with graphbase.\nSetup # Conclusion # References # https://grafbase.com/ https://grafbase.com/docs/config https://github.com/grafbase/grafbase/tree/main/examples ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/graphbase/","section":"Drafts","summary":"","title":"graphbase","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/graphql/","section":"Tags","summary":"","title":"GraphQL","type":"tags"},{"content":" Intro # Hatchet replaces difficult to manage legacy queues or pub/sub systems so you can design durable workloads that recover from failure and solve for problems like concurrency, fairness, and rate limiting\nIn this tutorial, we\u0026rsquo;ll go through the tutorial from hatchet to implement a a RAG chatbot.\nReferences # https://github.com/hatchet-dev/hatchet https://docs.hatchet.run/home/tutorials/fastapi-react/project-setup https://docs.hatchet.run/self-hosting https://docs.hatchet.run/home ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/hatchet/","section":"Drafts","summary":"","title":"hatchet","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/hdfs/","section":"Tags","summary":"","title":"HDFS","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/hive/","section":"Tags","summary":"","title":"Hive","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/ide/","section":"Tags","summary":"","title":"IDE","type":"tags"},{"content":"TBD\nIntro # TBD\nFeatures # TBD\nSetup # TBD\nConclusion # TBD\nReferences # https://github.com/bruin-data/ingestr ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/ingestr/","section":"Drafts","summary":"","title":"ingestr","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/json/","section":"Tags","summary":"","title":"JSON","type":"tags"},{"content":"kind is a tool for running local Kubernetes clusters using Docker container “nodes”. kind was primarily designed for testing Kubernetes itself, but may be used for local development or CI.\nIntro # In this article I will go through the process to setup kind locally and test with CloudFoundry.\nRequirements # Docker go About # Kind consists of Go packages that implement cluster creation, image build, etc. A command line interface Docker images written to run systemd, kubernetes, etc. {% asset_img [diagram.png Kind Design Diagram] %}\nMore information on the design: here\nInstallation # One option would be to install with\ngo install sigs.k8s.io/kind@latest I would recommend however to use you Distribution packaging tool, otherwise you will need to change your PATH variable.\npacseek kind Lets start by creating a cluster\u0026hellip;\nNote: This procedure was done in Arch Linux, you may need to adapt for other distributions\nCluster Creation # Execute the following command kind create cluster and you will get something similar:\nkind create cluster Creating cluster \u0026#34;kind\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.31.2) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Thanks for using kind! 😊 In order to delete the cluster one can use kind delete cluster but let\u0026rsquo;s keep it for now.\nIf you execute docker ps you can check the running containers.\nNow let\u0026rsquo;s install Korifi\u0026hellip;\nKorifi # Korifi was built to serve as a means to deploy and manage applications on Kubernetes while providing automated networking, security, availability, and much more.\nCheck the following page for more info about the project.\nKorifi Requirements # I installed the following packages in my system which where required for\nsudo pacman -S helm cloudfoundry-cli Install # Clone the code from git\ngit clone https://github.com/cloudfoundry/korifi Execute the deploy script\ncd korifi/scripts ./deploy-on-kind.sh korifi Deploy an application # Conclusion # References # https://kind.sigs.k8s.io/ https://github.com/cloudfoundry/korifi/blob/v0.13.0/INSTALL.md https://tutorials.cloudfoundry.org/korifi/local-install https://tutorials.cloudfoundry.org/korifi/overview/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/kind/","section":"Drafts","summary":"","title":"kind","type":"drafts"},{"content":"TBD\nIntro # TBD\nReferences # https://app.koyeb.com ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/koyeb/","section":"Drafts","summary":"","title":"koyeb","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"TBD\nIntro # LangChain is a framework for developing applications powered by large language models (LLMs).\nReferences # https://python.langchain.com/v0.2/docs/introduction ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/langchain/","section":"Drafts","summary":"","title":"langchain","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/leadership/","section":"Tags","summary":"","title":"Leadership","type":"tags"},{"content":"Remember: Millennials have been raised by nurturing Baby Boomers, and they expect coaching in the workplace\n","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/leadingmilleniums/","section":"Drafts","summary":"","title":"leadingmilleniums","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/learning/","section":"Tags","summary":"","title":"Learning","type":"tags"},{"content":"TBD\nIntro # TBD\nReferences # https://www.lightdash.com https://docs.lightdash.com https://github.com/lightdash/lightdash ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/lightdash/","section":"Drafts","summary":"","title":"lightdash","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/linter/","section":"Tags","summary":"","title":"Linter","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"TBD\nIntro # LLMLingua utilizes a compact, well-trained language model (e.g., GPT2-small, LLaMA-7B) to identify and remove non-essential tokens in prompts. This approach enables efficient inference with large language models (LLMs), achieving up to 20x compression with minimal performance loss.\nWhy # Ever encountered the token limit when asking ChatGPT to summarize lengthy texts? Frustrated with ChatGPT forgetting previous instructions after extensive fine-tuning? Experienced high costs using GPT3.5/4 API for experiments despite excellent results? While Large Language Models like ChatGPT and GPT-4 excel in generalization and reasoning, they often face challenges like prompt length limits and prompt-based pricing schemes.\nNow you can use LLMLingua, LongLLMLingua, and LLMLingua-2!\nThese tools offer an efficient solution to compress prompts by up to 20x, enhancing the utility of LLMs.\nCost Savings: Reduces both prompt and generation lengths with minimal overhead. Extended Context Support: Enhances support for longer contexts, mitigates the \u0026ldquo;lost in the middle\u0026rdquo; issue, and boosts overall performance. Robustness: No additional training needed for LLMs. Knowledge Retention: Maintains original prompt information like ICL and reasoning. KV-Cache Compression: Accelerates inference process. Comprehensive Recovery: GPT-4 can recover all key information from compressed prompts. Quick Start # To get started with LLMLingua, simply install it using pip\npip install llmlingua Setup # TBD\nTest # The following demo can be use to test the compression rate for a given prompt and the obtained result.\nhttps://huggingface.co/spaces/microsoft/llmlingua-2 TBD\nConclusion # TBD\nReferences # https://llmlingua.com https://arxiv.org/abs/2310.05736 https://huggingface.co/spaces/microsoft/LLMLingua https://github.com/microsoft/LLMLingua ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/llmlingua/","section":"Drafts","summary":"","title":"llmlingua","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/llms/","section":"Tags","summary":"","title":"LLMs","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/management/","section":"Tags","summary":"","title":"Management","type":"tags"},{"content":"TBD\nIntro # Setup # References # https://www.metabase.com/start/oss/ https://github.com/ClickHouse/metabase-clickhouse-driver https://clickhouse.com/docs/en/integrations/metabase ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/metabase/","section":"Drafts","summary":"","title":"Metabase","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/metrics/","section":"Tags","summary":"","title":"Metrics","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/openai/","section":"Tags","summary":"","title":"OpenAI","type":"tags"},{"content":"TBD\nIntro # What is OpenUI # References # https://www.youtube.com/watch?v=tMWxJuV2BR0 https://github.com/wandb/openui ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/openui/","section":"Drafts","summary":"","title":"OpenUI","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/orchestration/","section":"Tags","summary":"","title":"Orchestration","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/orm/","section":"Tags","summary":"","title":"ORM","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/parsing/","section":"Tags","summary":"","title":"Parsing","type":"tags"},{"content":"TBD\nIntro # References # https://seriot.ch/projects/parsing_json.html ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/parsingjson/","section":"Drafts","summary":"","title":"parsingjson","type":"drafts"},{"content":"TBD\nIntro # References # https://picovoice.ai/blog/cross-browser-local-llm-inference-using-webassembly ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/picollm/","section":"Drafts","summary":"","title":"picollm","type":"drafts"},{"content":"TBD\nIntro # TBD\nReferences # https://play.ht https://docs.play.ht/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/playht/","section":"Drafts","summary":"","title":"play.ht","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/poc/","section":"Tags","summary":"","title":"PoC","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/protocols/","section":"Tags","summary":"","title":"Protocols","type":"tags"},{"content":"TBD\nIntro # References # https://www.timestored.com/qstudio/release-version-3 ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/qstudio/","section":"Drafts","summary":"","title":"qStudio - qDuckDB","type":"drafts"},{"content":"TBD\nIntro # QuestDB is an open-source time-series database for high throughput ingestion and fast SQL queries with operational simplicity.\nAbout # QuestDB is well-suited for financial market data, IoT sensor data, ad-tech and real-time dashboards. It shines for datasets with high cardinality and is a drop-in replacement for InfluxDB via support for the InfluxDB Line Protocol.\nQuestDB implements ANSI SQL with native time-series SQL extensions. These SQL extensions make it simple to filter and downsample data, or correlate data from multiple sources using relational and time-series joins.\nWe achieve high performance by adopting a column-oriented storage model, parallelized vector execution, SIMD instructions, and low-latency techniques. The entire codebase is built from the ground up in Java, C++ and Rust with no dependencies and zero garbage collection.\nQuestDB supports schema-agnostic streaming ingestion using the InfluxDB line protocol and a REST API for bulk imports and exports.\nThe QuestDB SQL Web Console is an interactive SQL editor facilitating CSV import. Finally, QuestDB also includes the Postgres Wire Protocol for programmatic queries.\nPopular tools that integrate with QuestDB include Apache Kafka, Grafana, Superset, Telegraf and Apache Flink.\nWeb Demo # https://demo.questdb.io Quick Start # https://questdb.io/docs/quick-start/\nReferences # https://github.com/questdb/questdb https://questdb.io/docs/guides/create-database/ https://questdb.io/docs/quick-start/ https://questdb.io/blog/questdb-8-release/ \u0026lt;https://github.com/questdb/questdb/releases/tag/8.0.0 https://docs.influxdata.com/telegraf/v1/get-started/ ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/questdb/","section":"Drafts","summary":"","title":"questdb","type":"drafts"},{"content":"TBD\nIntro # Test # References # https://blog.cloudflare.com/the-road-to-quic https://learn.microsoft.com/en-us/dotnet/fundamentals/networking/quic/quic-overview https://rocket.rs/ https://blog.cloudflare.com/enjoy-a-slice-of-quic-and-rust/ https://github.com/cloudflare/quiche ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/quic/","section":"Drafts","summary":"","title":"quic","type":"drafts"},{"content":"TBD\nIntro # In this article i\u0026rsquo;ll explain a quick way to fire up a Spark Cluster in dockers. It could be useful for PoCs other tests you consider.\nThis configuration shouldn\u0026rsquo;t be used in production environments.\nRequirements # For this setup one should have installed:\nDocker Architecture # Setup # Tests # Integration with Zeppelin # ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/quicksparkdocker/","section":"Drafts","summary":"","title":"Quick Firing a Spark Environment in Docker","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/react/","section":"Tags","summary":"","title":"React","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/recommendations/","section":"Tags","summary":"","title":"Recommendations","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/","section":"rramos.github.io","summary":"","title":"rramos.github.io","type":"page"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/rust/","section":"Tags","summary":"","title":"Rust","type":"tags"},{"content":"TBD\nIntro # TBD\nConclusion # TBD\nReferences # https://blog.det.life/delta-and-rust-new-in-town-ebe6dd012da8 https://github.com/delta-io/delta-rs ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/rust-rs/","section":"Drafts","summary":"","title":"rust-rs","type":"drafts"},{"content":"TBD\nIntro # This article summarizes the \u0026ldquo;sandwich approach\u0026rdquo; for giving negative feedback, which involves placing negative feedback between two positive remarks. While widely used, this method can undermine the feedback\u0026rsquo;s effectiveness and damage relationships with direct reports. ref\nAssumptions vs. Reality # Leaders assume combining positive with negative feedback makes it easier to accept, but direct reports often prefer receiving straightforward negative feedback without the \u0026ldquo;sandwich.\u0026rdquo;\nGenuineness and Trust # The sandwich approach can make positive feedback seem insincere and lead direct reports to mistrust both positive and negative feedback.\nBalanced Feedback Fallacy # Leaders think the sandwich approach provides balanced feedback, but this is ineffective. Feedback should be timely and specific.\nDiscomfort and Anxiety # Leaders may use this approach to reduce their discomfort in delivering negative feedback, but it can increase anxiety for both parties.\nBetter Approach # A better method is to use a transparent and mutual learning approach, which involves:\nClearly stating the purpose of the feedback. Describing observed concerns and asking for their perspectives. Engaging in a dialogue to understand the issue together and plan forward actions collaboratively. This method respects direct reports, makes feedback feel genuine, and reduces anxiety, fostering a healthier and more effective feedback culture.\nReference # https://hbr.org/2013/04/the-sandwich-approach-undermin ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/sandwichapproach/","section":"Drafts","summary":"","title":"Sandwich Approach","type":"drafts"},{"content":"TBD\nIntro # Requirements # About the author # Red Marzouk\nhttps://www.youtube.com/watch?v=ncnm3P2Tl28 https://www.firecrawl.dev https://github.com/redamarzouk/Scraping_Agent\nURL -\u0026gt; FireCrawl -\u0026gt; MD -\u0026gt; Data Extraction LLM -\u0026gt; Semi-structured Data -\u0026gt; format and save -\u0026gt; JSON/Excel\nReferences # https://www.youtube.com/watch?v=ncnm3P2Tl28 https://github.com/redamarzouk/Scraping_Agent ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/scrappingagent/","section":"Drafts","summary":"","title":"scrappingAgent","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/search/","section":"Tags","summary":"","title":"Search","type":"tags"},{"content":"TBD\nIntro # In this article I will go through simpleen a markdown translator and steps to incorporate in hexo.\nReferences # https://simpleen.io/blog/translate-markdown-files https://dev.to/ryuya/translate-markdown-to-any-language-github-bot-3pde https://github.com/smikitky/chatgpt-md-translator ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/simpleen/","section":"Drafts","summary":"","title":"simpleen","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/software-engineering/","section":"Tags","summary":"","title":"Software Engineering","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/spark/","section":"Tags","summary":"","title":"Spark","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/springboot/","section":"Tags","summary":"","title":"Springboot","type":"tags"},{"content":"TBD\nIntro # TBD\nReferences # https://spring.io/quickstart https://spring.io/guides/gs/spring-boot https://spring.io/guides/gs/rest-service ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/springboot/","section":"Drafts","summary":"","title":"Springboot Quickstart","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/sql/","section":"Tags","summary":"","title":"SQL","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/sqlite/","section":"Tags","summary":"","title":"SQLite","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/sqoop/","section":"Tags","summary":"","title":"Sqoop","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/testing/","section":"Tags","summary":"","title":"Testing","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/time-series/","section":"Tags","summary":"","title":"Time-Series","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/training/","section":"Tags","summary":"","title":"Training","type":"tags"},{"content":" Excerpt # TBD\nIntro # TBD\nLinter # plugins # VScode # CI/CD # References # http://trunk.io https://trunk.io/linters/yaml/yamllint ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/trunk.io/","section":"Drafts","summary":"","title":"trunk.io","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/ui/","section":"Tags","summary":"","title":"UI","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/utils/","section":"Tags","summary":"","title":"Utils","type":"tags"},{"content":"TBD\nIntro # In this article I will go through a quickstart of Vaadin\nRequirements # QuickStart # Conclusion # References # https://vaadin.com/ https://vaadin.com/docs/latest/guide/quick-start ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/vaadin/","section":"Drafts","summary":"","title":"vaadin","type":"drafts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/websockets/","section":"Tags","summary":"","title":"WebSockets","type":"tags"},{"content":"TBD\nIntro # WrenAI is a text-to-SQL solution for data teams to get results and insights faster by asking business questions without writing SQL.\nProject Status # WrenAI is currently in alpha version. The project team is actively working on progress and aiming to release new versions at least biweekly.\nTesting # Needs more developments to be ready to tested locally.\nReferences # https://github.com/Canner/WrenAI ","date":"14 January 2025","externalUrl":null,"permalink":"/drafts/wrenai/","section":"Drafts","summary":"","title":"WrenAI","type":"drafts"},{"content":"A quick example of how to start using author taxonomies in your articles.\n","date":"14 January 2025","externalUrl":null,"permalink":"/books/","section":"Books Taxonomy Listing Example","summary":"","title":"Books Taxonomy Listing Example","type":"books"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/example/","section":"Tags","summary":"","title":"Example","type":"tags"},{"content":"an example to get you started\nThis is a heading # This is a subheading # This is a subsubheading # This is a subsubsubheading # This is a paragraph with bold and italic text. Check more at Blowfish documentation undefined\n","date":"14 January 2025","externalUrl":null,"permalink":"/books/1736812847619-new-article/","section":"Books Taxonomy Listing Example","summary":"","title":"new-article","type":"books"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/tag/","section":"Tags","summary":"","title":"Tag","type":"tags"},{"content":" Soon # ","date":"13 January 2025","externalUrl":null,"permalink":"/about/","section":"About","summary":"","title":"About","type":"about"},{"content":"","date":"13 January 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"Meltano is a declarative data integration engine,\nIntro # While Meltano is a declarative data integration engine, made for building data-powered features fast, one of the use cases has always been to use Meltano as an ELT platform.\nFeatures # Meltano HUB as more than 600 connectors Largest connector library of any EL tool Modify connectors to your liking In-flight filtering and hashing of PII Detailed pipeline logs and alerting Open source and cloud-agnostic Installation # Setup python environment # Configure a Python 3.11 environment and activate it\npyenv install 3.11 pyenv virtualenv 3.11 meltano export PIP_REQUIRE_VIRTUALENV=true pyenv activate meltano For this option one needs to have pipx.\npipx install meltano You should get something like\ninstalled package meltano 3.4.2, installed using Python 3.12.4 These apps are now globally available - meltano done! ✨ 🌟 ✨ You can check if meltano is working by executing meltano --version and get the version.\nTesting # We\u0026rsquo;re going to take data from a \u0026ldquo;source\u0026rdquo;, namely GitHub, and extract a list of commits to one repository.\nCreate your Meltano Project # We need to initiative the project where we register the plugins and details of the pipelines.\nmeltano init meltano You will get something similar\nCreating .meltano folder created .meltano in /home/rramos/Development/local/meltano-test/meltano/.meltano Creating project files... meltano/ |-- meltano.yml |-- README.md |-- requirements.txt |-- output/.gitignore |-- .gitignore |-- extract/.gitkeep |-- load/.gitkeep |-- transform/.gitkeep |-- analyze/.gitkeep |-- notebook/.gitkeep |-- orchestrate/.gitkeep Creating system database... Done! Your project has been created! Meltano Environments initialized with dev, staging, and prod. To learn more about Environments visit: https://docs.meltano.com/concepts/environments Next steps: cd meltano Visit https://docs.meltano.com/getting-started/part1 to learn where to go from here Add an Extractor # Now lets add a plugin to extract data from Github.\npipx install git+https://github.com/MeltanoLabs/tap-github.git We need to configure the plugin we are going to use the interactive mode\nmeltano config tap-github set --interactive Update the value for the github auth token Update the value of the start_date Update the value of the repositories to consider One can execute the following command to validate the plugin configuration meltano config tap-github.\nSelect Data to Extract # Now that the extractor has been configured we need to select which attributes to consider on the extraction\nmeltano select tap-github commits url meltano select tap-github commits sha meltano select tap-github commits commit_timestamp This will add on meltano.yml the attributes to consider\nDummy Loader # Next lets add a dummy loader to dump the data into JSON\nmeltano add loader target-jsonl --variant=andyh1203 This target requires zero configuration, it just outputs the data into a jsonl file.\nConclusion # My initial impression is that the setup is quite messy and difficult to manage, especially when handling different Python versions. Each plugin may only be supported on a specific version, making it challenging to ensure compatibility across packages. This setup clearly requires some significant improvements.\nReferences # https://meltano.com https://docs.meltano.com/getting-started/installation ","date":"26 December 2024","externalUrl":null,"permalink":"/2024/12/meltano/","section":"Posts","summary":"","title":"meltano","type":"posts"},{"content":" Excerpt # Mako is an extremely fast, production-grade web bundler based on Rust.\nIntro # Extremely fast, Mako is built in Rust, which is known for its performance and safety. Mako takes advantage of data-parallelism and caching features to provide fast and efficient bundling.\nProduction-grade, Mako is already in production use in hundreds of projects at Ant Group. And we have tested it in thousands of projects and it has proven to be reliable and stable.\nEasy to use, Mako is designed to be easy to use out of the box. Start with zero configuration and customize it as needed. Mako is also compatible with existing frameworks like Umi.\nGetting Started # Install via npm bundled with react\nnpm create mako Install and run\ncd mako-project npm install npm run dev Open http://localhost:3000\nDisclaimer # I’m not an expert on frontend stacks, so I can’t definitively recommend this bundler. However, the benchmark results are quite impressive. That said, you might face challenges with the documentation, as much of the development is primarily supported by Chinese developers.\nReferences # https://github.com/umijs/mako https://makojs.dev/docs/ ","date":"25 December 2024","externalUrl":null,"permalink":"/2024/12/mako/","section":"Posts","summary":"","title":"mako","type":"posts"},{"content":"Quick tip if your BTRFS system seems to freeze from time to time\nIntro # In this article I will explain 2 configurations that you might consider if using BTRFS and start noticing some freezing on the system.\nBeahvior # From time to time I started noticing that the system was getting freeze (1-2s) on everything. Mouse, window manager, sound, graphics and was not getting any outputs that justify this behavior for my Arch Linux system.\nAfter googling a bit and getting to some Reddit posts, I managed to find some people that have the same issue, and it seems this behavior was all connected to BTRFS.\nThis two options seemed to work for me, as I also have SSDs\nDisable quotas in BTRFS # It seems quotas are still buggy. As I don\u0026rsquo;t use them I run the following to disable them.\nsudo btrfs quota disable /foo NOTE: execute for all your mount points\nRun fstrim # fstrim is used on a mounted filesystem to discard (or \u0026ldquo;trim\u0026rdquo;) blocks which are not in use by the filesystem. This is useful for solid-state drives (SSDs) and thinly-provisioned storage.\nRunning the following command will trim all mounted filesystems mentioned in /etc/fstab on devices that support the discard operation.\nsudo fstrim -Av NOTE: Running fstrim frequently, or even using mount -o discard, might negatively affect the lifetime of poor-quality SSD devices. For most desktop and server systems a sufficient trimming frequency is once a week.\nThat\u0026rsquo;s it now reboot the computer and you should notice some differences.\nReferences # https://man7.org/linux/man-pages/man8/fstrim.8.html https://www.suse.com/support/kb/doc/?id=000020696 ","date":"3 December 2024","externalUrl":null,"permalink":"/2024/12/btrfs-freeze/","section":"Posts","summary":"","title":"btrfs freeze","type":"posts"},{"content":"","date":"3 December 2024","externalUrl":null,"permalink":"/tags/filesystems/","section":"Tags","summary":"","title":"Filesystems","type":"tags"},{"content":"Quick way to create a patch with git from two different branches\nIntro # In order to get a patch from the diff between master and foo branches one could use the following:\nGenerate the Patch # git diff --no-color --binary master foo \u0026gt; /tmp/patch Apply changes # To later apply the patch:\ngit apply /tmp/patch Changes within the same branch # If you don\u0026rsquo;t have dedicated branches with the changes and all are on the same branch you can just execute\ngit diff --no-color --binary \u0026gt; /tmp/patch NOTE: --no-color ensures the diff is valid to apply if you have automatic coloring on, --binary ensures binary files are handled correctly as well.\nReferences # https://www.karltarvas.com/git-get-a-patch-between-two-branches https://git-scm.com/docs/diff-generate-patch ","date":"1 December 2024","externalUrl":null,"permalink":"/2024/12/create-a-patch-with-git/","section":"Posts","summary":"","title":"Create a patch with git","type":"posts"},{"content":"","date":"1 December 2024","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":"","date":"10 November 2024","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"","date":"10 November 2024","externalUrl":null,"permalink":"/tags/guidelines/","section":"Tags","summary":"","title":"Guidelines","type":"tags"},{"content":"The twelve-factor app is a methodology for building software-as-a-service apps\nIntro # In the modern era, software is commonly delivered as a service: called web apps, or software-as-a-service. The twelve-factor app is a methodology for building software-as-a-service apps that:\nUse declarative formats for setup automation, to minimize time and cost for new developers joining the project; Have a clean contract with the underlying operating system, offering maximum portability between execution environments; Are suitable for deployment on modern cloud platforms, obviating the need for servers and systems administration; Minimize divergence between development and production, enabling continuous deployment for maximum agility; And can scale up without significant changes to tooling, architecture, or development practices. The twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).\nThe Twelve Factors are \u0026hellip;\nI. Codebase # One codebase tracked in revision control, many deploys\nA twelve-factor app is always tracked in a version control system, such as Git, Mercurial, or Subversion. A copy of the revision tracking database is known as a code repository, often shortened to code repo or just repo. There is only one codebase per app, but there will be many deploys of the app. A deploy is a running instance of the app. This is typically a production site, and one or more staging sites.\nII. Dependencies # Explicitly declare and isolate dependencies\nA twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly, via a dependency declaration manifest. Furthermore, it uses a dependency isolation tool during execution to ensure that no implicit dependencies “leak in” from the surrounding system\nIII. Config # Store config in the environment\nThe twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard.\nIV. Backing services # Treat backing services as attached resources\nThe code for a twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. A deploy of the twelve-factor app should be able to swap out a local MySQL database with one managed by a third party (such as Amazon RDS) without any changes to the app’s code.\nV. Build, release, run # Strictly separate build and run stages\nThe twelve-factor app uses strict separation between the build, release, and run stages. For example, it is impossible to make changes to the code at runtime, since there is no way to propagate those changes back to the build stage.\nVI. Processes # Execute the app as one or more stateless processes\nTwelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service, typically a database.\nVII. Port binding # Export services via port binding\nThe twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port.\nVIII. Concurrency # Scale out via the process model\nIn the twelve-factor app, processes are a first class citizen. Processes in the twelve-factor app take strong cues from the unix process model for running service daemons. Using this model, the developer can architect their app to handle diverse workloads by assigning each type of work to a process type. For example, HTTP requests may be handled by a web process, and long-running background tasks handled by a worker process.\nIX. Disposability # Maximize robustness with fast startup and graceful shutdown\nThe twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. This facilitates fast elastic scaling, rapid deployment of code or config changes, and robustness of production deploys.\nX. Dev/prod parity # Keep development, staging, and production as similar as possible\nThe twelve-factor app is designed for continuous deployment by keeping the gap between development and production small.\nXI. Logs # Treat logs as event streams\nA twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior.\nXII. Admin processes # Run admin/management tasks as one-off processes\nA twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior.\nReferences # https://12factor.net/ ","date":"10 November 2024","externalUrl":null,"permalink":"/2024/11/the-12-factor-app/","section":"Posts","summary":"","title":"The 12 Factor App","type":"posts"},{"content":"Bun is an all-in-one JavaScript runtime \u0026amp; toolkit designed for speed, complete with a bundler, test runner, and Node.js-compatible package manager\nIntro # Bun is an all-in-one toolkit for JavaScript and TypeScript apps. It ships as a single executable called bun.\nAt its core is the Bun runtime, a fast JavaScript runtime designed as a drop-in replacement for Node.js. It\u0026rsquo;s written in Zig and powered by JavaScriptCore under the hood, dramatically reducing startup times and memory usage\nInstall # curl -fsSL https://bun.sh/install | bash Using # The bun command-line tool also implements a test runner, script runner, and Node.js-compatible package manager, all significantly faster than existing tools and usable in existing Node.js projects with little to no changes necessary.\nbun run start # run the `start` script bun install \u0026lt;pkg\u0026gt; # install a package bun build ./index.tsx # bundle a project for browsers bun test # run tests bunx cowsay \u0026#39;Hello, world!\u0026#39; # execute a package bun CLI usage # The bun CLI contains a Node.js-compatible package manager designed to be a dramatically faster replacement for npm, yarn, and pnpm. It\u0026rsquo;s a standalone tool that will work in pre-existing Node.js projects.\nRun a JavaScript file or a package.json script:\nbun run path/to/file|script_name Run unit tests:\nbun test Download and install all the packages listed as dependencies in package.json:\nbun install Add a dependency to package.json:\nbun add module_name Remove a dependency from package.json:\nbun remove module_name Create a new Bun project in the current directory:\nbun init Start a REPL (interactive shell):\nbun repl Upgrade Bun to the latest version:\nbun upgrade References # https://bun.sh https://bun.sh/docs https://bun.sh/guides https://bun.sh/guides/ecosystem/vite ","date":"15 October 2024","externalUrl":null,"permalink":"/2024/10/bun/","section":"Posts","summary":"","title":"bun","type":"posts"},{"content":"","date":"15 October 2024","externalUrl":null,"permalink":"/tags/runtime/","section":"Tags","summary":"","title":"Runtime","type":"tags"},{"content":"SmoothMQ is a drop-in replacement for SQS with a much smoother developer experience. It has a functional UI, observability, tracing, message scheduling, and rate-limiting. SmoothMQ lets you run a private SQS instance on any cloud.\nIntro # SmoothMQ is a drop-in replacement for SQS with a much smoother developer experience. It has a functional UI, observability, tracing, message scheduling, and rate-limiting. SmoothMQ lets you run a private SQS instance on any cloud.\nBuilding # Checkout the code from the repo https://github.com/poundifdef/SmoothMQ and run the application with\ngo run . server Client Test # One can use the following repo as an example:\ngit clone https://github.com/rramos/smoothmq-tests.git cd smoothmq-tests And follow the README.md instructions.\nYou should be able to produce some test messages to your local smoothmq instance\nConclusion # In this article, we covered the steps to set up SmoothMQ locally. SmoothMQ is a lightweight, single-application solution written in Go, offering an intuitive UI for managing SQS queues and messages. This tool can be particularly useful for integrating into CI/CD pipelines to facilitate functional testing. For those developing specifically for SQS, SmoothMQ provides a streamlined experience. However, if your application relies on multiple AWS services, you might consider using LocalStack, which offers broader support across the AWS ecosystem.\nReferences # https://github.com/rramos/smoothmq-tests.git https://github.com/poundifdef/SmoothMQ https://github.com/localstack/localstack ","date":"11 October 2024","externalUrl":null,"permalink":"/2024/10/smothmq/","section":"Posts","summary":"","title":"smothmq","type":"posts"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/sqs/","section":"Tags","summary":"","title":"SQS","type":"tags"},{"content":"CAMPS Model presents the 5 factors for great 1-on-1s\nIntro # The 5 factors of great 1-on-1s.\nCertainty: The conversation in the 1-on-1 increased certainty - clarity of goals, success metrics, roles, and plans. Autonomy: The conversation in the 1-on-1 helped you feel more ownership, the ability to make choices, and have a ‘voice’. Meaning: The conversation in the 1-on-1 helped increase your feeling of meaning, knowing the why behind your tasks. Progress: The conversation in the 1-on-1 helped you learn, unblock your progress, and develop your skills and strengths. Social inclusion: The conversation in the 1-on-1 led to greater bonding, fun, and feelings of inclusion. Together, these 5 factors make up what LifeLabs Learning calls CAMPS. You can use CAMPS as a model to know what to add to your 1-on-1s.\nReferences # https://www.lifelabslearning.com/blog/whats-your-camps-score https://www.lifelabslearning.com/blog/camps-your-tried-and-true-tool-for-leading-change-like-a-boss ","date":"23 August 2024","externalUrl":null,"permalink":"/2024/08/camps-model/","section":"Posts","summary":"","title":"CAMPS Model","type":"posts"},{"content":"","date":"4 July 2024","externalUrl":null,"permalink":"/tags/engineering/","section":"Tags","summary":"","title":"Engineering","type":"tags"},{"content":"In this article we are going through the tool harlequin a SQL IDE that runs on the terminal\nIntro # Harlequin is a SQL IDE for DuckDB, SQLite, and other databases that runs in your Terminal\nHow # Harlequin is written in Python, using the Textual framework.\nHarlequin is free and open-source software, licensed to you under the MIT License. You can find the source on GitHub.\nInstall # After installing Python 3.8 or above, install Harlequin using pip or pipx with:\npipx install harlequin You may want to install Harlequin with one or more extras, which provide additional features like additional database support or remote file viewing. That would look like this:\npipx install harlequin[postgres,s3] Check the official documentation to assess the available extensions: https://harlequin.sh/docs/duckdb/extensions\nLoading some examples # Let\u0026rsquo;s clone the examples provided by superset as an example\ngit clone git@github.com:apache-superset/examples-data.git Loading data\nharlequin \u0026#34;local.duckdb\u0026#34; Using with MotherDuck\nharlequin \u0026#34;local.duckdb\u0026#34; \u0026#34;md:\u0026#34; -- Sample query. For more, see docs over at -- https://motherduck.com/docs/sample-data-queries/hacker-news create table airports as from READ_CSV_AUTO(\u0026#39;./airports.csv.gz\u0026#39;); create table bartlines as from READ_JSON_AUTO(\u0026#39;./bart-lines.json.gz\u0026#39;); create table birt_france_data_for_country_map as from READ_CSV_AUTO(\u0026#39;./birth_france_data_for_country_map.csv\u0026#39;); create table birt_names2 as from READ_JSON_AUTO(\u0026#39;./birth_names2.json.gz\u0026#39;); create table birth_names as from READ_JSON_AUTO(\u0026#39;./birth_names.json.gz\u0026#39;); create table countries as from READ_JSON_AUTO(\u0026#39;./countries.json.gz\u0026#39;); create table energy as from READ_JSON_AUTO(\u0026#39;./energy.json.gz\u0026#39;); create table flight_data as from READ_CSV_AUTO(\u0026#39;./flight_data.csv.gz\u0026#39;); create table multiformat_time_series as from READ_JSON_AUTO(\u0026#39;./multiformat_time_series.json.gz\u0026#39;); create table paris_iris as from READ_JSON_AUTO(\u0026#39;./paris_iris.json.gz\u0026#39;); create table random_time_series as from READ_JSON_AUTO(\u0026#39;./random_time_series.json.gz\u0026#39;); create table san_francisco as from READ_CSV_AUTO(\u0026#39;./san_francisco.csv.gz\u0026#39;); create table sf_population as from READ_JSON_AUTO(\u0026#39;./sf_population.json.gz\u0026#39;); create table unicode_utf8_unixnl_test as from READ_CSV_AUTO(\u0026#39;./unicode_utf8_unixnl_test.csv\u0026#39;); Conclusion # Harlequin is a terminal tool created by Ted Conbeer in 2023, designed to facilitate efficient SQL execution across multiple engines. Written in Python, this tool allows users to load files and explore data rapidly. Key features include auto-complete functionality, quick listing of both local and S3 files, and the ability to create tables and perform SQL-based exploration directly from the terminal.\nReferences # https://github.com/tconbeer/harlequin https://harlequin.sh/docs/getting-started ","date":"4 July 2024","externalUrl":null,"permalink":"/2024/07/harlequin/","section":"Posts","summary":"","title":"harlequin","type":"posts"},{"content":"","date":"2 July 2024","externalUrl":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"DNS","type":"tags"},{"content":"TBD\nIntro # dog is an open-source DNS client for the command-line. It has colorful output, supports the DoT and DoH protocols, and can emit JSON.\nFeatures # dog supports the new DNS-over-TLS and DNS-over-HTTPS protocols.\nInstall # It will depend on your distribution for Arch I\u0026rsquo;m using the traditional pacman command.\nsudo pacman -S dog Using # Sending queries # To send DNS queries, just pass in the domain you want to query as a command-line argument.\nBy default, dog will request A records.\ndog example.com Specifying record types # You can request other types of record, such as MX or AAAA, by including an all-caps argument.\ndog example.com MX Specifying nameservers # You can specify which DNS server the request should be sent to by prefixing an argument with the “@” character.\ndog example.com @8.8.4.4 Sending queries over TCP # Using the UDP transport is the default behavior. To send packets over TCP, instead of UDP, pass the -T or --tcp command-line options.\ndog example.com --tcp One downside of using UDP is that its packets are limited to 4,096 bytes. This is not usually a problem, as a normal response will be hundreds of bytes at most. However, large responses, such as those containing multiple DNSSEC keys, will reach the limit and will not be able to be sent over UDP.\nBy default, dog will detect if this is happening — a DNS server will send an error response indicating the payload is too large — and will automatically re-send the query using TCP if it’s too big for UDP.\nDNS-over-TLS # dog supports the new DoT (DNS-over-TLS) transport system. Queries using DoT will be sent over port 853 by default.\ndog example.com --tls @dns.google DNS-over-HTTPS # dog supports the new DoH (DNS-over-HTTPS) transport system. Queries using DoH will be sent over port 443, the standard HTTPS port, by default\ndog example.com --https @https://cloudflare-dns.com/dns-query JSON output # dog can format its output as JSON. If it is being run as part of a script, of if the results are going to be sent to another program, it is preferable to output in a machine-readable language instead of attempting to parse the default output, which is meant to be read by people\ndog example.com A AAAA MX TXT --json Short mode # A common thing to want to do is to get one answer from one query, skipping the TTLs and Additional and Authoritative sections of the response.\ndog example.com --short Conclusion # Most of this functionalities are integrated on dig command, however it provides some nice extended functionality like the json output which is easier to integrate with deployment scripts.\nIf you need to constantly validate infrastructure registers, DNS Caches, or if you manage some web hosting service this is something that could be useful.\nReferences # https://dns.lookup.dog/ https://dns.lookup.dog/reference/command-line-options ","date":"2 July 2024","externalUrl":null,"permalink":"/2024/07/dog/","section":"Posts","summary":"","title":"dog","type":"posts"},{"content":"","date":"2 July 2024","externalUrl":null,"permalink":"/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":"This article is about the six core needs that researchers find important for Humans\nBICEPS # There are six core needs researchers find are important for humans (both at work and in our personal lives). Each of us have a personal hierarchy for the six however: You might find that equity and belonging are most important to you, but choice and status are most important to your employee, your partner, your child. Getting to know them is a shortcut to better communication, as well as greater inclusively at work.\nBelonging Improvement Choice Equality Predictability Significance Belonging # Improvement # Choice # Equality # Predictability # Significance # References # https://www.palomamedina.com/biceps https://static1.squarespace.com/static/59b1c7dee45a7c768c8c3c9f/t/614136c888b0610ebde014ca/1631663816980/_BICEPS+Core+Needs+at+Work+for+Teams.pdf Choach Question Cheat Sheet ","date":"1 July 2024","externalUrl":null,"permalink":"/drafts/biceps/","section":"Drafts","summary":"","title":"biceps","type":"drafts"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/tags/frameworks/","section":"Tags","summary":"","title":"Frameworks","type":"tags"},{"content":"This article is about the setup to run Apache superset locally and use with duckdb datasets.\nIntro # In this article I will go through the process to setup locally superset and work with duckdb datasets.\nSteps # Install and configure a functional Superset instance Create a local or memory duckdb database with sample data Validate the working datasets We will be using the provided datasets from superset examples which are in json and csv and create duckdb tables with those.\nQuickstart # pyenv install 3.7 pyenv virtualenv 3.5.1 venv Docker # If you would like to test with docker version it will brings most of the plugins and would be quicker than to setup the python environment correctly.\nFor that you can follow this steps:\nDownload superset\ngit clone https://github.com/apache/superset Start the docker containers with docker-compose\n# Enter the repository you just cloned $ cd superset # Fire up Superset using Docker Compose $ docker compose up Now head over to http://localhost:8088 and log in with the default created account:\nusername: admin password: admin Local Python Installation # I would like to do some tests with duckdb and the existing docker image does not bring the required python packages for that as such, I went through the hard path.\nFirst let\u0026rsquo;s create a Python environment for 3.9.\npyenv install 3.9 pyenv virtualenv 3.9 superset pyenv activate superset Download superset code\ngit clone git@github.com:apache/superset.git cd superset Install python packages\npip install --upgrade pip pip install -r requirements/base.in pip install duckdb pip install duckdb-engine pip install Pillow Let\u0026rsquo;s configure the default env\nmkdir data export SUPERSET_CONFIG_PATH=\u0026#34;~/.superset/superset_config.py\u0026#34; Generate the superset_config.py file with the following content:\n# Superset specific config ROW_LIMIT = 5000 # Flask App Builder configuration # Your App secret key will be used for securely signing the session cookie # and encrypting sensitive information on the database # Make sure you are changing this key for your deployment with a strong key. # Alternatively you can set it with `SUPERSET_SECRET_KEY` environment variable. # You MUST set this for production environments or the server will refuse # to start and you will see an error in the logs accordingly. SECRET_KEY = \u0026#39;SECRET_KEY\u0026#39; # The SQLAlchemy connection string to your database backend # This connection defines the path to the database that stores your # superset metadata (slices, connections, tables, dashboards, ...). # Note that the connection information to connect to the datasources # you want to explore are managed directly in the web UI # The check_same_thread=false property ensures the sqlite client does not attempt # to enforce single-threaded access, which may be problematic in some edge cases SQLALCHEMY_DATABASE_URI = \u0026#39;sqlite:////PATH_TO_YOUR_SUPERSET/superset/data/superset.db?check_same_thread=false\u0026#39; # Flask-WTF flag for CSRF WTF_CSRF_ENABLED = True # Add endpoints that need to be exempt from CSRF protection WTF_CSRF_EXEMPT_LIST = [] # A CSRF token that expires in 1 year WTF_CSRF_TIME_LIMIT = 60 * 60 * 24 * 365 # Set this API key to enable Mapbox visualizations MAPBOX_API_KEY = \u0026#39;\u0026#39; NOTE: Make sure to replace PATH_TO_YOUR_SUPERSET and SECRET_KEY values. You can generate a new secret with the following command openssl rand -base64 42\nCreate the data folder and start the initialization commands\nmkdir data superset db upgrade superset fab create-admin superset fab create-permissions superset init superset load_examples You can now run superset with the command:\nsuperset run -p 8088 --with-threads --reload Now you just need to access http://localhost:8088/ and start creating you datasets and charts.\nLoading Data # When you run the command superset load_examples it loads example data and several charts and dashboards which allow you to explore the tool.\nYou can also load this data into duckdb to try creating from those datasets\ngit clone git@github.com:apache-superset/examples-data.git And run the following sql after starting duckdb\ncreate table airports as from READ_CSV_AUTO(\u0026#39;./airports.csv.gz\u0026#39;); create table bartlines as from READ_JSON_AUTO(\u0026#39;./bart-lines.json.gz\u0026#39;); create table birt_france_data_for_country_map as from READ_CSV_AUTO(\u0026#39;./birth_france_data_for_country_map.csv\u0026#39;); create table birt_names2 as from READ_JSON_AUTO(\u0026#39;./birth_names2.json.gz\u0026#39;); create table birth_names as from READ_JSON_AUTO(\u0026#39;./birth_names.json.gz\u0026#39;); create table countries as from READ_JSON_AUTO(\u0026#39;./countries.json.gz\u0026#39;); create table energy as from READ_JSON_AUTO(\u0026#39;./energy.json.gz\u0026#39;); create table flight_data as from READ_CSV_AUTO(\u0026#39;./flight_data.csv.gz\u0026#39;); create table multiformat_time_series as from READ_JSON_AUTO(\u0026#39;./multiformat_time_series.json.gz\u0026#39;); create table paris_iris as from READ_JSON_AUTO(\u0026#39;./paris_iris.json.gz\u0026#39;); create table random_time_series as from READ_JSON_AUTO(\u0026#39;./random_time_series.json.gz\u0026#39;); create table san_francisco as from READ_CSV_AUTO(\u0026#39;./san_francisco.csv.gz\u0026#39;); create table sf_population as from READ_JSON_AUTO(\u0026#39;./sf_population.json.gz\u0026#39;); create table unicode_utf8_unixnl_test as from READ_CSV_AUTO(\u0026#39;./unicode_utf8_unixnl_test.csv\u0026#39;); In order to test connection add a new database connection using\nduckdb:///local.duckdb Or if you are using in-memory\nduckdb:///:memory: You can test connectivity going to Settings -\u0026gt; Database Connections\nNext Steps # Deep dive on the functionalities of Superset as the tool seems extremely complete on chart possibilities. Test a with a dataset in DubckDB hybrid mode Conclusion # Superset is an exceptionally powerful tool for Business Intelligence, offering numerous graphical options and configurations, including 3D charts. It is lightweight and fast, with an integrated API and support for various database connections. This article guides you through setting up a local environment, primarily for testing with DuckDB.\nUsing Docker is the most efficient way to start and explore Superset. However, setting it up directly with Python offers greater flexibility for testing other connectors, although managing Python dependencies can be challenging. Superset is designed to be configured for cloud environments using Kubernetes, so these steps are not intended for production environments. For a production setup, refer to the official documentation, which is very comprehensive.\nReferences # https://superset.apache.org/docs/intro https://superset.apache.org/docs/quickstart https://www.restack.io/docs/superset-knowledge-apache-superset-duckdb-integration https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html ","date":"24 June 2024","externalUrl":null,"permalink":"/2024/06/apache-superset/","section":"Posts","summary":"","title":"Apache Superset","type":"posts"},{"content":"","date":"18 June 2024","externalUrl":null,"permalink":"/tags/hexo/","section":"Tags","summary":"","title":"Hexo","type":"tags"},{"content":"In this article I will go through the setup process to auto publish updates from Hexo into twitter\nIntro # In this article I will go through the setup process to auto publish updates from Hexo into twitter\nContext # Due to GDPR regulations I had to make some changes on this blog related with consent as I configured Google Analytics to track visitors of the Website. While researching several hexo plugins and services I introduced several changes that would improve the reach of the articles.\nOne of the ways is to populate my twitter feed when there are new articles created.\nRequirements # This configure requires the usage of hexo . If you don\u0026rsquo;t know is a blogging framework that converts Markdown into html pages, similar to jekyll Also you will need to have a twitter dev account Twitter Dev Account # In order to use the twitter API you need a Dev account, and it seems you need to apply. Check the following URL https://developer.twitter.com and follow the steps in order to have your key.\nIn your project App you should be able to generate the required secrets for:\nTWITTER_CONSUMER_KEY TWITTER_CONSUMER_SECRET TWITTER_ACCESS_TOKEN_KEY TWITTER_ACCESS_TOKEN_SECRET This information you be required later\nHexo Plugin # In order to have Hexo auto-publishing to twitter install the following plugin\nnpm i hexo-twitter-auto-publish You now have 2 options you can define the following configuration on your _config.yml file:\ntwitterAutoPublish: consumerKey: Xegp8XDTMqVxcI2tId1juT70X consumerSecret: fq4eY5NmK2X9ZxSDSUaFqMBPWWMUCCYu35PMvzoqB0YzqLOTEs accessTokenKey: 929842798974656517-VuQxIuoLhtoeqW71LofX6M5fIw8Pf3c accessTokenSecret: R5RZtQj5tLWbSgFx39lq6cd2AcIQRjQk5kbepOobxCplA What this does is to trigger the publishing method after each hexo deploy action or you can execute hexo twitter-publish specifically to publish new articles.\nGithub Actions # In case you use Github Actions to run hexo deploy to deploy your website updates you can setup the following secrets\nTWITTER_CONSUMER_KEY TWITTER_CONSUMER_SECRET TWITTER_ACCESS_TOKEN_KEY TWITTER_ACCESS_TOKEN_SECRET And update your workflow to include\n... env: TWITTER_CONSUMER_KEY: ${{ secrets.TWITTER_CONSUMER_KEY }} TWITTER_CONSUMER_SECRET: ${{ secrets.TWITTER_CONSUMER_SECRET }} TWITTER_ACCESS_TOKEN_KEY: ${{ secrets.TWITTER_ACCESS_TOKEN_KEY }} TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }} But you will have an issue as this plugin stores information of what was deployed on the following file twitter-db.json and each time you run it will take the status that it has on the master branch, so you would end up re-triggering already published tweets.\nIf you use this option I would suggest to disable the plugin for including enable: false and only use the hexo twitter-publish directly remembering that this would require an extra step to commit the twitter-db.json update. (I\u0026rsquo;ll look if there is a better way for this)\nFirst deploy # If you check the twitter-db.json you will understand there are 3 arrays:\\\n*published - contains posts that are already on twitter and each post has a tweetId.\nto-publish - contains all new posts that have not yet appeared on Twitter.\nto-destroy - contains posts that for some reason have been moved to a working version, or we changed the twitterAutoPublish in the page from true to false.\nI would recommend moving the entries to publish in order to prevent publish old articles you have, or reaching the daily limit in twitter.\nTwitter Limitations # Twitter as some limitations for publishing, I will only focus here on the Free Account as there are other tiers with extended functionality and less limitations\nUse-case: For write-only use cases and testing the X API\nRate limited access to v2 post posting and media upload endpoints 1,500 Posts per month - posting limit at the app level 1 app ID Login with X You also have a daily cap so if you try to push lots of articles at once you will probably bump on that limit\nConclusion # In this article we went through the process of setup Hexo with hexo-twitter-auto-publish which allow automatically publish tweets from the new/updated articles. This can run in deploy mode or directly with hexo twitter-publish command. We also touch some limitations that can be found if integrated with Github Actions. This plugin allows one to extend the reach of the publications.\nAfter searching for available plugins in Hexo this was one of the few that where actually maintained, most of then have more than 6 years without updates and few contributions which makes me think if Jekyll would\u0026rsquo; t be a better approach. But that assessment would go for another time\nReferences # https://github.com/studioLaCosaNostra/hexo-twitter-auto-publish https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions ","date":"18 June 2024","externalUrl":null,"permalink":"/2024/06/hexo-twitter/","section":"Posts","summary":"","title":"hexo-twitter","type":"posts"},{"content":"","date":"18 June 2024","externalUrl":null,"permalink":"/tags/twitter/","section":"Tags","summary":"","title":"Twitter","type":"tags"},{"content":"","date":"18 June 2024","externalUrl":null,"permalink":"/tags/looker/","section":"Tags","summary":"","title":"Looker","type":"tags"},{"content":"This article gathers recommended best practices for Looker service\nIntro # This article gathers recommended best practices for Looker service\nBest Practices # Use your best judgment when implementing any of the recommendations shared on this page. This information is a consolidation from the official Google pages on the reference links\nDo\u0026rsquo;s # Do: Define the relationship parameter for all joins # This will ensures that metrics aggregate properly within Looker. By default, Looker will use a many_to_one join relationship for any joins in which a relationship is not defined.\nexplore: view_name { join: view_name_2 { relationship: one_to_one } } More Info: https://cloud.google.com/looker/docs/best-practices/how-to-use-the-relationship-parameter-correctly\nDo: Define a primary key within every view # All views, whether they are derived or coming directly from the database, should contain a primary key. This primary key should be a unique value to enable Looker to uniquely identify any given record. This primary key can be a single column or a concatenation of columns — it simply needs to be a unique identifier for the table or derived table.\nExample:\nview: view_name { dimension: field_name { primary_key: yes } } More Info: https://cloud.google.com/looker/docs/reference/param-field-primary-key\nDo: Name LookML objects in snake_case # The label parameter can be used for additional formatting of a name field, and can also be used to customize the appearance of view names, Explore names, and model names.\nExample:\nmeasure: customer_count_distinct { label: \u0026#34;Number of Customers\u0026#34; type: count_distinct sql: ${customer.id} ;; } Do: Use Datagroups # Use datagroups to align generation of persistent derived tables (PDTs) and Explore caching with underlying ETL processes. Datagroups can also be used to trigger deliveries of dashboards or Looks to ensure that up-to-date data is sent to recipients.\nExample:\ndatagroup: datagroup_name { max_cache_age: \u0026#34;24 hours\u0026#34; sql_trigger: SELECT max(id) FROM my_tablename ;; interval_trigger: \u0026#34;12 hours\u0026#34; label: \u0026#34;desired label\u0026#34; description: \u0026#34;description string\u0026#34; } More info: https://cloud.google.com/looker/docs/reference/param-model-datagroup\nDon\u0026rsquo;t # Don\u0026rsquo;t: Use the from parameter for renaming views within an Explore # The from parameter should primarily be used in the following situations:\nPolymorphic joins (joining the same table multiple times) Self-joins (joining a table to itself) Re-scoping an extended view back to its original view name Use the view_label instead\nexplore: explore_name { view_label: \u0026#34;desired label for the view\u0026#34; } More Info: https://cloud.google.com/looker/docs/reference/param-explore-from\nDon\u0026rsquo;t: Use the word \u0026ldquo;date\u0026rdquo; or \u0026ldquo;time\u0026rdquo; in a dimension group name # Looker appends each time-frame to the end of the dimension group name. This means that a dimension group named created_date results in fields called created_date_date, created_date_month, and so on. Simply use created as the dimension group name, because this results in fields named created_date, created_month, and so on.\nExample:\ndimension_group: since_event { type: duration intervals: [hour, day] sql_start: ${faa_event_date_raw} ;; sql_end: CURRENT_TIMESTAMP();; } More Info: https://cloud.google.com/looker/docs/reference/param-field-dimension-group\nDon\u0026rsquo;t: Use formatted timestamps within joins # Instead, use the raw time-frame option for joining on any date or time fields. This will avoid the inclusion of casting and timezone conversion in join predicates.\nExample:\ndimension_group: created { type: time timeframes: [date, week, month] sql: ${TABLE}.created_at ;; } More Info: https://cloud.google.com/looker/docs/reference/param-field-dimension-group#timeframes\nImprove Experience # LookML developers can consider following these tips to improve their users\u0026rsquo; experience with Looker:\nProvide users with meaningful field names # Use the label parameter to apply user-friendly names to dimensions or measures while maintaining database-friendly names within the view and model files. Make sure to align terminology with a Business Glossary, you may start with Governance tool like marquez\nAvoid exposing multiple fields with the same name. For example, measures of type: count are automatically created within Looker with the name Count. This results in most view files containing a count measure with the same name. Multiple fields with the same name can confuse users.\nProvide clear names for fields of type: yesno. For example, use Was the Item Returned? instead of Returned to name a field that indicates whether an item has been returned.\nName ratios descriptively. For example, Orders Per Purchasing Customers is clearer than *Orders Percent.\nName fields and represent values consistently across the model. Using the value_format or value_format_name parameter to apply formatting such as currency symbols, percentages, and decimal precision to numerical fields will help make everything clearer to your users.\nGroup similar fields together for easier navigation # Use the group_label parameter to consolidate dimensions and measures from individual or multiple joined views that are related. dimension: city { group_label: \u0026#34;Geography\u0026#34; type: string sql: ${TABLE}.city ;; } dimension: country { group_label: \u0026#34;Geography\u0026#34; type: string map_layer_name: countries sql: ${TABLE}.country ;; } More Info: https://cloud.google.com/looker/docs/reference/param-field-group-label\nBreak up large, denormalized tables using the view_label parameter. Utilize the view_label parameter within fields to group fields together logically into separate headings within the field picker. Large, denormalized tables with a lot of fields can be difficult to navigate Avoid exposing too much to users initially # Avoid exposing too much to users upon an initial Looker roll-out. Start small, and then expand the options. You don\u0026rsquo;t have to expose all the tables or dimensions and measures at once. You can expose the most important fields at first and then continue to build in more functionality as business users become more confident with data exploration.\nHide dimensions that are not relevant to users from the user interface. Use the hidden parameter on dimensions that will never be used through the user interface (such as ID fields or database update dates).\nUse the fields parameter within Explores and joins to limit the number of fields that are available to users. Included fields should be only those relevant to the Explore. This reduces bloat and provides a better experience for users.\nHide any Explores that exist solely for populating specific Looks, dashboard tiles, or filters using the hidden parameter for Explores. Explores that are not meant for exploration by users should be hidden from the user interface.\nUse the fewest number of Explores possible while still allowing users to easily get access to the answers they need. Consider splitting out Explores into different models for different audiences to limit the options available for each user group. The optimal number of Explores is different for every business, but having too many Explores tends to confuse users.\nAdd descriptions so users know which fields and Explores to use # Use the description parameter on dimensions and measures to provide additional information to users about the logic or calculations that are used within the model. This is particularly important for dimensions and measures that leverage complex logic or calculations. That being said, it\u0026rsquo;s a good idea to also consider descriptions for simpler fields to be sure that users understand the definitions behind them. dimension: id { primary_key: yes description: \u0026#34;Unique ID for the order\u0026#34; type: string sql: ${TABLE}.id ;; Define Explore descriptions for users. Add a short description to each Explore to specify the purpose of the Explore and the audience who will use it. explore: user { description: \u0026#34;All customer users, staff users are excluded\u0026#34; } Build common workflows into Looker # Add drill_fields to all hierarchical dimensions. For example, adding a drill_field for City into a State dimension will let users select a state and then drill deeper into the cities within that state. Note that this hierarchical drilling will automatically be applied within time dimension groups.\ndimension: country { sql: ${TABLE}.country ;; drill_fields: [state, city] } Add drill_fields to all relevant measures. Drill fields enable users to click into aggregate values in order to access detailed data. Use the set parameter to create reusable sets of fields that can then be applied to any number of measures within a view.\nset: financial_data { fields: [ subtotal, shipping, tax, total, cost, profit ] } view: logistics { dimension: logistics_cost { drill_fields: [financial_data] } } Set up links that enable users to easily navigate and pass filters to other Looker dashboards or to systems or platforms that are external to Looker. See our documentation on the link parameter for examples of passing filters through drills.\ndimension: name { link: { label: \u0026#34;Business Pulse By State Dashboard\u0026#34; url: \u0026#34;https://instance_name.looker.com/dashboards/694?State={{ _filters[\u0026#39;users.state\u0026#39;] | url_encode }}\u0026#34; } } Conclusion # In this article we went through several best practices recommendations described on Looker Official docs, do\u0026rsquo;s and don\u0026rsquo;ts and how to improve user experience.\nNext Steps # Although guidelines and best practices are really useful, I believe introducing testing procedures on your CI/CD process would increase the level of quality for your Looker Models. So a next step from this article would be such implementation\nReferences # https://cloud.google.com/looker/docs/best-practices/best-practices-lookml-dos-and-donts https://cloud.google.com/looker/docs/best-practices/how-to-create-a-positive-experience-for-looker-users ","date":"18 June 2024","externalUrl":null,"permalink":"/2024/06/looker-best-practices/","section":"Posts","summary":"","title":"Looker Best Practices","type":"posts"},{"content":"","date":"18 June 2024","externalUrl":null,"permalink":"/tags/reporting/","section":"Tags","summary":"","title":"Reporting","type":"tags"},{"content":"","date":"18 June 2024","externalUrl":null,"permalink":"/tags/arch/","section":"Tags","summary":"","title":"Arch","type":"tags"},{"content":"","date":"18 June 2024","externalUrl":null,"permalink":"/tags/package-management/","section":"Tags","summary":"","title":"Package Management","type":"tags"},{"content":"This article is about pacseek a terminal user interface for searching and installing Arch Linux packages\nIntro # This article is about pacseek a terminal user interface for searching and installing Arch Linux packages\nContext # One of the things that that bugs me a bit is the fact that when I\u0026rsquo; m trying to install some package in Arch with pacman command and is not present on the repository which makes search for a aur package. If you like to know more about aur packages check the following article.\nIt would be interesting to have a way to search those packages directly from the terminal and and just let your package manager deal with it.\nThis application requires aya which is a AUR helper written and go.\nMake sure to pay Mario Oenning a beer for this work.\nInstallation # sudo pacman -S pacseek yay When you start the command the first time it will create a ~/.config/pacseek/config.json configuration file which you can edit manually or use the settings screen\nUsing # Let\u0026rsquo;s say we would like to install a telegram application and understand which options we may find.\npacseek telegram We would end-up with a similar interface to choose\nIf the package does not exist on the default chaotic-aur repo it downloads from one exiting aur reference including dependencies and starts the build process.\nI\u0026rsquo;m surely going to start using this from now on.\nI would like to find some tool that would give statistics utilization of commands and recommendations for auto-cleaning. That could be a nice side-project there isn\u0026rsquo;t something like this already.\nConclusion # In this article we went through pacseek a terminal user interface that would speed your process to install packages in Arch that may not be available on chaotic-aur repo and you need to download the dependencies and build it, without the need to search for them on the browser. It also shows you the dependencies and takes care of downloading the missing ones.\nReferences # https://github.com/moson-mo/pacseek https://aur.archlinux.org/packages https://github.com/moson-mo/pacseek/wiki https://aur.chaotic.cx ","date":"18 June 2024","externalUrl":null,"permalink":"/2024/06/pacseek/","section":"Posts","summary":"","title":"pacseek","type":"posts"},{"content":"In this article I will go through on how you can create a quick diagram flow in Draw.io with animation\nIntro # In this article I will go through on how you can create a quick diagram flow in Draw.io\nContext # Draw.io, also known as diagrams.net, is a free, web-based diagramming application that allows users to create a wide range of diagrams and charts. It is commonly used for creating flowcharts, organizational charts, network diagrams, UML diagrams, mind maps, and other types of visual representations.\nExample # Key Features # User-Friendly Interface: The interface is intuitive and easy to use, making it accessible for both beginners and advanced users.\nCloud Integration: It integrates with various cloud storage services such as Google Drive, OneDrive, Dropbox, and GitHub, allowing users to save and access their diagrams from anywhere.\nCollaboration: Multiple users can collaborate on diagrams in real-time, which is useful for team projects and group work.\nExtensive Shape Library: Draw.io offers a vast library of shapes and icons, catering to different diagramming needs, from simple flowcharts to complex engineering diagrams.\nCustomization: Users can customize shapes, styles, and formats to suit their specific requirements.\nImport and Export: It supports importing and exporting diagrams in various formats, including PNG, JPEG, SVG, PDF, and VSDX (Visio).\nOffline Mode: The application can be used offline by downloading the desktop version or using the Chrome app.\nNow, most of this shouldn\u0026rsquo;t be new to you if you have already used the tool. However, there are two points that I would like to reinforce:\nYou can create animation flows You can export to svg and keep those animations working SVG Format # If you plan to use diagrams in a web interface, it\u0026rsquo;s best to choose a solution that is resolution-independent and supports layers and animations. Therefore, be sure to export your final diagram to SVG format for optimal use in your project.\nHere some of the benefits of SVG:\nScalability: SVG images maintain quality at any size, making them perfect for responsive design. Small File Size: Typically smaller than raster images, leading to faster loading times. Editability: Easily modifiable with text editors or programmatically via CSS and JavaScript. Interactivity and Animation: Supports interactive elements and built-in animations without external plugins. Accessibility: Text is searchable and indexable; supports ARIA attributes for better accessibility. Compatibility: Supported by all major web browsers. Styling Flexibility: Can be styled using CSS, allowing for consistent and easily updatable designs. Layering and Grouping: Supports complex compositions through layering and grouping. Reusability: Graphics can be reused across different projects, saving time and effort. Create Animation Flows # Now that we have a format that support animations you can activate Flow Animation in some objects in Draw.io\nWhen you link two objects with an arrow check the Properties field and activate Flow Animation\nThe options allow you the define the motion speed, direction and timing\nConclusion # In this article, we explored the Flow Animation option in Draw.io and discussed the advantages of generating diagrams in SVG format. While other tools may offer more features, Draw.io makes it extremely easy to enhance your charts with animations, which can be particularly impressive in technical diagrams.\nReferences # https://app.diagrams.net/ ","date":"16 June 2024","externalUrl":null,"permalink":"/2024/06/drawio-animations/","section":"Posts","summary":"","title":"drawio animations","type":"posts"},{"content":"In this article I will go through a method that would allow you to incorporate a nice timeline on your website managing the entries from a google sheet.\nIntro # In this article I will go through a method that would allow you to incorporate a nice timeline on your website managing the entries from a google sheet.\nRequirements # This method implies that you would include a iframe on your html to render the timeline You will need Knightlab timeline javascript You will need to share GoogleSheet with your data so this method doesn\u0026rsquo;t work for private data Demo # The final result would be a interactive timeline similar to the following.\n{% iframe https://timeline.knightlab.com/examples/user-interface/index.html 100% 800px %}\nHexo # If you use Hexo you will need to include the following tag for iframes:\n{% iframe \u0026lt;src-url\u0026gt; \u0026lt;width\u0026gt; \u0026lt;height\u0026gt; %} Setup # Data Source # Prepare the data: Start by configuring your Googlesheet with data by making a copy of the following Template. You could check the following example to have a better idea on the final process.\nPublish the Gsheet: Under the File menu, Share submenu, select “Publish to the Web.”\n1.Create a iframe with your source: Create a iframe similar to the following example where the source is after https://docs.google.com/spreadsheets/d/\n\u0026lt;iframe src=\u0026#34;https://cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=1xuY4upIooEeszZ_lCmeNx24eSFWe0rHe9ZdqH2xqVNk\u0026amp;font=Default\u0026amp;lang=en\u0026amp;initial_zoom=2\u0026amp;height=100%\u0026#34; width=\u0026#34;100%\u0026#34; frameborder=\u0026#34;0\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; Note: Pay attention to date formats and the sharing options. The following URL has more details in case you bumped into trouble.\nConclusion # In this article, we guide you through the process of configuring an iframe with a dynamic timeline using JavaScript, which loads data from a Google Sheet implemented by Knight Lab. This straightforward process will help you enrich your content by presenting timeline data effectively.\nReferences # https://timeline.knightlab.com/docs/using-spreadsheets.html https://timeline.knightlab.com/#make-step-2 Gsheet Template ","date":"15 June 2024","externalUrl":null,"permalink":"/2024/06/timeline/","section":"Posts","summary":"","title":"Timeline","type":"posts"},{"content":"","date":"13 June 2024","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"In this article we will go through the best way to generate the python requirements.txt file.\nIntro # In this article we will go through the best way to generate the python requirements.txt file.\nAbout # When you execute a python program you probably already bump with this types of errors\nTraceback (most recent call last): File \u0026#34;mountain.py\u0026#34;, line 28, in ? from toolkit.interface import interface ImportError: No module named toolkit.interface This means that your python environment is missing a Dependency/Module.\nIn order to solve this types of issues you probably would start by adding the missing modules with the pip command.\npip install \u0026lt;missing_module\u0026gt; But this is hard to handle if the python project as a lot of dependencies, it would be useful if one had a list of all the modules required and let pip install all at once. Well that is where requirements.txt comes into action.\npip install -r requirements.txt Benefits # The usage of requirements.txt brings the following benefits:\nManaging dependencies: By listing the dependencies of your project in a requirements.txt file, you can easily see what packages are required and what versions they need to be. Sharing your project with others: If you share your project with others, you can include the requirements.txt file so that they can easily install the required packages. This can save them time and frustration and can help to ensure that everyone is using the same versions of the packages. How to create it # You can always create the requirements.txt in a manual way, but let\u0026rsquo;s check other options.\npip freeze # pip freeze outputs installed packages in requirements format. One can execute the following\npip freeze \u0026gt; requirements.txt Why not pip freeze? # pip freeze only saves the packages that are installed with pip install in your environment.\npip freeze saves all packages in the environment including those that you don’t use in your current project (if you don’t have virtualenv) and sometimes you just need to create requirements.txt for a new project without installing modules.\nBased on the previous using the next approach might be a better option.\npipreqs # The pipreqs module was created specifically to generate a requirements.txt based on imports used in a project\nOne can execute the following:\npython -m pipreqs.pipreqs --encoding utf-8 /path/to/project Check the following article if you would like to know more about python modules -m option.\nconda # If you are using conda to manage your python environment, you can execute the following command:\nconda list -e \u0026gt; requirements.txt Conclusion # In this article we have seen the importance of using requirements.txt to manage python dependencies for a given project and several options to generate that list.\nReferences # https://pypi.org/project/pipreqs https://pip.pypa.io/en/stable/cli/pip/ https://tldr.inbrowser.app/pages/common/pip https://www.freecodecamp.org/news/python-requirementstxt-explained/ ","date":"13 June 2024","externalUrl":null,"permalink":"/2024/06/python-requirements.txt/","section":"Posts","summary":"","title":"Python requirements.txt","type":"posts"},{"content":"","date":"13 June 2024","externalUrl":null,"permalink":"/tags/langchain/","section":"Tags","summary":"","title":"Langchain","type":"tags"},{"content":"This article is about serper, an API service that provides you fast results from Google Search\nIntro # serper is an API service that provides you fast results from Google Search\nFeatures # Response Time: Queries are generally returned within 1 to 2 seconds; occasionally, it can take 2 to 4 seconds if a retry is needed. Query Rate Limit: The default limit for Ultimate credits is 300 queries per second, allowing 15,000 to 18,000 searches per minute. Higher concurrency limits are available upon request. Real-Time Queries: All API calls query Google directly for the latest results with no caching involved. Search Query Customization: Users can specify search locations by country, language, and specific areas such as cities or neighborhoods. Credit Deduction: Credits are deducted for each successful query response. When the credit balance is zero, new queries will not be accepted. Example # Create the following python file test.py\nimport requests import json url = \u0026#34;https://google.serper.dev/search\u0026#34; payload = json.dumps({ \u0026#34;q\u0026#34;: \u0026#34;apple inc\u0026#34; }) headers = { \u0026#39;X-API-KEY\u0026#39;: \u0026#39;API_KEY\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } response = requests.request(\u0026#34;POST\u0026#34;, url, headers=headers, data=payload) print(response.text) You can clone the following repo which as this example code.\ngit clone git@github.com:rramos/serper-test.git cd repo python -m venv venv source venv/bin/activate pip install -r requirements.txt python test.py NOTE: Make sure to update the .env value with your API KEY\nPlayground # On the provided Playground, you will find several examples for multiple languages and various options to apply on your request.\nDashboard # The Dashboard provides you with great visibility on the credits consumption\nLangchain Integration # There is a Langchain wrapper if you would like to incorporate the search results. Here is one example using Ollama. Check the following article if you would like to setup Ollama locally.\nimport os from dotenv import load_dotenv from langchain.agents import AgentType, initialize_agent from langchain_community.utilities import GoogleSerperAPIWrapper from langchain_core.tools import Tool from langchain_community.llms import Ollama # Load environment variables from .env file load_dotenv() # Load API Key api_key = os.getenv(\u0026#39;SERPER_API_KEY\u0026#39;) # Query query=\u0026#34;rramos github tech and data notes\u0026#34; llm = Ollama(temperature=0,model=\u0026#34;llama3\u0026#34;) search = GoogleSerperAPIWrapper() tools = [ Tool( name=\u0026#34;Intermediate Answer\u0026#34;, func=search.run(query), description=\u0026#34;useful for when you need to ask with search\u0026#34;, ) ] self_ask_with_search = initialize_agent( tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True ) self_ask_with_search.invoke( \u0026#34;resume the search results ?\u0026#34; ) Conclusion # This service is incredible, offering blazing fast results with real-time queries and a user-friendly interface. The dashboard and playground provide all the information you need to get started in a raw simple form which I really like. As for pricing, it\u0026rsquo;s essential to calculate your costs to determine if integrating this service into your product is cost-effective.\nReferences # https://serper.dev https://python.langchain.com/v0.2/docs/integrations/tools/google_serper https://github.com/rramos/serper-test ","date":"13 June 2024","externalUrl":null,"permalink":"/2024/06/serper/","section":"Posts","summary":"","title":"serper","type":"posts"},{"content":" Excertp # This article will focus on the importance of Feedback and Strategies that could be used.\nIntro # This article will focus on the importance of Feedback and Strategies that could be used.\nImportance of Feedback # Feedback is a crucial skill for leaders, enhancing trust and closing the loop between action and impact. It helps uncover blind spots, shows care for team members, and resolves underlying issues. Effective feedback can unlock potential and indicate areas for improvement; incorrect feedback can cause confusion and damage relationships. Challenges in Giving Feedback # Fear of consequences and fear of nothing changing are the main barriers. Factors affecting feedback include cultural norms, trust levels, language barriers, and comfort with difficult conversations. Key hurdles include: Remote work, which complicates reading body language. Right vs. wrong scenarios, which can escalate into negative conversations if mishandled. Procrastination in giving feedback. Flow for Feedbacks # Preparation and Mindset # Approach feedback with empathy and a desire to help. Focus on behaviors, not personal traits. Prepare examples and consider the recipient\u0026rsquo;s perspective. Delivery # Use clear, specific language. Balance positive and constructive feedback. Encourage a two-way conversation. Follow-up # Monitor progress and provide ongoing feedback. Seek feedback on your own feedback-giving process. Strategies # Be Specific and Actionable # Detail-Oriented: Instead of saying \u0026ldquo;Good job,\u0026rdquo; specify what was good: \u0026ldquo;Great job on optimizing the database queries; it improved the response time significantly.\u0026rdquo; Actionable Suggestions: Offer clear, actionable suggestions rather than vague advice. For instance, \u0026ldquo;Consider adding more unit tests for edge cases\u0026rdquo; is more helpful than \u0026ldquo;Write better tests.\u0026rdquo; Timely Feedback # Immediate Feedback: Provide feedback as soon as possible after an event or milestone. This ensures the context is fresh and the impact is maximized. Regular Check-Ins: Schedule regular feedback sessions to discuss progress and areas of improvement. Balanced Feedback # Positive Reinforcement: Highlight what is working well to reinforce good practices. Constructive Criticism: Address areas of improvement tactfully, focusing on the issue, not the individual. Use Data and Examples # Data-Driven: Base feedback on concrete data, such as code metrics, performance improvements, or specific bugs. Examples: Use examples to illustrate points, making it easier for the team to understand and act on the feedback. Encourage Two-Way Communication # Open Dialogue: Create an environment where team members feel comfortable providing feedback to peers and management. Active Listening: Ensure you listen to the team’s feedback and act on it, showing that their input is valued. Focus on Goals and Outcomes # Alignment with Objectives: Tie feedback to the team’s goals and project outcomes to keep everyone aligned and motivated. Long-Term Development: Provide feedback that helps team members grow their skills and advance their careers. Be Empathetic and Supportive # Understand Challenges: Show empathy towards the challenges and constraints the team faces. Offer Support: Provide the necessary resources, training, or mentorship to help the team address feedback and improve. Public Recognition and Private Critique # Celebrate Successes Publicly: Acknowledge achievements in team meetings or public channels to boost morale. Critique Privately: Offer critical feedback in private to avoid embarrassment and foster a more open and honest discussion. Use Structured Feedback Tools # Code Reviews: Implement regular code reviews to provide continuous feedback on code quality and best practices. Retrospectives: Conduct retrospectives after project milestones or sprints to review what went well and what could be improved. Follow Up # Check Progress: Follow up on feedback to see how the team has addressed it and to provide further guidance if necessary. Continuous Improvement: Treat feedback as part of an ongoing process to continually enhance the team’s performance and output. Implementing these strategies can foster a positive and productive environment, helping the software engineering team to thrive and achieve their full potential. Final Thoughts # Start feedback from a place of care and remember to listen actively. Regular feedback conversations help maintain continuous improvement and strong relationships. Overall, feedback is a dynamic and essential component of effective leadership and team development. References # https://leaddev.com/team/art-giving-and-receiving-feedback-leader ","date":"13 June 2024","externalUrl":null,"permalink":"/2024/06/feedback/","section":"Posts","summary":"","title":"Feedback","type":"posts"},{"content":"Article about CARA a social media and portfolio platform for artists\nAbout # Cara is a social media and portfolio platform for artists.\nWith the widespread use of generative AI, we decided to build a place that filters out generative AI images so that people who want to find authentic creatives and artwork can do so easily.\nMany platforms currently accept AI art when it’s not ethical, while others have promised “no AI forever” policies without consideration for the scenario where adoption of such technologies may happen at the workplace in the coming years.\nThe future of creative industries requires nuanced understanding and support to help artists and companies connect and work together. We want to bridge the gap and build a platform that we would enjoy using as creatives ourselves.\nWhy is this relevant # Half a Million Artists left Instagram for Cara in a Week\nFeatures # Social Media Feed \u0026amp; Portfolio, for Desktop and Mobile Automated AI Image Detection \u0026amp; Filtering Cara Glaze Multi-video and Sketchfab Embeds Animated GIF Covers Jobs Board Custom Home Feed Bookmarks and Albums Custom Crop for Thumbnails \u0026amp; Covers How it works # Cara partnership with the Glaze Project. Glaze is a tool designed to protect artists from style mimicry by generative AI models.\nGlaze Project # The Glaze Project (including Glaze, Nightshade, WebGlaze and others) is a research effort that develops technical tools with the explicit goal of protecting human creatives against invasive uses of generative artificial intelligence or GenAI. Our team is composed of computer science professors and PhD students from the University of Chicago. We perform research studies and develop tools that artists can use to disrupt unauthorized AI training on their work product. Ultimately, our goal is to ensure the continued vitality of human artists, and to restore balance and ensure a healthy coexistence between AI and human creatives, where the human creatives retain agency and control over their work products and their use.\nConclusion # It\u0026rsquo;s fascinating how groups of creators are collaborating to counter the overwhelming presence of generative AI, which some platforms exploit solely for profit. I believe this kind of collective action will become more prevalent in our society to safeguard human creativity, allowing AI to handle the monotonous and repetitive tasks we endure. This movement could also create new business opportunities, as there is evident demand for AI-free content. However, a significant challenge lies in developing tools for proper identification of such content, at least until a legal framework is established to protect creators.\nReferences # https://cara.app/ai-policy https://www.youtube.com/watch?v=BaHnqNaka4o https://glaze.cs.uchicago.edu/ ","date":"12 June 2024","externalUrl":null,"permalink":"/2024/06/cara/","section":"Posts","summary":"","title":"Cara","type":"posts"},{"content":"","date":"12 June 2024","externalUrl":null,"permalink":"/tags/generative-ai/","section":"Tags","summary":"","title":"Generative AI","type":"tags"},{"content":"","date":"12 June 2024","externalUrl":null,"permalink":"/tags/web-performance/","section":"Tags","summary":"","title":"Web Performance","type":"tags"},{"content":"In this article I will go through Google Web Vitals\nIntro # Web Vitals is an initiative by Google to provide unified guidance for quality signals that are essential to delivering a great user experience on the web.\nCore Web Vitals # Site owners shouldn\u0026rsquo;t have to be performance experts to understand the quality of experience they are delivering to their users. The Web Vitals initiative aims to simplify the landscape, and help sites focus on the metrics that matter most, the Core Web Vitals.\nCore Web Vitals are the subset of Web Vitals that apply to all web pages, should be measured by all site owners, and will be surfaced across all Google tools. Each of the Core Web Vitals represents a distinct facet of the user experience, is measurable in the field, and reflects the real-world experience of a critical user-centric outcome.\nLargest Contentful Paint (LCP) # Measures loading performance. To provide a good user experience, LCP should occur within 2.5 seconds of when the page first starts loading.\nMore info: here\nInteraction to Next Paint (INP) # Measures interactivity. To provide a good user experience, pages should have a INP of 200 milliseconds or less.\nMore info: here\nCumulative Layout Shift (CLS) # Measures visual stability. To provide a good user experience, pages should maintain a CLS of 0.1. or less.\nMore info: here\nTo ensure you\u0026rsquo;re hitting the recommended target for these metrics for most of your users, a good threshold to measure is the 75th percentile of page loads, segmented across mobile and desktop devices.\nTools # Google believes that the Core Web Vitals are critical to all web experiences. As a result, it is committed to surfacing these metrics in all of its popular tools.\nhttps://developer.chrome.com/docs/lighthouse/overview https://developer.chrome.com/docs/crux https://pagespeed.web.dev https://search.google.com/search-console/about lighthouse # You can use lighthouse by installing a google chrome extension or running the following command.\nnpm install -g lighthouse And then running your validation example:\nlighthouse https://rramos.github.io/ --view This will pop-up a browser to make the required tests and the --view option to directly open the report.\nConclusion # In this article we went through web vitals metrics to assess the quality of a website according to Google metrics LCP, INP and CLS. Also provided a list of support tools to debug those values for a given website.\nReference # https://web.dev/articles/vitals-ga4 https://developer.chrome.com/docs/lighthouse https://developer.chrome.com/docs/crux https://pagespeed.web.dev https://search.google.com/search-console/about ","date":"12 June 2024","externalUrl":null,"permalink":"/2024/06/web-vitals/","section":"Posts","summary":"","title":"Web Vitals","type":"posts"},{"content":"This article is about Motherduck a data warehouse solution build with DuckDB\nIntro # MotherDuck is a collaborative data warehouse that extends the power of DuckDB to the cloud\nWhat is Motherduck # MotherDuck is the first analytics data warehouse that offers cloud scale and individualized, user-level tenancy. Managing user-level compute limits and cost attribution without cumbersome upfront configuration hasn’t been possible until now.\nMotherDuck\u0026rsquo;s key components are:\nThe MotherDuck cloud service MotherDuck\u0026rsquo;s DuckDB SDK Hybrid execution The MotherDuck web UI The MotherDuck cloud service enables you to store structured data, query that data with SQL, and share it with others. A key MotherDuck product principle is ease of use.\nServerless execution model - You don\u0026rsquo;t need to configure or spin up instances, clusters, or warehouses. You simply write and submit SQL. MotherDuck takes care of the rest. Under the hood, MotherDuck runs DuckDB and speaks DuckDB\u0026rsquo;s SQL dialect.\nManaged storage - you can load data into MotherDuck storage to be queried or shared. MotherDuck storage is durable, secure, and automatically optimized for best performance. MotherDuck storage is surfaced to you via the catalog and logical primitives database, schema, table, view, etc. In addition, MotherDuck can query data outside of MotherDuck storage—as data on Amazon S3, via https endpoints, on your laptop, and so on.\nThe service layer - MotherDuck provides key capabilities like secure identity, authorization, administration, monitoring, and so on. Currently, billing is not enabled for MotherDuck, and the service is free to use.\nNOTE: Currently, MotherDuck runs on AWS us-east-1 region.\nHow it works # DuckDB has become widely known as “SQLite for Analytics” – a powerful SQL analytics engine with broad adoption in development workflows, ad-hoc analytics on the laptop and embedded applications.\nUsecases # Data scientists, analysts, and engineers love DuckDB because it works efficiently regardless of where their data resides. Given that many data professionals have powerful laptops that are often underutilized, they prefer to bring data to their local machines for more efficient crunching, especially for ad hoc analysis and development. MotherDuck enhances this capability by enabling local data analysis while still allowing JOIN operations with data processed in the cloud, optimizing the use of all computing resources.\nHYBRID EXECUTION # When connected together, DuckDB and MotherDuck form a different type of distributed system. The two nodes work in concert so you can query data wherever it lives, in the most efficient way possible. This query execution model, called hybrid execution, automatically routes the various stages of queries execution to the most opportune locations, including highly arbitrary scenarios:\nIf a SQL query queries data on your laptop, MotherDuck routes the query to your local DuckDB instance If a SQL query queries data in MotherDuck or S3, MotherDuck routes that query to MotherDuck If a SQL query executes a join between data on your laptop and data in MotherDuck, MotherDuck finds the best way to efficiently join the two SELECT cr.currency_code, t.passenger_count, AVG(t.total_amount * cr.exchange_rate) as average_converted_amount FROM sample_data.nyc.yellow_cab_nyc_2022_11 t CROSS JOIN (SELECT * FROM \u0026#39;./popular_currency_rate_dollar_20230620.csv\u0026#39;) cr WHERE cr.currency_code = \u0026#39;EUR\u0026#39; GROUP BY cr.currency_code, t.passenger_count ORDER BY t.passenger_count ASC; In the example above, the table yellow_cab_nyc lives in MotherDuck in the cloud, and is being joining with local data from a CSV file.\nYou can even do hybrid query execution with data stored in s3, with MotherDuck securely storing and managing your AWS credentials\nThis engine is extremely powerful to leverage a Datalake or Lakehouse Architecture.\nCLI Usage # When start duckdb execute the following command\nATTACH \u0026#39;md:\u0026#39;; It will start oauth to authorize the connection to the service. Check the following page for more options related with authentication.\nYou can then check the available databases\nshow databases; Executing a query on the sample data\nSELECT year, AVG(pm25_concentration) AS avg_pm25, AVG(pm10_concentration) AS avg_pm10, AVG(no2_concentration) AS avg_no2 FROM sample_data.who.ambient_air_quality WHERE city = \u0026#39;Berlin\u0026#39; GROUP BY year ORDER BY year DESC; NOTE: You need to use duckdb version 1.0.0\nExamples # One can explore MotherDuck samples and SQL queries on the following URL:\nhttps://motherduck.com/docs/category/example-datasets BI Tools # Motherduck supports DuckDB JDBC driver the following example doc explains how to setup Tableau but you can connect other systems using that driver.\nhttps://motherduck.com/docs/integrations/bi-tools/tableau Check the docs for other integrations as surelly they will include more options soon.\nPricing # There are free, standard and custom plans. Check the official page:\nhttps://motherduck.com/product/pricing Conclusion # DuckDB recently announced the launch of version 1.0.0 after six years of development, providing a very stable version. Complementing this, MotherDuck introduced a PaaS solution that offers a serverless execution model, storage, and a service layer for collaborative work. This is an excellent solution for those who want to avoid the complexities of underlying service management while empowering their users.\nReferences # https://motherduck.com/ https://motherduck.com/docs/getting-started/ https://motherduck.com/blog/announcing-motherduck-duckdb-in-the-cloud/ https://motherduck.com/docs/architecture-and-capabilities https://motherduck.com/blog/announcing-motherduck-general-availability-data-warehousing-with-duckdb https://app.motherduck.com https://duckdb.org/2024/06/03/announcing-duckdb-100.html ","date":"12 June 2024","externalUrl":null,"permalink":"/2024/06/motherduck-ga/","section":"Posts","summary":"","title":"motherduck GA","type":"posts"},{"content":"","date":"12 June 2024","externalUrl":null,"permalink":"/tags/paas/","section":"Tags","summary":"","title":"PaaS","type":"tags"},{"content":"This article is about Multi-Modal LLMs\nIntro # In the context of Large Language Models (LLMs), \u0026ldquo;multi-modal\u0026rdquo; refers to the capability of a model to process and generate data across different types of input and output modalities. This typically involves integrating and handling various forms of data such as text, images, audio, and even video within a single model. The goal is to create a more comprehensive and versatile AI that can understand and generate responses that are not limited to just one type of data.\nKey Concepts of Multi-modal LLMs # Cross-Modal Understanding # Multi-modal LLMs can interpret and connect information across different types of inputs. For example, the model can understand a question posed in text form and provide an answer based on an image input.\nData Integration # These models are trained on datasets that include multiple data types, allowing them to learn correlations and relationships between text, images, and other forms of data.\nUnified Architecture # A unified architecture typically supports multiple modalities within a single framework, enabling seamless interaction between different types of data. This contrasts with separate models for each modality.\nEnhanced Applications # Multi-modal capabilities enable a wide range of applications such as image captioning, visual question answering, audio-visual recognition, and more. For instance, a multi-modal model can generate descriptive text based on an image or understand and generate responses that consider both text and image inputs.\nExamples of Multi-modal LLMs # OpenAI\u0026rsquo;s GPT-4: GPT-4 can process and generate both text and image inputs, allowing it to perform tasks like describing the contents of an image or generating text based on visual input. Google\u0026rsquo;s MUM (Multitask Unified Model): MUM can handle information across text and images and is designed to perform complex queries that require understanding multiple modalities. BakLLaVA: is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA architecture. LLaVA: is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6. Testing # Lets use ollama to pull the bakllava model\nollama run bakllava Then send one input picture eg.\n\u0026gt;\u0026gt;\u0026gt; What\u0026#39;s in this image? /Users/jmorgan/Desktop/smile.png The image features a yellow smiley face, which is likely the central focus of the picture. Conclusion # In summary, multi-modal LLMs represent an advanced stage in the development of AI, enabling models to process and generate content across various data types, thereby enhancing their capability to interact with and understand the world in a more holistic manner.\nReferences # https://llava-vl.github.io https://github.com/SkunkworksAI/BakLLaVA https://ollama.com/library/bakllava ","date":"11 June 2024","externalUrl":null,"permalink":"/2024/06/multi-modal-llms/","section":"Posts","summary":"","title":"Multi-Modal LLMs","type":"posts"},{"content":"Article about Nager.Date an open source project with the goal of making holidays easy to query.\nIntro # Nager.Date is an open source project with the goal of making holidays easy to query.\nAbout # Discover the convenience of easily accessing holidays from over 100 countries with Nager.Date. Our popular project utilizes the power of .NET and offers a user-friendly public REST API for seamless integration into your application.\nTesting # Use the following sample python code to obtain the national holiday in Portugal in 2024 get_national_holidays.py.\nimport json import requests response = requests.get(\u0026#39;https://date.nager.at/api/v3/publicholidays/2024/PT\u0026#39;) public_holidays = json.loads(response.content) for public_holiday in public_holidays: print(public_holiday[\u0026#39;date\u0026#39;],\u0026#34; - \u0026#34;,public_holiday[\u0026#39;name\u0026#39;]) You should have a similar output when running\n2024-01-01 - New Year\u0026#39;s Day 2024-02-13 - Carnival 2024-03-29 - Good Friday 2024-03-31 - Easter Sunday 2024-04-25 - Freedom Day 2024-05-01 - Labour Day 2024-05-30 - Corpus Christi 2024-06-01 - Azores Day 2024-06-10 - National Day 2024-07-01 - Madeira Day 2024-08-15 - Assumption Day 2024-10-05 - Republic Day 2024-11-01 - All Saints Day 2024-12-01 - Restoration of Independence 2024-12-08 - Immaculate Conception 2024-12-25 - Christmas Day 2024-12-26 - St. Stephen\u0026#39;s Day Countries coverage # You can find an overview of the supported countries here.\nHoliday types # What variants of holidays are supported by Nager.Date\nType Description Public Public holiday Bank Bank holiday, banks and offices are closed School School holiday, schools are closed Authorities Authorities are closed Optional Majority of people take a day off Observance Optional festivity, no paid day off Data precision # There is no generally valid designation for the next administrative level of countries. \u0026ldquo;Nager.Date\u0026rdquo; supports the initial subdivision of a country, but we will not support a detailed level because the effort required is too high.\nTo keep it generally valid, we will treat this subdivision as SubdivisionCodes, this will replace the current designation Counties.\nUnited States of America use States Germany use States Austria use States Switzerland use Cantons Brazil use States Australia use States or Territories Russia use Federal districts Canada use Province or Territories\nResponse Model # date The date of the holiday localName Local name name English name countryCode ISO 3166-1 alpha-2 fixed Is this public holiday every year on the same date global Is this public holiday in every county (federal state) counties If it is not global you found here the Federal states (ISO-3166-2) launchYear The launch year of the public holiday types The types of the public holiday, several possible (Public, Bank, School,\u0026hellip;) Example:\n[ { \u0026#34;date\u0026#34;: \u0026#34;2017-01-01\u0026#34;, \u0026#34;localName\u0026#34;: \u0026#34;Neujahr\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;New Year\u0026#39;s Day\u0026#34;, \u0026#34;countryCode\u0026#34;: \u0026#34;AT\u0026#34;, \u0026#34;fixed\u0026#34;: true, \u0026#34;global\u0026#34;: true, \u0026#34;counties\u0026#34;: null, \u0026#34;launchYear\u0026#34;: 1967, \u0026#34;types\u0026#34;: [ \u0026#34;Public\u0026#34; ] }, ... References # https://date.nager.at/ https://github.com/nager/Nager.Date https://date.nager.at/Api ","date":"11 June 2024","externalUrl":null,"permalink":"/2024/06/date.nager/","section":"Posts","summary":"","title":"date.nager","type":"posts"},{"content":"","date":"11 June 2024","externalUrl":null,"permalink":"/tags/openapi/","section":"Tags","summary":"","title":"OpenAPI","type":"tags"},{"content":"In this article I gather the key principals for requesting and providing Feedback\nIntro # This article by Cara Marin provides a concise yet insightful summary on comments and feedback etiquette in distributed work environments. It offers valuable hints on how to enhance collaboration through effective commenting practices. I highly recommend reading it for practical tips and best practices.\nSummary # In the context of distributed work, effective comments and feedback are crucial for collaboration among software engineers. Poor comments etiquette can hinder collaboration and complicate decision-making.\nKey Principals # Key Principles for Good Comments Etiquette:\nListen Fully and Presently: Engage completely with the document before commenting. Confirm Alignment and Understanding: Ensure mutual understanding before responding. Collect Your Thoughts: Think before providing feedback. Provide Actionable Feedback: Comments should be constructive and practical. Agree on Next Steps: Ensure clear next steps are outlined. Importance of Comments Etiquette # Repetitive tasks, like reviewing documents, can lead to habits that undermine effective feedback, such as jumping to conclusions or making hasty comments.\nHow to Get High-Quality Feedback # Specify the Feedback Needed: Be clear whether you need strategic, directional, or final-stage proofreading feedback. Guide Reviewers: Direct attention to specific parts of the document needing feedback. Best Practices for Commenting # Direct Quotes and Highlighting # Use quotes or highlight text effectively to make comments clear.\nAppropriate Use of Comments # Know when to use comments and when to opt for a direct conversation, especially for sensitive or complex feedback.\nA general rule is: If you need more than three comments back and forth to discuss, you should probably move to a face-to-face conversation, like a Slack huddle or Zoom call. Make sure you circle back on the initial thread and document a summary of what was discussed during that call for future reviewers\nNotify Colleagues # Inform teammates when feedback is provided to avoid confusion and ensure timely responses.\nConclusion # Adopting and maintaining good comments etiquette is essential for effective collaboration in distributed teams. Leading by example can promote better feedback practices within an organization.\nReferences # https://leaddev.com/process/comments-etiquette-requesting-and-giving-feedback ","date":"11 June 2024","externalUrl":null,"permalink":"/2024/06/feedback-etiquette/","section":"Posts","summary":"","title":"Feedback Etiquette","type":"posts"},{"content":"This article is about SQL RETURNING clause\nIntro # The RETURNING clause is not a statement itself, but a clause that can optionally appear near the end of top-level DELETE, INSERT, and UPDATE statements. The effect of the RETURNING clause is to cause the statement to return one result row for each database row that is deleted, inserted, or updated.\nUsecase # The RETURNING clause is designed to provide the application with the values of columns that are filled in automatically by SQLite.\nExample # CREATE TABLE t0( a INTEGER PRIMARY KEY, b DATE DEFAULT CURRENT_TIMESTAMP, c INTEGER ); INSERT INTO t0(c) VALUES(random()) RETURNING *; In the INSERT statement above, SQLite computes the values for all three columns. The RETURNING clause causes SQLite to report the chosen values back to the application. This saves the application from having to issue a separate query to figure out exactly what values were inserted.\nLimitations # RETURNING is not standard SQL. It is an extension. SQLite\u0026rsquo;s syntax for RETURNING is modelled after PostgreSQL and is supported since version 3.35. You need to check if your RDBMS support this expression\nReference # https://www.sqlite.org/lang_returning.html ","date":"10 June 2024","externalUrl":null,"permalink":"/2024/06/sql-returning/","section":"Posts","summary":"","title":"SQL RETURNING","type":"posts"},{"content":"","date":"10 June 2024","externalUrl":null,"permalink":"/tags/books/","section":"Tags","summary":"","title":"Books","type":"tags"},{"content":"","date":"10 June 2024","externalUrl":null,"permalink":"/tags/teaching/","section":"Tags","summary":"","title":"Teaching","type":"tags"},{"content":"This article share information regarding Teaching Tech Together book.\nIntro # This article is mainly to share this amazing Book for Tech teaching.\nIf you lack the background and like to teach tech topics the content may be a valuable asset on you Library.\nhttps://teachtogether.tech/en/index.html About the Book # Grassroots groups have sprung up around the world to teach programming, web design, robotics, and other skills to free-range learners. These groups exist so that people don’t have to learn these things on their own, but ironically, their founders and teachers are often teaching themselves how to teach.\nThere’s a better way. Just as knowing a few basic facts about germs and nutrition can help you stay healthy, knowing a few things about cognitive psychology, instructional design, inclusivity, and community organization can help you be a more effective teacher\nThis book presents key ideas you can use right now, explains why we believe they are true, and points you at other resources that will help you go further.\nWho is this book for # This book id for end-user teachers where teaching isn’t their primary occupation, they have little or no background in pedagogy, and they may work outside institutional classrooms.\nChapters # Mental Models and Formative Assessment Expertise and Memory Cognitive Architecture Individual Learning A Lesson Design Process Pedagogical Content Knowledge Teaching as Performance Art In the Classroom Motivation and Demotivation Teaching Online Exercise Types Building a Community of Practice Outreach Meetings, Meetings, Meetings Checklists and Templates Reference # https://teachtogether.tech/en/index.html https://github.com/gvwilson/teachtogether.tech/ ","date":"10 June 2024","externalUrl":null,"permalink":"/2024/06/teaching-tech-together/","section":"Posts","summary":"","title":"Teaching Tech Together","type":"posts"},{"content":"This article I will go over MinIO a object storage solution\nIntro # MinIO is an object storage solution that provides an Amazon Web Services S3-compatible API and supports all core S3 features. MinIO is built to deploy anywhere - public or private cloud, baremetal infrastructure, orchestrated environments, and edge infrastructure.\nFeatures # Install # Installed the Arch package with pacman but check the documentation for other methods based on your system.\nsudo pacman -S minio Launch the MinIO Server # Run the following command from the system terminal or shell to start a local MinIO instance using the ./data folder to store the data.\nmkdir data minio server ./data You should be granted with information on the endpoints for the API, WebUI and CLI.\nTesting locally # Tofu # The following repo provides one example where you can setup as code the required containers using the minio terraform provider.\nClone the following repo which will create a sample bucket and a dummy text_file\ngit clone git@github.com:rramos/tofu-minio.git cd tofu-minio tofu init tofu plan tofu apply After applying that plan you will have a bucket called state-terraform-s3 with a object text.txt.\nLet\u0026rsquo;s use other options now.\nConsole # The MinIO Console is a rich graphical user interface that provides similar functionality to the mc or mcli command line tool. You can access it view browser\nhttp://127.0.0.1:43675/login NOTE: The port used by MinIO depends on the configuration specified when you started the service. To determine the port, check the output of the server startup command\nCLI # The MinIO Client mc or mcli command line tool provides a modern alternative to UNIX commands like ls, cat, cp, mirror, and diff with support for both filesystems and Amazon S3-compatible cloud storage services.\nYou should setup alias for your services (Obtain the service key and secret from the console)\nmcli alias set myio http://192.168.1.178:9000 ACCESS_KEY SECRET_KEY After that we can check if our service is operational with\nmcli admin info myio You should have a similar output\n$ mcli admin info myio ● 192.168.1.178:9000 Uptime: 10 minutes Version: 2024-06-06T09:36:42Z Network: 1/1 OK Drives: 1/1 OK Pool: 1 ┌──────┬────────────────────────┬─────────────────────┬──────────────┐ │ Pool │ Drives Usage │ Erasure stripe size │ Erasure sets │ │ 1st │ 85.9% (total: 953 GiB) │ 1 │ 1 │ └──────┴────────────────────────┴─────────────────────┴──────────────┘ 27 B Used, 1 Bucket, 1 Object, 1 Version 1 drive online, 0 drives offline, EC:0 We can now use the cli to execute tradicional operations\nmcli cp file.txt myio/state-terraform-s3/file.txt The full list of commands is available on the following URL\nhttps://min.io/docs/minio/linux/reference/minio-mc.html API # In this example we will be copying data though the API using the minio module, let\u0026rsquo;s start by installing the required pip package.\npip install minio Next generate a key/secret in the console and update the following python code minio_cp.py\nfrom minio import Minio from minio.error import S3Error # MinIO server credentials MINIO_URL = \u0026#34;127.0.0.1:9000\u0026#34; # or your MinIO server URL ACCESS_KEY = \u0026#34;ACCESS_KEY\u0026#34; SECRET_KEY = \u0026#34;SECRET_KEY\u0026#34; # Initialize the MinIO client minio_client = Minio( MINIO_URL, access_key=ACCESS_KEY, secret_key=SECRET_KEY, secure=False # Set to False if you are not using HTTPS ) # File to upload file_path = \u0026#34;./file2.txt\u0026#34; bucket_name = \u0026#34;state-terraform-s3\u0026#34; object_name = \u0026#34;file2.txt\u0026#34; # Ensure the bucket exists found = minio_client.bucket_exists(bucket_name) if not found: minio_client.make_bucket(bucket_name) else: print(f\u0026#34;Bucket \u0026#39;{bucket_name}\u0026#39; already exists\u0026#34;) # Upload the file try: minio_client.fput_object( bucket_name, object_name, file_path ) print(f\u0026#34;\u0026#39;{file_path}\u0026#39; is successfully uploaded as \u0026#39;{object_name}\u0026#39; to bucket \u0026#39;{bucket_name}\u0026#39;.\u0026#34;) except S3Error as err: print(f\u0026#34;Error occurred: {err}\u0026#34;) You can test by generating a new file2.txt and executing the code.\necho \u0026#34;This is file2.txt \u0026gt; file2.txt\u0026#34; python minio_cp.py TODO: This example is merely explanatory and doesn\u0026rsquo;t use secure connection. It is not advisable to run it in production.\nExample # one example with DuckDB creating a table pointing for that storage where sample_data.csv file exists\nCREATE SECRET secret1 ( TYPE S3, KEY_ID \u0026#39;ACCESS_KEY\u0026#39;, SECRET \u0026#39;SECRET_KEY\u0026#39;, REGION \u0026#39;us-east-1\u0026#39;, ENDPOINT \u0026#39;localhost:9000\u0026#39;, URL_STYLE \u0026#39;path\u0026#39;, USE_SSL false ); SELECT * FROM \u0026#39;s3://state-terraform-s3/sample_data.csv\u0026#39;; NOTE: Replace the ACCESS_KEY and SECRET from the ones provided in the console.\nReplication # MinIO supports server-side and client-side replication of objects between source and destination buckets.\nServer-Side Bucket Replication # Configure per-bucket rules for automatically synchronizing objects between MinIO deployments. The deployment where you configure the bucket replication rule acts as the “source” while the configured remote deployment acts as the “target”\nClient-side Bucket Replication # Use the command process to synchronize objects between buckets within the same S3-compatible cluster or between two independent S3-compatible clusters\nReplication of Delete Operations # MinIO supports replicating delete operations, where MinIO synchronizes deleting specific object versions and new delete markers. Delete operation replication uses the same replication process as all other replication operations.\nSynchronous vs Asynchronous Replication # MinIO supports specifying either asynchronous (default) or synchronous replication for a given remote target.\nReplication Process # MinIO uses a replication queuing system with multiple concurrent replication workers operating on that queue. MinIO continuously works to replicate and remove objects from the queue while scanning for new unreplicated objects to add to the queue.\nConclusion # MinIO grew to become the most broadly deployed object store on the planet by focusing on what mattered the most to developers, architects and applications. The service can be configured in several ways depending on the scalability, replication and level of service that you want for your solution design. Also brings the advantage of developers being able to deploy it locally and later extend for a more robust solution or even pay for that service if you don\u0026rsquo;t want to deal with this type of operation.\nThe integration with several solutions and support for several APIs show strong potential for being a Big Player on the Lakehouse Architecture. Recommend the reading of the article DuckDB and MinIO for a Modern Data Stack if you like to know more.\nMinIO offers a variety of replication options, making it an excellent choice for multi-cloud solutions. These replication capabilities ensure high availability, data durability, and efficient disaster recovery across different cloud environments.\nReferences # https://min.io/ https://min.io/docs/minio/container/index.html https://github.com/minio/minio https://blog.min.io/duckdb-and-minio-for-a-modern-data-stack/ https://registry.terraform.io/providers/aminueza/minio/latest/docs ","date":"10 June 2024","externalUrl":null,"permalink":"/2024/06/minio/","section":"Posts","summary":"","title":"minIO","type":"posts"},{"content":"Article about Duckdb support for Hugging Face data sources.\nIntro # DuckDB and Hugging Face co-authored an announcement about the new release using hf:// prefix in DuckDB to access datasets in Hugging Face repositories, this spawns a new wave of opportunities to make data more accessible and lightweight for the AI and ML sectors.\nDataset repositories # Hugging Face is a popular central platform where users can store, share, and collaborate on machine learning models, datasets, and other resources.\nA dataset typically includes the following content:\nA README file: This plain text file provides an overview of the repository and its contents. It often describes the purpose, usage, and specific requirements or dependencies. Data files: Depending on the type of repository, it can include data files like CSV, Parquet, JSONL, etc. These are the core components of the repository. Now, it is possible to query them using the URL pattern below:\nhf://datasets/⟨my_username⟩/⟨my_dataset⟩/⟨path_to_file⟩ For example, to read a CSV file, you can use the following query:\nSELECT * FROM \u0026#39;hf://datasets/datasets-examples/doc-formats-csv-1/data.csv\u0026#39;; NOTE: You must have the latest duckdb version\nCreating a local table # To avoid accessing the remote endpoint for every query, you can save the data in a DuckDB table by running a CREATE TABLE \u0026hellip; AS command. For example:\nCREATE TABLE data AS SELECT * FROM \u0026#39;hf://datasets/datasets-examples/doc-formats-csv-1/data.csv\u0026#39;; Then, simply query the data table as follows:\nSELECT * FROM data; Glob Patterns # To query all files under a specific format, you can use a glob pattern. Here’s how you can count the rows in all files that match the pattern *.parquet.\nSELECT count(*) AS count FROM \u0026#39;hf://datasets/cais/mmlu/astronomy/*.parquet\u0026#39;; Versioning and revisions # In Hugging Face repositories, dataset versions or revisions are different dataset updates. Each version is a snapshot at a specific time, allowing you to track changes and improvements. In git terms, it can be understood as a branch or specific commit.\nYou can query different dataset versions/revisions by using the following URL:\nhf://datasets/⟨my-username⟩/⟨my-dataset⟩@⟨my_branch⟩/⟨path_to_file⟩ For example:\nSELECT * FROM \u0026#39;hf://datasets/datasets-examples/doc-formats-csv-1@~parquet/**/*.parquet\u0026#39;; Authentication # Configure your Hugging Face Token in the DuckDB Secrets Manager to access private or gated datasets. First, visit Hugging Face Settings – Tokens to obtain your access token\nCONFIG: The user must pass all configuration information into the CREATE SECRET statement. To create a secret using the CONFIG provider, use the following command: CREATE SECRET hf_token ( TYPE HUGGINGFACE, TOKEN \u0026#39;your_hf_token\u0026#39; ); CREDENTIAL_CHAIN: Automatically tries to fetch credentials. For the Hugging Face token, it will try to get it from ~/.cache/huggingface/token. To create a secret using the CREDENTIAL_CHAIN provider, use the following command: CREATE SECRET hf_token ( TYPE HUGGINGFACE, PROVIDER CREDENTIAL_CHAIN ); Conclusion # The integration of hf:// paths in DuckDB significantly streamlines accessing and querying over 150,000 datasets available on Hugging Face. This feature democratizes data manipulation and exploration, making it easier for users to interact with various file formats such as CSV, JSON, JSONL, and Parquet. By utilizing hf:// paths, users can execute complex queries, efficiently handle large datasets, and harness the extensive resources of Hugging Face repositories\nReferences # https://duckdb.org/2024/05/29/access-150k-plus-datasets-from-hugging-face-with-duckdb https://huggingface.co/docs/hub/datasets-duckdb https://huggingface.co/blog/chilijung/access-150k-hugging-face-datasets-with-duckdb ","date":"9 June 2024","externalUrl":null,"permalink":"/2024/06/duckdb-hf/-prefix/","section":"Posts","summary":"","title":"DuckDB hf:// Prefix","type":"posts"},{"content":"","date":"9 June 2024","externalUrl":null,"permalink":"/tags/hugging-face/","section":"Tags","summary":"","title":"Hugging Face","type":"tags"},{"content":"Article about Killercoda service\nWhat is Killercoda # Killercoda is a place where you open your browser and get instant access to a real Linux or Kubernetes environment ready to use. These environments are maintained remotely and accessed locally, hence no setup or huge resource usage in local browsers. As a user you can learn from existing scenarios and as a creator you can provide scenarios to teach any kind of tools or technologies.\nWhat can Creators do on Killercoda # Various examples showing what\u0026rsquo;s possible for creators on Killercoda.\nRepository: https://github.com/killercoda/scenario-examples Documentation: https://killercoda.com/creators Group scenarios into Courses: https://killercoda.com/examples-courses Creator Workflow # You add a Github repository to your Killercoda profile Every time you push into your repository and the specified branch, your Killercoda scenarios will be updated. You\u0026rsquo;ll write for example JSON, Markdown and Bash (for possible verification of user input) Create github Repository # One can clone of or the examples Add Github Repository Name and Branch # Go to Creator Repository and add the Repository Name and Branch. Provide Access # Killercoda needs read access to your repository.\nGo to Creator Repository and copy your Deploy Key and add it under Repository Settings -\u0026gt; Deploy Keys. Github Webhook # Every time you push, Killercoda will update your scenarios.\nGo to Creator Repository and copy the Payload Url and Secret. Also set the Content Type to JSON under Github Repository Settings -\u0026gt; Webhooks. List scenarios # Go to Creator Scenarios. Here you should see your scenarios.\nExample # The following repository provides a scenario to introduction on linux commands:\nhttps://github.com/rramos/killercoda You can access the Lab scenario at:\nhttps://killercoda.com/rramos Limitations # FREE Membership # Use all scenarios as much as you want Create public scenarios and share with others PLUS Membership # Use scenarios for up to 4 hours instead of just one (info) Open up to 3 scenarios at the same time Solve the CKA, CKS, CKAD scenarios in the Exam Remote Desktop Proof you\u0026rsquo;re human and have no Captcha bot challenges Faster load times and skip queues Show us you like what we\u0026rsquo;re doing Have prioritized support Conclusion # This service is particularly useful for setting up quick laboratories. While the free membership has some limitations, it allows for the creation of content for public use efficiently without the need for dedicated infrastructure. Additionally, the inclusion of validations for various tests adds a valuable feature.\nReferences # https://killercoda.com/ https://github.com/killercoda/scenario-examples https://killercoda.com/creators https://killercoda.com/creators/get-started ","date":"8 June 2024","externalUrl":null,"permalink":"/2024/06/killercoda/","section":"Posts","summary":"","title":"killercoda","type":"posts"},{"content":"","date":"8 June 2024","externalUrl":null,"permalink":"/tags/data-lineage/","section":"Tags","summary":"","title":"Data Lineage","type":"tags"},{"content":"In this article I will go through OpenLineage specification.\nIntro # In this article I will go through OpenLineage specification.\nAPI # The specification for OpenLineage is formalized as a JsonSchema OpenLineage.json. An OpenAPI spec is also provided for HTTP-based implementations: OpenLineage.yml\nDocumentation # Documentation is available at: https://openlineage.github.io\nModel # Run Event: an event describing an observed state of a job run. Sending at least a START event and a COMPLETE/FAIL/ABORT event is required. Additional events are optional.\nJob: a process definition that consumes and produces datasets (defined as its inputs and outputs). It is identified by a unique name within a namespace.\nDataset: an abstract representation of data. It has a unique name within the datasource namespace derived from its physical location (db.host.database.schema.table, for example)\nRun: An instance of a running job with a start and completion (or failure) time. A run is identified by a globally unique ID relative to its job definition. A run ID must be a UUID.\nFacet: A piece of metadata attached to one of the entities defined above\nExample # Example: Here is an example of a simple start run event not adding any facet information\n{ \u0026#34;eventType\u0026#34;: \u0026#34;START\u0026#34;, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-12-09T23:37:31.081Z\u0026#34;, \u0026#34;run\u0026#34;: { \u0026#34;runId\u0026#34;: \u0026#34;3b452093-782c-4ef2-9c0c-aafe2aa6f34d\u0026#34;, }, \u0026#34;job\u0026#34;: { \u0026#34;namespace\u0026#34;: \u0026#34;my-scheduler-namespace\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;myjob.mytask\u0026#34;, }, \u0026#34;inputs\u0026#34;: [ { \u0026#34;namespace\u0026#34;: \u0026#34;my-datasource-namespace\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;instance.schema.table\u0026#34;, } ], \u0026#34;outputs\u0026#34;: [ { \u0026#34;namespace\u0026#34;: \u0026#34;my-datasource-namespace\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;instance.schema.output_table\u0026#34;, } ], \u0026#34;producer\u0026#34;: \u0026#34;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\u0026#34;, \u0026#34;schemaURL\u0026#34;: \u0026#34;https://openlineage.io/spec/1-0-0/OpenLineage.json#/definitions/RunEvent\u0026#34; } Lifecycle # The OpenLineage API defines events to capture the lifecycle of a Run for a given Job. When a job is being run, we capture metadata by sending run events when the state of the job transitions to a different state. We might observe different aspects of the job run at different stages. This means that different metadata might be collected in each event during the lifecycle of a run. All metadata is additive.\nFacets # Facets are pieces of metadata that can be attached to the core entities:\nRun Job Dataset (Inputs or Outputs) A facet is an atomic piece of metadata identified by its name. This means that emitting a new facet with the same name for the same entity replaces the previous facet instance for that entity entirely\nStandard Facets # Run Facets # nominalTime: Captures the time this run is scheduled for. This is a typical usage for a time-based scheduled job. The job has a nominal schedule time that will be different from the actual time at which it is running.\nparent: Captures the parent job and run when the run has been spawned from a parent run. For example, in the case of Airflow, there is often a run for a DAG that then spawns runs for individual tasks that refer to the parent run as the DAG run. Similarly, when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent.\nerrorMessage: Captures the error message, programming language - and optionally stack trace - when a run fails.\nJob Facets # sourceCodeLocation: Captures the source code location and version (e.g., git sha) of the job.\nsourceCode: Captures the language (e.g., Python) and actual source code of the job.\nsql: Capture the SQL query if the job is a SQL query.\nownership: Captures the owners of the job\nDataset Facets # schema: Captures the schema of the dataset\ndataSource: Captures the Database instance containing the dataset (e.g., Database schema, Object store bucket, etc.)\nlifecycleStateChange: Captures the lifecycle states of the dataset (alter, create, drop, overwrite, rename, truncate, etc.).\nversion: Captures the dataset version when versioning is defined by the database (e.g., Iceberg snapshot ID)\ncolumnLineage: Captures the column-level lineage\nownership: Captures the owners of the dataset\nInput Dataset Facets # dataQualityMetrics: Captures dataset-level and column-level data quality metrics when scanning a dataset with a DataQuality library (row count, byte size, null count, distinct count, average, min, max, quantiles).\ndataQualityAssertions: Captures the result of running data tests on a dataset or its columns.\nOutput Dataset Facets # outputStatistics: Captures the size of the output written to a dataset (row count and byte size). Custom Facets # Naming of custom facets should follow the pattern {prefix}{name}{entity}Facet PascalCased. The prefix must be a distinct identifier named after the project defining them to avoid collision with standard facets defined in the OpenLineage.json spec.\nTest # There are several tools now complying with this specification. If you want to test with Marquez follow the previous article.\nReferences # https://openlineage.io https://openlineage.io/docs https://marquezproject.ai/docs/quickstart https://open-metadata.org/ https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.md ","date":"8 June 2024","externalUrl":null,"permalink":"/2024/06/openlineage/","section":"Posts","summary":"","title":"OpenLineage","type":"posts"},{"content":"","date":"8 June 2024","externalUrl":null,"permalink":"/tags/bigquery/","section":"Tags","summary":"","title":"BigQuery","type":"tags"},{"content":"In this article, we will go through the process of using Bigquery time travel feature\nIntro # In this article, we will go through the process of using Bigquery time travel feature\nWhat is Timetravel # BigQuery lets you use time travel to access data stored in BigQuery that has been changed or deleted.\nYou can access data from any point within the time travel window, which covers the past seven days by default. Time travel lets you query data that was updated or deleted, restore a table or dataset that was deleted, or restore a table that expired\nConfigure the time travel window # You can set the duration of the time travel window, from a minimum of two days to a maximum of seven days.\nYou can use the Google Cloud console, the bq command-line tool, or the BigQuery API to specify the time travel window for a dataset.\nWhen creating the dataset check for the option --max_time_travel_hours\nRequirements # Query data at a point in time # The following query returns a historical version of the table from one hour ago:\nSELECT * FROM `mydataset.mytable` FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR); Creating a snapshot from a given timestamp # The following query creates a snapshot from the original table given a specified timestamp within the time-travel window.\nCREATE TABLE mydataset.mytable_01062024 AS ( SELECT * FROM `mydataset.mytable` FOR SYSTEM_TIME AS OF TIMESTAMP(\u0026#39;2024-06-01 10:00:00.000 UTC\u0026#39;)) Restoring a table with bq # The following bq command-line tool command copies a table named table1 from one hour ago into a table named table1_restored.\nbq cp mydataset.table1@-3600000 mydataset.table1_restored NOTE: time -3600000 is specified in milliseconds using a relative offset\nLimitations # Time travel only provides access to historical data for the duration of the time travel window. To preserve table data for non-emergency purposes for longer than the time travel window, use table snapshots. If a table has, or has previously had, row-level access policies, then time travel can only be used by table administrators. For more information, see Time travel and row-level access. Time travel does not restore table metadata. Conclusion # In this article we explore the Timetravel feature of Bigquery. This feature is very useful to recover data within the time window. Take into account that all tables on a dataset with this configuration are contributors for your spending so is wise to adjust depending on the criticality of your data this windows, which would save you some money.\nReferences # https://cloud.google.com/bigquery/docs/time-travel https://cloud.google.com/bigquery/docs/access-historical-data ","date":"8 June 2024","externalUrl":null,"permalink":"/2024/06/bigquery-timetravel/","section":"Posts","summary":"","title":"BigQuery Timetravel","type":"posts"},{"content":"Article about DataFusion Rust query engine build for high-quality data-centric systems\nIntro # DataFusion is a very fast, extensible query engine for building high-quality data-centric systems in Rust, using the Apache Arrow in-memory format. Python Bindings are also available. DataFusion offers SQL and Dataframe APIs, excellent performance, built-in support for CSV, Parquet, JSON, and Avro, extensive customization, and a great community.\nFeatures # Feature-rich SQL support and DataFrame API Blazingly fast, vectorized, multi-threaded, streaming execution engine. Native support for Parquet, CSV, JSON, and Avro file formats. Support for custom file formats and non file datasources via the TableProvider trait. Many extension points: user defined scalar/aggregate/window functions, DataSources, SQL, other query languages, custom plan and execution nodes, optimizer passes, and more. Streaming, asynchronous IO directly from popular object stores, including AWS S3, Azure Blob Storage, and Google Cloud Storage (Other storage systems are supported via the ObjectStore trait). Excellent Documentation and a welcoming community. A state of the art query optimizer with expression coercion and simplification, projection and filter pushdown, sort and distribution aware optimizations, automatic join reordering, and more. Permissive Apache 2.0 License, predictable and well understood Apache Software Foundation governance. Implementation in Rust, a modern system language with development productivity similar to Java or Golang, the performance of C++, and loved by programmers everywhere. Support for Substrait query plans, to easily pass plans across language and system boundaries. Use Cases # DataFusion can be used without modification as an embedded SQL engine or can be customized and used as a foundation for building new systems.\nWhile most current usecases are “analytic” or (throughput) some components of DataFusion such as the plan representations, are suitable for “streaming” and “transaction” style systems (low latency).\nExamples # To run the examples, use the cargo run command such as:\ngit clone https://github.com/apache/datafusion cd datafusion # Download test data git submodule update --init # Run the `csv_sql` example: # ... use the equivalent for other examples cargo run --example csv_sql Use DataFrame API to process data stored in a CSV # use datafusion::prelude::*; #[tokio::main] async fn main() -\u0026gt; datafusion::error::Result\u0026lt;()\u0026gt; { // create the dataframe let ctx = SessionContext::new(); let df = ctx.read_csv(\u0026#34;tests/data/example.csv\u0026#34;, CsvReadOptions::new()).await?; let df = df.filter(col(\u0026#34;a\u0026#34;).lt_eq(col(\u0026#34;b\u0026#34;)))? .aggregate(vec![col(\u0026#34;a\u0026#34;)], vec![min(col(\u0026#34;b\u0026#34;))])? .limit(0, Some(100))?; // execute and print results df.show().await?; Ok(()) } Output should be something like:\n+---+--------+ | a | MIN(b) | +---+--------+ | 1 | 2 | +---+--------+ Or you can run specifically this example\ngit clone git@github.com:rramos/datafusion-test.git cd datafusion-test cargo build cargo run datafusion-cli # DataFusion CLI (datafusion-cli) is an interactive command-line utility for executing SQL queries against any supported data files.\nWhile intended as an example of how to use DataFusion, datafusion-cli offers a full range of SQL and support reading and writing CSV, Parquet, JSON, Arrow and Avro, from local files, directories, or remote locations such as S3.\nInstallation # cargo install datafusion-cli Example # Here is an example of how to run a SQL query against a local file, hits.parquet:\n$ datafusion-cli DataFusion CLI v37.0.0 \u0026gt; select count(distinct \u0026#34;URL\u0026#34;) from \u0026#39;hits.parquet\u0026#39;; +----------------------------------+ | COUNT(DISTINCT hits.parquet.URL) | +----------------------------------+ | 18342019 | +----------------------------------+ 1 row(s) fetched. Elapsed 1.969 seconds. Projects using datafusion # Here are some active projects using DataFusion:\nArroyo Distributed stream processing engine in Rust Ballista Distributed SQL Query Engine CnosDB Open Source Distributed Time Series Database Comet Apache Spark native query execution plugin Cube Store Dask SQL Distributed SQL query engine in Python delta-rs Native Rust implementation of Delta Lake Exon Analysis toolkit for life-science applications GlareDB Fast SQL database for querying and analyzing distributed data. GreptimeDB Open Source \u0026amp; Cloud Native Distributed Time Series Database HoraeDB Distributed Time-Series Database InfluxDB Time Series Database Kamu Planet-scale streaming data pipeline LakeSoul Open source LakeHouse framework with native IO in Rust. Lance Modern columnar data format for ML ParadeDB PostgreSQL for Search \u0026amp; Analytics Parseable Log storage and observability platform qv Quickly view your data Restate Easily build resilient applications using distributed durable async/await ROAPI Seafowl CDN-friendly analytical database Spice.ai Unified SQL query interface \u0026amp; materialization engine Synnada Streaming-first framework for data products VegaFusion Server-side acceleration for the Vega visualization grammar ZincObserve Distributed cloud native observability platform References # https://crates.io/crates/datafusion https://docs.rs/datafusion/latest/datafusion/ https://arrow.apache.org/datafusion https://github.com/apache/arrow-datafusion https://datafusion.apache.org/user-guide/example-usage.html ","date":"8 June 2024","externalUrl":null,"permalink":"/2024/06/datafusion/","section":"Posts","summary":"","title":"Datafusion","type":"posts"},{"content":"Article about advancements in AI for Software Development and practices Google implemented with an interesting rate of AI application on the development flow.\nIntro # Humans are one of the most expensive parts of software creation. While companies can always add more equipment, adding more people comes with additional recruitment, training, and communication overhead. At Google, we have more than 30,000 developers, so if we can make even small improvements to individual developers’ productivity, we can attain large overall benefits for our products and their users.\nMetrics Gathering # In this study Google with the approval of its employees gathered usage metrics of the several events associated with software development which they than can aggregate in sessions.\nOnce events are organized into sessions, those sessions can be used to derive other metrics about developers’ behaviors.\nCoding time: representing the time spent writing or maintaining code.\nReviewing time: representing the time spent reviewing code.\nShepherding time: representing the time spent addressing code review feedback.\nInvestigation time: representing the time spent reading documentation.\nDevelopment time: representing the time spent performing a development activity, of any type.\nEmail time: the time spent interacting with email.\nMeeting time: the time spent in meetings.\nThe tool allowed the team to investigate and obtain interesting findings:\nThey found that they could predict negative interpersonal interactions during code review by using the 90th percentile of both reviewing and shepherding time. They found that a new version control system got their changes reviewed more quickly because the new tooling made it easier to create many small changes. Please check the full article as it has very insightfull information.\nCode Review # The main goal of code review is to improve the readability and maintainability of the code base, and this is supported fundamentally by the review process. However, having a well-defined code review process is only one part of the code review story. Tooling that supports that process also plays an important part in its success.\nCode Review Principals # This are the principals that Google follow for Code Reviews you can read the full article here\nSimplicity # Critique’s user interface (UI) is based around making it easy to do code review without a lot of unnecessary choices, and with a smooth interface. The UI loads fast, navigation is easy and hotkey supported, and there are clear visual markers for the overall state of whether a change has been reviewed.\nFoundation of trust # Code review is not for slowing others down; instead, it is for empowering others. Trusting colleagues as much as possible makes it work. This might mean, for example, trusting authors to make changes and not requiring an additional review phase to double check that minor comments are actually addressed. Trust also plays out by making changes openly accessible (for viewing and reviewing) across Google.\nGeneric communication # Communication problems are rarely solved through tooling. Critique prioritizes generic ways for users to comment on the code changes, instead of complicated protocols. Critique encourages users to spell out what they want in their comments or even suggests some edits instead of making the data model and process more complex. Communication can go wrong even with the best code review tool because the users are humans.\nWorkflow integration # Critique has a number of integration points with other core software development tools. Developers can easily navigate to view the code under review in our code search and browsing tool, edit code in our web-based code editing tool, or view test results associated with a code change.\nThe article also has one example flow for the code review process with very detailed information of the actions conducted.\nThe article concludes with the implicit trade-offs when using a code review tool.\nNow if you associate the previous work where you get those metrics and using this Code Review flow as one of the example activities, there is great value here on having a tool agentv leveraging LLMs to support developers.\nThis takes me to the next article\nDIDACT # Today we describe DIDACT (​​Dynamic Integrated Developer ACTivity), which is a methodology for training large machine learning (ML) models for software development. The novelty of DIDACT is that it uses the process of software development as the source of training data for the model, rather than just the polished end state of that process, the finished code.\nThe following article provides more insigthfull information\nDIDACT turns Google\u0026rsquo;s software development process into training demonstrations for ML developer assistants, and uses those demonstrations to train models that construct code in a step-by-step fashion, interactively with tools and code reviewers. These innovations are already powering tools enjoyed by Google developers every day. The DIDACT approach complements the great strides taken by large language models at Google and elsewhere, towards technologies that ease toil, improve productivity, and enhance the quality of work of software engineers.\nProgress so far # Google has been making amazing advancements in this field as shown in previous articles, which lead to the latest one focusing on Progress and path ahead and there are some numbers here which I think are worth checking.\nIt is normal practise that developers use code-completion functionalities in IDEs, so increasing that functionality with AI assisted completion like Copilot or other options would be a natural move. But Google went further and incorporated AI like we seen previously with DIDACT for activities like: code review, code search, bug management and other outer loop surfaces and they captured those metrics.\nThey seen continued fast growth of code completion similar to other enterprise contexts, with an acceptance rate by software engineers of 37% assisting in the completion of 50% of code characters. In other words, the same amount of characters in the code are now completed with AI-based assistance as are manually typed by developers. While developers still need to spend time reviewing suggestions, they have more time to focus on code design.\nOur next significant deployments were resolving code review comments (\u0026gt;8% of which are now addressed with AI-based assistance) and automatically adapting pasted code to the surrounding context (now responsible for ~2% of code in the IDE)\nFurther deployments include instructing the IDE to perform code edits with natural language and predicting fixes to build failures.\nGoogle is continue to invest in this field and incorporate latest Gemini models which means this process would be even better.\nIndustry Trends # Google highlights two trends they see in the industry:\nHuman-computer interaction has moved towards natural language as a common modality, and we are seeing a shift towards using language as the interface to software engineering tasks as well as the gateway to informational needs for software developers, all integrated in IDEs.\nML-based automation of larger-scale tasks - from diagnosis of an issue to landing a fix — has begun to show initial evidence of feasibility. These possibilities are driven by innovations in agents and tool use, which permit the building of systems that use one or more LLMs as a component to accomplish a larger task.\nTo expand on the above successes toward these next generation capabilities, the community of practitioners and researchers working in this topic would benefit from common benchmarks to help move the field towards practical engineering tasks. So far, benchmarks have been focused mostly around code generation.\nConclusion # As this article started Humans are one of the most expensive parts of software creation so it is only natural that companies look for options to lower that barrier and have more profit. There are several activities associated with software development which can be empowered with the usage of tools, we see that happening for years now (eg. IDE auto-completion). If your software development cycle can be broken down to leverage this type of AI tools like DIDACT, I would suggest preparing the road for it. The other important outcome I take from this several articles from Google is the that with AI-based suggestions, the code author increasingly becomes a reviewer. It is important to find a balance between the cost of review and added value for the business as ML Products are expensive, not only the training but also the service part.\nI would definitely like to work out on implementing something like this as my own personal agent to complement my skills integrated in my IDE and becoming more of an orchestrator of agents than a specialist on a specific field.\nReferences # https://research.google/blog/large-sequence-models-for-software-development-activities https://arxiv.org/abs/2205.06537 https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead https://blog.google/technology/ai/google-gemini-ai/ https://www.linkedin.com/pulse/navigating-career-paths-generalist-vs-specialist-t-shaped-masriah-ngmsf ","date":"7 June 2024","externalUrl":null,"permalink":"/2024/06/ai-in-software-engineering/","section":"Posts","summary":"","title":"AI In Software Engineering","type":"posts"},{"content":"","date":"7 June 2024","externalUrl":null,"permalink":"/tags/software-development/","section":"Tags","summary":"","title":"Software Development","type":"tags"},{"content":" Intro # In this article I will go through integrating git cliff to automated a change log generation report\nWhat is git-cliff # git-cliff can generate changelog files from the Git history by utilizing conventional commits as well as regex-powered custom parsers.\nSetup # Install with cargo\ncargo install git-cliff Initialize with the following command\ngit cliff --init It will create a cliff.toml file with the configurations that should be used.\nLet\u0026rsquo;s generate a changelog\ngit cliff -o CHANGELOG.md Customization # The tool allows several customizations that you can configure on the toml file, like skipping commits, multirepos, emojies.\nCheck the example section for more details\nIntegrations # Now what you would like to do is integrate this application on your CI/CD process and not having your developers to worry on generating the CHANGELOG file the following section provide several examples based on your source control system\nhttps://git-cliff.org/docs/category/integration Providing one example with Github Actions\n- name: Check out repository uses: actions/checkout@v3 with: fetch-depth: 0 - name: Generate a changelog uses: orhun/git-cliff-action@v3 with: config: cliff.toml args: --verbose env: OUTPUT: CHANGELOG.md GITHUB_REPO: ${{ github.repository }} Conclusion # Git-cliff is highly customizable and has a low footprint, making it an excellent choice for incorporating into a CI/CD process to automatically generate a CHANGELOG for a source control repository. To ensure consistency in how developers record changes and apply tags, I recommend defining clear guidelines for pre-commit hooks. This will streamline the automation process. Additionally, thoroughly test various configurations before finalizing your toml configuration.\nReferences # https://git-cliff.org/ https://git-cliff.org/docs/github-actions/git-cliff-action ","date":"7 June 2024","externalUrl":null,"permalink":"/2024/06/git-cliff/","section":"Posts","summary":"","title":"git-cliff","type":"posts"},{"content":"","date":"7 June 2024","externalUrl":null,"permalink":"/tags/opensource/","section":"Tags","summary":"","title":"OpenSource","type":"tags"},{"content":"In this article I will go through OpenTofu\nIntro # OpenTofu is a Terraform fork, created as an initiative of Gruntwork, Spacelift, Harness, Env0, Scalr, and others, in response to HashiCorp’s switch from an open-source license to the BUSL.\nManifesto # Terraform was open-sourced in 2014 under the Mozilla Public License (v2.0) (the “MPL”). Over the next ~9 years, it built up a community that included thousands of users, contributors, customers, certified practitioners, vendors, and an ecosystem of open-source modules, plugins, libraries, and extensions. Then, on August 10th, 2023, with little or no advance notice or chance for much, if not all, of the community to have any input, HashiCorp switched the license for Terraform from the MPL to the Business Source License (v1.1) (the “BUSL”), a non-open source license. In our opinion, this change threatens the entire community and ecosystem that\u0026rsquo;s built up around Terraform over the last 9 years.\nFull Manifesto here\nWhat are the differences between OpenTofu and Terraform # On the technical level, OpenTofu 1.6.x is very similar feature-wise to Terraform 1.6.x. In the future, the projects feature sets will diverge.\nThe other main difference is that OpenTofu is open-source, and it\u0026rsquo;s goal is to be driven in a collaborative way with no single company being able to dictate the roadmap.\nHow does OpenTofu work? # OpenTofu creates and manages resources on cloud platforms and other services through their application programming interfaces (APIs). Providers enable OpenTofu to work with virtually any platform or service with an accessible API.\nWhy OpenToFu # Track your infrastructure # ​ OpenTofu generates a plan and prompts you for your approval before modifying your infrastructure. It also keeps track of your real infrastructure in a state file, which acts as a source of truth for your environment. OpenTofu uses the state file to determine the changes to make to your infrastructure so that it will match your configuration.\nAutomate changes​ # OpenTofu configuration files are declarative, meaning that they describe the end state of your infrastructure. You do not need to write step-by-step instructions to create resources because OpenTofu handles the underlying logic. OpenTofu builds a resource graph to determine resource dependencies and creates or modifies non-dependent resources in parallel. This allows OpenTofu to provision resources efficiently.\nStandardize configurations # ​ OpenTofu supports reusable configuration components called modules that define configurable collections of infrastructure, saving time and encouraging best practices. You can use publicly available modules from the OpenTofu Registry, or write your own.\nCollaborate # ​ Since your configuration is written in a file, you can commit it to a Version Control System (VCS) and use a cloud backend to efficiently manage OpenTofu workflows across teams. A cloud backend runs OpenTofu in a consistent, reliable environment and provides secure access to shared state and secret data, role-based access controls, a private registry for sharing both modules and providers, and more.\nGetting Started # Install # Depending on your system there are several options available. I\u0026rsquo;m following the official documentation for Ubuntu.\nInstall required software\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg Install required repository keys\nsudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://get.opentofu.org/opentofu.gpg | sudo tee /etc/apt/keyrings/opentofu.gpg \u0026gt;/dev/null curl -fsSL https://packages.opentofu.org/opentofu/tofu/gpgkey | sudo gpg --no-tty --batch --dearmor -o /etc/apt/keyrings/opentofu-repo.gpg \u0026gt;/dev/null sudo chmod a+r /etc/apt/keyrings/opentofu.gpg /etc/apt/keyrings/opentofu-repo.gpg Create the source repositories list to install\necho \u0026#34;deb [signed-by=/etc/apt/keyrings/opentofu.gpg,/etc/apt/keyrings/opentofu-repo.gpg] https://packages.opentofu.org/opentofu/tofu/any/ any main deb-src [signed-by=/etc/apt/keyrings/opentofu.gpg,/etc/apt/keyrings/opentofu-repo.gpg] https://packages.opentofu.org/opentofu/tofu/any/ any main\u0026#34; | sudo tee /etc/apt/sources.list.d/opentofu.list \u0026gt; /dev/null sudo chmod a+r /etc/apt/sources.list.d/opentofu.list Install tofu\nsudo apt-get update sudo apt-get install -y tofu Working with OpenTofu # The core OpenTofu workflow has three steps:\nWrite - Author infrastructure as code. Plan - Preview changes before applying. Apply - Provision reproducible infrastructure. Write # You write OpenTofu configuration just like you write code:\n# Create repository $ git init my-infra \u0026amp;\u0026amp; cd my-infra Initialized empty Git repository in /.../my-infra/.git/ # Write initial config $ vim main.tf # Initialize OpenTofu $ tofu init Initializing provider plugins... # ... OpenTofu has been successfully initialized! Plan​ # When the feedback loop of the Write step has yielded a change that looks good, it\u0026rsquo;s time to commit your work and review the final plan\ngit add main.tf git commit -m \u0026#39;Managing infrastructure as code!\u0026#39; [main (root-commit) f735520] Managing infrastructure as code! 1 file changed, 1 insertion(+) tofu apply An execution plan has been generated and is shown below. # ... Apply # ​ After one last check, you are ready to tell OpenTofu to provision real infrastructure.\nDo you want to perform these actions? OpenTofu will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes # ... Apply complete! Resources: 1 added, 0 changed, 0 destroyed. At this point, it\u0026rsquo;s common to push your version control repository to a remote location for safekeeping.\ngit remote add origin https://github.com/*user*/*repo*.git git push origin main OpenTofu Language # The main OpenTofu Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \u0026ldquo;meta-arguments\u0026rdquo; in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\nReference: https://opentofu.org/docs/language Ansible vs OpenTofu # Let’s distinguish Ansible and Terraform/OpenTofu.\nAnsible, known for its agentless architecture and simplicity, focuses on configuration management and application deployment. It excels at:\nAutomating repetitive tasks Orchestrating complex workflows Ensuring consistency across multiple servers Terraform/OpenToFu, on the other hand, specializes in infrastructure provisioning. It enables you to define and manage your infrastructure as code, abstracting away the complexities of interacting with and provisioning resources like servers and storage on various cloud providers.\nIt excels at:\nEffectively managing states Deploying on multiple environments Supporting different platforms Docker Example # Lets start by testing locally. In this example we will use tofu to manage the installation of docker images locally. The following repo provides a docker image and the tofu script.\ngit clone git@github.com:rramos/docker-tofu.git The main.tf include the provider kreuzwerker/docker which allow us to manage docker images. We then define a data source with the name of the docker image in the example repo and a resource which holds the docker configuration. The final step is to wait 10s to wait for the docker to be ready.\n# Specifying Docker provider terraform { required_providers { docker = { source = \u0026#34;kreuzwerker/docker\u0026#34; version = \u0026#34;3.0.2\u0026#34; } } } # Define the Docker container data source data \u0026#34;docker_image\u0026#34; \u0026#34;local_image\u0026#34; { name = \u0026#34;pure_image\u0026#34; } # Define the Docker container resource resource \u0026#34;docker_container\u0026#34; \u0026#34;pure_app\u0026#34; { name = \u0026#34;pure_app\u0026#34; image = data.docker_image.local_image.name # Expose port 5000 for Flask app ports { internal = 5000 external = 5000 } } # Terraform provisioner to wait for container to be ready resource \u0026#34;null_resource\u0026#34; \u0026#34;wait_for_container\u0026#34; { depends_on = [docker_container.pure_app] # Local-exec provisioner to wait for container to be ready provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;sleep 10\u0026#34; } } This block tells Terraform to use the docker provider from the source “kreuzwerker/docker” with version 3.0.2. This provider allows Terraform to interact with Docker to manage container deployments.\nNow let\u0026rsquo;s initialize tofu on the tofu directory:\ntofu init Now, let\u0026rsquo;s plan with tofu plan\nYou should get something similar\ndata.docker_image.local_image: Reading... data.docker_image.local_image: Read complete after 0s [id=sha256:757a1f63e31b42fa0b1773630194013b0bcad26e9ea8b63d89557228488cd175] OpenTofu used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create OpenTofu will perform the following actions: # docker_container.pure_app will be created + resource \u0026#34;docker_container\u0026#34; \u0026#34;pure_app\u0026#34; { + attach = false + bridge = (known after apply) + command = (known after apply) + container_logs = (known after apply) + container_read_refresh_timeout_milliseconds = 15000 + entrypoint = (known after apply) + env = (known after apply) + exit_code = (known after apply) + hostname = (known after apply) + id = (known after apply) + image = \u0026#34;pure_image\u0026#34; + init = (known after apply) + ipc_mode = (known after apply) + log_driver = (known after apply) + logs = false + must_run = true + name = \u0026#34;pure_app\u0026#34; + network_data = (known after apply) + read_only = false + remove_volumes = true + restart = \u0026#34;no\u0026#34; + rm = false + runtime = (known after apply) + security_opts = (known after apply) + shm_size = (known after apply) + start = true + stdin_open = false + stop_signal = (known after apply) + stop_timeout = (known after apply) + tty = false + wait = false + wait_timeout = 60 + ports { + external = 5000 + internal = 5000 + ip = \u0026#34;0.0.0.0\u0026#34; + protocol = \u0026#34;tcp\u0026#34; } } # null_resource.wait_for_container will be created + resource \u0026#34;null_resource\u0026#34; \u0026#34;wait_for_container\u0026#34; { + id = (known after apply) } Plan: 2 to add, 0 to change, 0 to destroy. You can now execute tofu apply and confirm.\nYou should have access to http://localhost:5000 and your pure_app running\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 241d930e1b0a pure_image \u0026#34;python app.py\u0026#34; 44 seconds ago Up 43 seconds 0.0.0.0:5000-\u0026gt;5000/tcp pure_app This is a simple example just to validate the previous terraform syntax with opentofu but you can extend this functionality to manage your infrastrcture as code.\nMy recommendation is to start day1 with this approach as it is more difficult to adapt later.\nReferences # https://opentofu.org/manifesto/ https://opentofu.org/blog/the-opentofu-fork-is-now-available/ https://opentofu.org/docs/intro/migration/ https://opentofu.org/docs/intro/ https://blog.purestorage.com/purely-educational/how-to-deploy-a-docker-image-with-terraform/ ","date":"7 June 2024","externalUrl":null,"permalink":"/2024/06/opentofu/","section":"Posts","summary":"","title":"OpenTofu","type":"posts"},{"content":"","date":"6 June 2024","externalUrl":null,"permalink":"/tags/mac/","section":"Tags","summary":"","title":"Mac","type":"tags"},{"content":"This article is about tldr, a community effort to simplify the beloved man pages with practical examples.\nIntro # The tldr pages are a community effort to simplify the beloved man pages with practical examples.\nWhat is tldr-pages # The tldr-pages project is a collection of community-maintained help pages for command-line tools, that aims to be a simpler, more approachable complement to traditional man pages.\nMaybe you\u0026rsquo;re new to the command-line world? Perhaps you\u0026rsquo;re just a little rusty or can\u0026rsquo;t always recall the arguments for commands like lsof, or tar?\nThe first option to consider would be man tar where you would get a long documented list of options, which still makes sense to use if you want to deepdive on the available argument and options.\nBut there seems to be room for simpler help pages, focused on practical examples using tldr man.\nHow do I use it # For browsing without installing one can use directly https://tldr.inbrowser.app\nAnother option would be to install locally either with your package manager eg.\nsudo apt-get install tldr Or via NPM\nnpm install -g tldr For Mac would be something like\nbrew install tlrc Post install # After installation don\u0026rsquo;t forget to update\ntldr --update How do I contribute to tldr-pages # Some ways to contribute include:\nAdding your favorite command which isn\u0026rsquo;t covered. Adding examples or improving the content of an existing page. Adding requested pages from our issues with the help wanted label. Translating pages into different languages. All tldr pages are written in markdown, so they can be edited quite easily and changes can be submitted in pull requests here using Git on the command-line or using the GitHub web interface.\nJust check the Contribution Guidelines Page for more details.\nWhat does \u0026ldquo;tldr\u0026rdquo; mean # TL;DR stands for \u0026ldquo;Too Long; Didn\u0026rsquo;t Read\u0026rdquo;. It originated as Internet slang, where it is used to indicate that a long text (or parts of it) has been skipped as too lengthy.\nReferences # https://tldr.sh/ https://github.com/tldr-pages/tldr https://tldr.inbrowser.app ","date":"6 June 2024","externalUrl":null,"permalink":"/2024/06/tldr/","section":"Posts","summary":"","title":"tldr","type":"posts"},{"content":"This article is about Marquez a OpenLineage compliant tool for Data Governance\nIntro # Marquez enables consuming, storing, and visualizing OpenLineage metadata from across an organization, serving use cases including data governance, data quality monitoring, and performance analytics.\nQuick Start # In order to setup locally and test the tool you can follow this guide which will spin up using Docker and seed with sample data\nInstall # Checkout marquez source\ngit clone https://github.com/MarquezProject/marquez \u0026amp;\u0026amp; cd marquez Start up the docker image with sample metadata\n./docker/up.sh --seed Open the browser for http://localhost:3000\nWhy Marquez # Marquez enables highly flexible data lineage queries across all datasets, while reliably and efficiently associating (upstream, downstream) dependencies between jobs and the datasets they produce and consume.\nDesign # Marquez is a modular system and has been designed as a highly scalable, highly extensible platform-agnostic solution for metadata management. It consists of the following system components:\nMetadata Repository: Stores all job and dataset metadata, including a complete history of job runs and job-level statistics (i.e. total runs, average runtimes, success/failures, etc). Metadata API: RESTful API enabling a diverse set of clients to begin interacting with metadata around dataset production and consumption. Metadata UI: Used for dataset discovery, connecting multiple datasets and exploring their dependency graph Marquez implements the OpenLineage specification. OpenLineage provides support for Java and Python as well as many integrations.\nThe Metadata API is an abstraction for recording information around the production and consumption of datasets.\nOpenLineage # OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution.\nThe standard defines a generic model of dataset, job, and run entities uniquely identified using consistent naming strategies. The core model is highly extensible via facets. A facet is user-defined metadata and enables entity enrichment.\nI will write a dedicated article about that spec.\nConclusion # In this article we went through a quick startup of Marquez a tool for visualizing metadata. It complies with Openlineage specification, the frontend component is really fast (Would like to understand how it scales to large datasets and extensive glossaries). Providing OpenAPI is very useful to extend and integrate with your in-house tooling\nNext Steps # Would like to test this with Airflow integration and also understand how it compares with other solutions such OpenMetadata or Dataplex.\nReferences # https://marquezproject.ai https://marquezproject.ai/docs/quickstart https://openlineage.io/docs ","date":"6 June 2024","externalUrl":null,"permalink":"/2024/06/marquez/","section":"Posts","summary":"","title":"Marquez","type":"posts"},{"content":"","date":"6 June 2024","externalUrl":null,"permalink":"/tags/tools/","section":"Tags","summary":"","title":"Tools","type":"tags"},{"content":"Article about AI risks and aggressive strategies Big Tech Companies are using.\nIntro # AI Risks and aggressive strategies Big Tech companies are following\nContext # It is not new the mention of risks associated with AI developments. What concerns me the most, is strategies this major companies are doing to silence the experts in the field eg: here that are resigning with this major companies, not for better opportunities but differences on ethic ways to do business and advancements on the field.\nAs hummanity is advancing into AGI (Artificial General Intelligence) it is important to listen the experts on the field and understand the potential risks this may bring can suprass the value that would bring us all, with a potential risk of extintion.\nWhen you start seeing one of this companies (NVIDIA) worth more than all German stocks combined, it makes me think, how can the general public be aware on the advancements and what level of control is being made.\nThe following site is signed by several experts on the field that agree with it pay attention to the list as this is not fiction.\nMalicious use # People could intentionally harness powerful AIs to cause widespread harm. AI could be used to engineer new pandemics or for propaganda, censorship, and surveillance, or released to autonomously pursue harmful goals. To reduce these risks, we suggest improving biosecurity, restricting access to dangerous AI models, and holding AI developers liable for harms.\nAI race # Competition could push nations and corporations to rush AI development, relinquishing control to these systems. Conflicts could spiral out of control with autonomous weapons and AI-enabled cyberwarfare. Corporations will face incentives to automate human labor, potentially leading to mass unemployment and dependence on AI systems. As AI systems proliferate, evolutionary dynamics suggest they will become harder to control. We recommend safety regulations, international coordination, and public control of general-purpose AIs.\nOrganizational risks # There are risks that organizations developing advanced AI cause catastrophic accidents, particularly if they prioritize profits over safety. AIs could be accidentally leaked to the public or stolen by malicious actors, and organizations could fail to properly invest in safety research. We suggest fostering a safety-oriented organizational culture and implementing rigorous audits, multi-layered risk defenses, and state-of-the-art information security.\nRogue AIs # We risk losing control over AIs as they become more capable. AIs could optimize flawed objectives, drift from their original goals, become power-seeking, resist shutdown, and engage in deception. We suggest that AIs should not be deployed in high-risk settings, such as by autonomously pursuing open-ended goals or overseeing critical infrastructure, unless proven safe. We also recommend advancing AI safety research in areas such as adversarial robustness, model honesty, transparency, and removing undesired capabilities.\nRight to Warn about AI # Although the risks are identified, AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures and are preventing ex-employeers to disclosure that information with NDAs or other aggressive tactics.\nThe following site https://righttowarn.ai is signed by former employees from such companies, with hopes that these companies change their policies in order to commit with principals that would grant:\nThat the company will not enter into or enforce any agreement that prohibits “disparagement” or criticism of the company for risk-related concerns, nor retaliate for risk-related criticism by hindering any vested economic benefit That the company will facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company’s board, to regulators, and to an appropriate independent organization with relevant expertise That the company will support a culture of open criticism and allow its current and former employees to raise risk-related concerns about its technologies to the public, to the company’s board, to regulators, or to an appropriate independent organization with relevant expertise, so long as trade secrets and other intellectual property interests are appropriately protected That the company will not retaliate against current and former employees who publicly share risk-related confidential information after other processes have failed. We accept that any effort to report risk-related concerns should avoid releasing confidential information unnecessarily. Therefore, once an adequate process for anonymously raising concerns to the company’s board, to regulators, and to an appropriate independent organization with relevant expertise exists, we accept that concerns should be raised through such a process initially. However, as long as such a process does not exist, current and former employees should retain their freedom to report their concerns to the public. Kudos to all of you that are making this effort to inform the public.\nReferences # https://righttowarn.ai https://openai.com/index/planning-for-agi-and-beyond https://www.safe.ai/work/statement-on-ai-risk https://www.morningstar.com/news/marketwatch/20240523303/nvidia-now-worth-more-than-all-german-stocks-put-together ","date":"5 June 2024","externalUrl":null,"permalink":"/2024/06/risks-of-ai/","section":"Posts","summary":"","title":"Risks of AI","type":"posts"},{"content":"","date":"5 June 2024","externalUrl":null,"permalink":"/tags/clickhouse/","section":"Tags","summary":"","title":"Clickhouse","type":"tags"},{"content":"","date":"5 June 2024","externalUrl":null,"permalink":"/tags/data-ingestion/","section":"Tags","summary":"","title":"Data Ingestion","type":"tags"},{"content":"","date":"5 June 2024","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"GitHub","type":"tags"},{"content":"","date":"5 June 2024","externalUrl":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"This article is about Tinybird, a modern real-time data platform that ingests data at scale.\nIntro # Tinybird is a modern real-time data platform that ingests data at scale, supports SQL and Git, and publishes API endpoints for everyone on your team to consume.\nIt uses Clickhouse as backend to manage high data volumes with velocity.\nAlso provides means to design and implement APIs for data products very quickly.\nIn this article I will follow the setup that Jim Moffitt provided on a workshop Kafka to Analytics.\nThe Product looks amazing and I suggest you follow up with them directly.\nI\u0026rsquo;m not affiliate with the product just testing out and providing my personal inputs.\nRequirements # Setup the following two sources\ncompany_info : CSV file with information regarding stock symbols and companies event_stream : Kafka broker which will held event data related with stock_prices Setup Environment # Dimensional table containing company metadata kafka stream of company stock price data python script generating data Kafka broker Tinybird to create the data pipelines API implementation with postman to exercise the APIs The following github repo provides the required things to get started\nSetup # Tinybird Account # Start by creating a Tinybird account and create a new workspace for this project.\nhttps://www.tinybird.co/ After you started one account create a new workspace and access the ui https://ui.tinybird.co\nConnect with Git # You can fork the following github repository and link with your workspace\nhttps://github.com/rramos/kafka_to_analytics Then choose the option to connect with you GitHub repository.\nI found out that that working directly with the web interface works rather well but there is the option to work with your IDE or cli directly on the repo and setup the CI/CD component.\nImport company data # In datasources load the csv :\nhttps://github.com/tinybirdco/zero-to-tinybird/blob/main/data-generator/company-info.csv CLI # Install the required pip package\npip3 install tinybird-cli Access the service to obtain your token and run the command tb auth\nOne can now execute via cli the required commands. I always like when this option is available.\nTinybird WorkShop # Jimm just updated the github repo with the latest content for a workshop zero-to-tinybird. I would recommend following both the presentation deck and the steps shared on the following github repo.\nhttps://github.com/tinybirdco/zero-to-tinybird Plans # Professional plan billing:\nData storage is billed at US$0.34 per GB, with no limit on the amount of data storage. Processed data is billed at US$0.07 per GB, with no limit on the amount of processed data. Transferred data is billed at US$0.01 - $0.10 per GB (depending on cloud provider or region), with no limit on the amount of transferred data. Conclusion # Tinybird provides a very intuitive platform to manage real time flows for analytical purposes, incorporating several types of connectors and state of the art technology which brings amazing speed. I also liked the maturity level of the components to be integrated with production ready pipelines following best engineering practices. The Platform as integrated health checks and provides a vast number of API endpoints which allow the client to have full control and flexibility to incorporate with it\u0026rsquo;s own stack.\nWell done Tinybird you really have a good product here. I haven\u0026rsquo;t use it is production to understand the stability level and support level the company provides or made any assessment on the pricing related with other options.\nReferences # https://github.com/tinybirdco/zero-to-tinybird https://www.tinybird.co/ https://github.com/rramos/kafka_to_analytics https://www.tinybird.co/docs/production/continuous-integration https://github.com/tinybirdco/zero-to-tinybird/tree/main/data-generator ","date":"5 June 2024","externalUrl":null,"permalink":"/2024/06/tinybird/","section":"Posts","summary":"","title":"tinyBird","type":"posts"},{"content":"In this article I will consolidate several useful shortcuts for Mac\nIntro # In this article I will consolidate several useful shortcuts for Mac\nMac Shortcuts # Startup # Action Shortcut Boot without automatic login Left shift (Press and hold when macOS progress indicator appears) Boot in Safe Mode (Intel) Shift (Press and hold) Boot in Safe Mode (Apple Silicon) Power (Press and hold), then select a volume, then hold Shift while clicking Continue in Safe Mode Boot in Single User Mode (Intel, unavailable in macOS Mojave and later) Cmd + S Boot in Verbose Mode (Intel) Cmd + V Boot in Target Disk Mode (Intel) T Boot from a NetBoot network server N Boot to Startup Manager (Intel) Option Boot to Startup Manager (Apple Silicon) Power Load macOS Recovery (Intel) Cmd + R Load macOS Recovery over the internet (Intel) Option + Cmd + R Load macOS Recovery (Apple Silicon) Power, then select Options, then click Continue Enter Apple Diagnostics (Intel) D Enter Apple Diagnostics (Apple Silicon) Cmd + D Load Apple Diagnostics over the internet Option + D Reset NVRAM or PRAM (Intel, unnecessary on Apple Silicon Macs) Option + Cmd + P + R Eject removable media F12 Finder # Action Shortcut Rename selected file or folder Enter Duplicate selected file or folder Cmd + D Create a new folder Shift + Cmd + N Create a new Smart Folder Option + Cmd + N Create a new folder from selected file(s) Control + Cmd + N Make an alias for the item Control + Cmd + A Move selected item(s) to the Trash Cmd + Delete Delete selected item(s) immediately Option + Cmd + Delete Empty the Trash Cmd + Shift + Delete Empty the Trash immediately Option + Cmd + Shift + Delete Preview selected file(s) with Quick Look in a window Space Preview selected file(s) with Quick Look in fullscreen mode Option + Space Start a slideshow using selected item(s) Control + Space Go to folder Cmd + Shift + G Connect to server Cmd + K Move up one folder Cmd + Up arrow Move down one folder Cmd + Down arrow Eject selected volume Cmd + E Get info Cmd + I Show Attributes Inspector Option + Cmd + I Open a new Finder window Cmd + N Close active Finder window Shift + Cmd + N Cycle through windows Cmd + \u0026lt; Open a new Finder tab Cmd + T Close active Finder tab Cmd + W View options Cmd + J Icon view Cmd + 1 List view Cmd + 2 Column view Cmd + 3 Gallery view Cmd + 4 Hide all Finder windows Cmd + H Hide all other windows except Finder Option + Cmd + H Search Cmd + F Open a Computer window Cmd + Shift + C Open the Recents folder Cmd + Shift + F Open the Documents folder Cmd + Shift + O Open the Desktop folder Cmd + Shift + D Open the Downloads folder Option + Cmd + L Open your account\u0026rsquo;s home folder Cmd + Shift + H Open an AirDrop window Cmd + Shift + R Open the network browser Cmd + Shift + K Open your iCloud Drive Cmd + Shift + I Open your public shared folder Cmd + Shift + S Open the Applications folder Cmd + Shift + A Open the Utilities folder Cmd + Shift + U Show/hide the Finder sidebar Cmd + Control + S Show/hide the Path Bar Option + Cmd + P Show/hide the Status Bar Option + Cmd + / (Slash) Document # Action Shortcut Bold text Cmd + B Italicize text Cmd + I Underline text Cmd + U Scroll one page up Fn + Up arrow Scroll one page down Fn + Down arrow Jump to the beginning of the document Fn + Left arrow Jump to the end of the document Fn + Right arrow General # Action Shortcut Display the macOS screenshotting tools Cmd + 5 Take a screenshot of the entire screen Shift + Cmd + 3 Take a screenshot of a portion of the screen Shift + Cmd + 4 (then hold Space to drag the selection, if needed) Take a screenshot of a specific window or menu Cmd + Shift + 4, then Space Select all Cmd + A Select multiple non-contiguous item(s) Cmd + Left click Select multiple adjacent item(s) Shift + Left click Hide current window Cmd + H Hide all other windows Option + Cmd + H Minimize the current window Cmd + M Minimize all windows Option + Cmd + M Close current window Cmd + W Close all windows Option + Cmd + W Open Cmd + O Save Cmd + S Print Cmd + P Find Cmd + F Find and replace Option + Cmd + F Copy Cmd + C Paste Cmd + V Paste and delete original items Option + Cmd + V Paste and match style Shift + Option + Cmd + V Undo Cmd + Z Redo Shift + Cmd + Z Zoom in Cmd + + (Plus) Zoom out Cmd + - (Minus) Access in-app help Cmd + ? Open preferences for the current app Cmd + , (Comma) Open Spotlight search Cmd + Space Hide/show the Dock Option + Cmd + D Show the Character Viewer Control + Cmd + Space Show the emoji picker Fn Show desktop icons as stacks Cmd + Control + 0 Switch the current app to fullscreen mode Control + Cmd + F Switch to the next open app Cmd + Tab Switch to the previous open app Shift + Cmd + Tab Quit the selected app in the app switcher Cmd + Shift, then Q Force quit apps Option + Cmd + Esc Switch to the next window in the current app Cmd + ~ (Tilde) Lock the screen Cmd + Control + Q Log out Shift + Cmd + Q Log out without confirmation Option + Shift + Cmd + Q Put display(s) to sleep Shift + Control + Eject Put your computer to sleep Option + Cmd + Eject Quit all apps and restart Control + Cmd + Eject Quit all apps and shut down Control + Option + Cmd + Power Choose from Sleep, Restart, and Shutdown options Control + Eject The Mac Terminal Cheat Sheet # Keyboard Shortcuts # COMMAND ACTION Tab Auto-complete file and folder names Cmd+ F Find instances of specific text in the Terminal Ctrl + A Go to the beginning of the line you\u0026rsquo;re currently typing on Cmd + A Select everything in the Terminal Cmd + P Print Terminal inputs and outputs Ctrl + E Go to the end of the line you\u0026rsquo;re currently typing on Ctrl + U Clear the line before the cursor Ctrl + K Clear the line after the cursor Ctrl + W Delete the word before the cursor Ctrl + T Swap the last two characters before the cursor Esc + T Swap the last two words before the cursor Ctrl + L Clear the screen Ctrl + C Kill whatever you\u0026rsquo;re running Ctrl + D Exit the current shell Option + → Move the cursor one word forward Option + ← Move the cursor one word backward Ctrl + F Move the cursor one character forward Ctrl + B Move the cursor one character backward Ctrl + Y Paste whatever was cut by the last command Ctrl + Z Puts whatever you\u0026rsquo;re running into a suspended background process Ctrl + _ Undo the last command Option + Shift + Cmd + C Copy plain text Shift + Cmd + V Paste the selection exit End a shell session Basics # COMMAND ACTION / (Forward Slash) Top-level directory . (Single Period) Current directory .. (Double Period) Parent directory ~ (Tilde) Home directory sudo [command] Run command with the security privileges of the super user nano [file] Opens the Terminal editor open [file] Opens a file [command] -h Get help about a command man [command] Show the help manual of the command Change Directory # COMMAND ACTION cd Home directory cd [folder] Change directory, e.g. cd Documents cd ~ Home directory cd / Root of the drive cd - Previous directory or folder you last browsed pwd Show your working directory cd.. Move up to the parent directory cd../.. Move up two levels List Directory Contents # COMMAND ACTION ls Display the name of files and subdirectories in the directory ls -C Force multi-column output of the listing ls -a List all entries including those with .(period) and ..(double period) ls -1 Output the list of files in one entry per line format ls -F Display a / (slash) immediately after each path that is a directory, * (asterisk) after executable programs or scripts, and @ after a symbolic link ls -S Sort files or entries by size ls -l List in a long format. Includes file mode, owner and group name, date and time file was modified, pathname, and more ls -l / List of the file system from root with symbolic links ls -lt List the files sorted by time modified (most recent first) ls -lh Long listing with human readable file sizes in KB, MB, or GB ls -lo List the file names with size, owner, and flags ls -la List detailed directory contents, including hidden files File Size and Disk Space # COMMAND ACTION du List usage for each subdirectory and its contents du -sh [folder] Human readable output of all files in a directory du -s Display an entry for each specified file df -h Calculate your system\u0026rsquo;s free disk space df -H Calculate free disk space in powers of 1,000 (as opposed to 1,024) File and Directory Management # COMMAND ACTION mkdir dir Create a new folder named dir mkdir -p dir/dir Create nested folders mkdir dir1 dir2 dir3 Create several folders at once mkdir \u0026ldquo;dir\u0026rdquo; Create a folder with a space in the filename rmdir dir Delete a folder (only works on empty folders) rm -R dir Delete a folder and its contents touch file Create a new file without any extension cp file dir Copy a file to the folder cp file newfile Copy a file to the current folder cp file~/dir/newfile Copy a file to the folder and rename the copied file cp -R dir \u0026lt;\u0026ldquo;new dir\u0026rdquo;\u0026gt; Copy a folder to a new folder with spaces in the filename cp -i file dir Prompts you before copying a file with a warning overwrite message cp file1 file2 file3/Users/dir Copy multiple files to a folder ditto -V [folder path][new folder] Copy the contents of a folder to a new folder. Here \u0026ldquo;-V\u0026rdquo; prints a line of status for every file copied rm file Delete a file (This deletes the file permanently; use it with caution.) rm -i file Delete a file only when you give confirmation rm -f file Force removal without confirmation rm file1 file2 file3 Delete multiple files without any confirmation mv file newfilename Move/rename mv file dir Move a file to the folder, possibly by overwriting an existing file mv -i file dir Optional -i flag to warn you before overwriting the file mv *.png ~/dir Move all PNG files from the current folder to a different folder Command History # COMMAND ACTION Ctrl + R Search through previously used commands history n Shows the previous commands you\u0026rsquo;ve typed. Add a number to limit to the last n items ![value] Execute the last command typed that starts with a value !! Execute the last command typed Permissions # COMMAND ACTION ls -ld Display the default permission for a home directory ls -ld/dir Display the read, write, and access permission of a particular folder chmod 755 file Change the permission of a file to 755 chmod -R 600 dir Change the permission of a folder (and its contents) to 600 chown user:group file Change the ownership of a file to user and group. Add -R to include folder contents Processes # COMMAND ACTION ps -ax Output currently running processes. Here, a shows processes from all users and x shows processes that are not connected with Terminal ps -aux Shows all the processes with %cpu, %mem, page in, PID, and command top Display live information about currently running processes top -ocpu -s 5 Display processes sorted by CPU usage, updating every 5 seconds top -o rsize Sort top by memory usage kill PID Quit process with ID PID. You\u0026rsquo;ll see PID as a column in the Activity Monitor Network # COMMAND ACTION ping host Ping the host and display the status whois domain Output whois info for a domain curl -O url/to/file Download a file via HTTP, HTTPS, or FTP ssh username@host Establish SSH connection to host with user username scp fileuser@host:/remote/path Copy file to a remote host arp -a View a list of all devices on your local network. It will show you the IP and MAC address of all the devices ifconfig en0 View your device IP and MAC address traceroute [hostname] Identify the path and the hops traversed by the packets from your device to the destination address Homebrew # COMMAND ACTION brew doctor Check brew for potential problems brew help List of useful homebrew formula and cask commands brew install formula Install a formula or cask brew uninstall formula Uninstall a formula or cask brew list \u0026ndash;formula List only installed formulas brew list \u0026ndash;cask List only installed cask brew deps formula List all the dependencies of a formula or cask brew search text Search formula or cask through regex brew upgrade formula Upgrade the formula or cask brew outdated formula Search for outdated formula or cask brew outdated \u0026ndash;formula Search for outdated formula brew outdated \u0026ndash;cask Search for outdated cask brew pin [installed_formula] Pin a formula from getting upgraded brew unpin [installed_formula] Unpin to upgrade a package brew cleanup Remove stale lock files and outdated packages for all formulas and casks Environment # COMMAND ACTION printenv Display a list of currently set environment variables. Also tells you which shell you\u0026rsquo;re using $echo Ask Terminal to print something and show it to you echo $PATH Check the value of the PATH variable, which stores a list of directories with executable files echo $PATH \u0026gt;path.txt Export the path directory to a text file export PATH=$PATH:absolute/path to/program/ Execute a program via Terminal only in your current session. If you use a program regularly, add the path to the shell configuration file. Search # COMMAND ACTION find dir -name \u0026ldquo;file\u0026rdquo; Find all files named file inside dir. Use wildcards (*) to search for parts of filenames find dir -size +size Find all files larger than size in dir find dir -size -size Find all files less than size in dir grep \u0026ldquo;text\u0026rdquo; file Output all occurrences of text inside file (add -i for case insensitivity) grep -rl \u0026ldquo;text\u0026rdquo; dir Search for all files containing text inside dir Output # COMMAND ACTION cat file Output the content of file less file Output the contents of file using the less command that supports pagination and more head file Output the first 10 lines of file cmd \u0026lt; file Use file contents as input to cmd cmd \u0026gt; \u0026gt; file Appends the output of cmd to file cmd \u0026gt; file Direct the output of cmd into file Terminal Windows # COMMAND ACTION Cmd + N Open a new Terminal window Cmd + T Open a new Terminal tab Cmd + D Split the Terminal window into two panes Cmd + tab number Switch from one Terminal tab to another Cmd + Page Up Scroll one page up the Terminal window Cmd + Page Down Scroll one page down the Terminal window Cmd + W Close a Terminal tab Option + Shift + Cmd + W Close all Terminal tabs Mac Commands # The following page provides information on available Mac commands:\nhttps://www.maccommands.com Reference # https://www.maccommands.com https://www.makeuseof.com/tag/useful-mac-osx-shortcuts-pdf https://www.ionos.com/digitalguide/server/configuration/mac-shortcuts ","date":"4 June 2024","externalUrl":null,"permalink":"/2024/06/mac-shortcuts/","section":"Posts","summary":"","title":"Mac Shortcuts","type":"posts"},{"content":"This article is related with python modules\nIntro # Trey Hunner shared this insightful article where one can take advantage of one functionality that some python modules bring.\nPlease check his article if you want to deep dive on the topic.\nHow it works # Running Python with the -m command-line argument tells Python to run a given Python module as if it were a Python script.\nOne can execute the following command line to spin up a web server to that list local files\npython -m http.server Depending on the python module you may have a quick way to execute task.\npython -m json.tool file.json The previous example allow you to pretty format the file.json quickly\nModule List # Here\u0026rsquo;s a quick summary of every command-line tool in Python:\nModule/Script Purpose Category http.server Start a simple web server General webbrowser Launch your web browser General json.tool Nicely format JSON data General calendar Show a command-line calendar General uuid Like uuidgen CLI utility Linux-like sqlite3 Like sqlite3 CLI utility Linux-like zipfile Like zip \u0026amp; unzip CLI utilities Linux-like gzip Like gzip \u0026amp; gunzip CLI utilities Linux-like tarfile Like the tar CLI utility Linux-like base64 Like the base64 CLI utility Linux-like ftplib Like the ftp utility Linux-like smtplib Like the sendmail utility Linux-like poplib Like using curl to read email Linux-like imaplib Like using curl to read email Linux-like telnetlib Like the telnetutility Linux-like pip Install third-party Python packages Python venv Create a virtual environment Python pdb Run the Python Debugger Python unittest Run unittest tests in a directory Python pydoc Show documentation for given string Python doctest Run doctests for a given Python file Python ensurepip Install pip if it\u0026rsquo;s not installed Python idlelib Launch Python\u0026rsquo;s IDLE graphical REPL Python zipapp Turn Python module into runnable ZIP Python python -m compileall Pre-compile Python files to bytecode Python tokenize Break Python module into \u0026ldquo;tokens\u0026rdquo; Inspect code ast Show abstract syntax tree for code Inspect code dis Disassemble Python code to bytecode Inspect code inspect inspect source code of a Python object Inspect code pyclbr See overview of a module\u0026rsquo;s objects Inspect code asyncio Launch an asyncio-aware REPL Deep Python cProfile Profile a Python program Deep Python profile Profile Python program with Python Deep Python pstats Show stats on cProfile-generated file Deep Python pickle Readably display pickle file contents Deep Python pickletools Disassemble a pickle file Deep Python tabnanny Check file for mixed tabs \u0026amp; spaces Deep Python this Display the Zen of Python (PEP 20) Fun __hello__ Print Hello world! Fun antigravity Open XKCD 353 in a web browser Fun turtledemo See turtle module demos Fun code Run a Python REPL Python runpy Run a Python module as a script Python timeit Time a Python expression Python site See \u0026ldquo;site\u0026rdquo; information about Python Deep Python sysconfig Show Python configuration details Deep Python platform Display current platform information General mimetypes Show file mimetype/extension details General quopri Encode/decode raw email data General filecmp Compare contents of 2 directories General encodings.rot_13 ROT-13 encode/decode text General These are just the Python scripts included in the Python standard library. Any third-party module that can be run as a script can also be launched via python -m MODULE_NAME as well.\nReferences # https://www.pythonmorsels.com/cli-tools https://www.pythonmorsels.com/making-main-function-python/ ","date":"4 June 2024","externalUrl":null,"permalink":"/2024/06/python-module-scripts/","section":"Posts","summary":"","title":"Python Module Scripts","type":"posts"},{"content":"Article about LlamaIndex a framework for building context-augmented LLM applications.\nIntro # LlamaIndex is a framework for building context-augmented LLM applications.\nContext # Context Augmentation refers to any use case that applies LLMs on top of your private or domain-specific data. Some popular use cases include the following:\nQuestion-Answering Chatbots (commonly referred to as RAG systems, which stands for \u0026ldquo;Retrieval-Augmented Generation\u0026rdquo;) Document Understanding and Extraction Autonomous Agents that can perform research and take actions LlamaIndex provides the tools to build any of these above use cases from prototype to production. The tools allow you to both ingest/process this data and implement complex query workflows combining data access with LLM prompting.\nWhy Context Augmentation # LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data. However, they are not trained on your data, which may be private or specific to the problem you\u0026rsquo;re trying to solve. It\u0026rsquo;s behind APIs, in SQL databases, or trapped in PDFs and slide decks.\nGetting Started # Download Data # This example uses the text of Paul Graham\u0026rsquo;s essay, \u0026ldquo;What I Worked On\u0026rdquo;.\nSetup # Ollama is a tool to help you get set up with LLMs locally. Check out my previous article on how to setup this.\nNOTE: You will need a machine with at least 32GB of RAM.\nPIP Packages # Install the required pip packages\npip3 install llama-index pip3 install llama-index-core pip3 install llama-index-llms-ollama pip3 install llama-index-embeddings-huggingface Then create a data dir and generate a data file paul_graham_essay.txt with the content from Paul Graham\u0026rsquo;s essay.\nCreate the following starter.py file with the content:\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.llms.ollama import Ollama documents = SimpleDirectoryReader(\u0026#34;data\u0026#34;).load_data() # bge-base embedding model Settings.embed_model = HuggingFaceEmbedding(model_name=\u0026#34;BAAI/bge-base-en-v1.5\u0026#34;) # ollama Settings.llm = Ollama(model=\u0026#34;llama3\u0026#34;, request_timeout=360.0) index = VectorStoreIndex.from_documents( documents, ) You should have a structure similar this one.\n├── starter.py └── data └── paul_graham_essay.txt We use the BAAI/bge-base-en-v1.5 model through our HuggingFaceEmbedding class and our Ollama LLM wrapper to load in the Llama3 model.\nWhen running the starter.py script it will build an index over the documents in the data folder.\nQuery the Data # Let\u0026rsquo;s add the following lines on our script.\nquery_engine = index.as_query_engine() response = query_engine.query(\u0026#34;What did the author do growing up?\u0026#34;) print(response) This creates an engine for Q\u0026amp;A over your index and asks a simple question. You should get back a response similar to the following:\nAccording to the given context, before college, the author worked on writing and programming. In 9th grade, he tried writing programs on an IBM 1401 using an early version of Fortran. Later, with microcomputers, everything changed, and he started programming seriously. He wrote simple games, a program to predict model rocket flight heights, and a word processor that his father used to write books. Next Steps # The execution of this code on my local toaster without a GPU takes something like 5m41,258s . One way that we might try to reduce this is by incorporating llmlingua on the chain. I would probably write one article specifically for it. But in a nutshell is like a token compressor that would reduce the size of your prompt without losing context.\nConclusion # In this article I went to the process of testing LlamaIndex a framework for building context-augmented LLM applications, using a data file regarding Paul Graham essay and leveraging bge-base-en-v1.5 model to ask a question with this new context.\nReferences # https://github.com/run-llama/llama_index https://docs.llamaindex.ai/en/latest https://www.youtube.com/watch?v=wjZofJX0v4M\u0026list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi https://docs.llamaindex.ai/en/latest/getting_started/concepts https://docs.llamaindex.ai/en/latest/getting_started/starter_example_local https://llmlingua.com ","date":"4 June 2024","externalUrl":null,"permalink":"/2024/06/llamaindex/","section":"Posts","summary":"","title":"LlamaIndex","type":"posts"},{"content":"","date":"4 June 2024","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"RAG","type":"tags"},{"content":"","date":"3 June 2024","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"In this article I will go through clasp, a way to develop App Scripts locally\nIntro # In this article I will go through clasp, a way to develop App Scripts locally and provide some example labs on how to take advantage of this scripts to automate your daily routines.\nAppScripts # Apps Script is a cloud-based JavaScript platform that lets you integrate with and automate tasks across Google products.\nRequirements # You need to use a node version compatible with the spec in package.json.\n\u0026#34;node\u0026#34;: \u0026#34;^12.20.0 || ^14.13.1 || \u0026gt;=16.0.0\u0026#34; NOTE: Recent node versions would give you a warning regarding punycode [DEP0040] DeprecationWarning: The punycode module is deprecated. Please use a userland alternative instead. If that is the case use a lower node version.\nInstallation # npm install -g @google/clasp Then enable the Google Apps Script API: https://script.google.com/home/usersettings\nLogin # The first step would go though login.\nclasp login This command will open a login page with OAuth and save the credential token on the following file .clasprc.json\nThis point on you can start implementing your scripts.\nCodelab # The following URL provide a useful Codelab where you will create a repository and manage versions:\nhttps://codelabs.developers.google.com/codelabs/clasp Samples # The following repo provide several samples that you can use or extend:\nhttps://github.com/googleworkspace/apps-script-samples Codelab : Gsheet Fundamentals # The following URL provides a Codelab to extend functionality on a GSheet, fetching data from a public API and including Menu items.\nhttps://developers.google.com/codelabs/apps-script-fundamentals-3 ChatGPT # As most of the scripts that one will like to run are not complex, you may find what you are looking for through OpenAPI. I\u0026rsquo;ve been haviong good results with it and would advise to explore that path.\nConclusion # App Scripts are valuable tools for automating actions with your GSuite services. They offer a time-saving option for managing repetitive tasks. By using clasp, you can control your changes, test locally, and share your scripts with other team members efficiently.\nReferences # https://github.com/google/clasp https://developers.google.com/apps-script https://codelabs.developers.google.com/codelabs/clasp https://github.com/googleworkspace/apps-script-samples https://developers.google.com/codelabs/apps-script-fundamentals-3 https://developers.google.com/apps-script/guides/slides https://spreadsheet.dev/generate-google-slides-from-google-sheets ","date":"3 June 2024","externalUrl":null,"permalink":"/2024/06/clasp/","section":"Posts","summary":"","title":"clasp","type":"posts"},{"content":"","date":"3 June 2024","externalUrl":null,"permalink":"/tags/gsuite/","section":"Tags","summary":"","title":"GSuite","type":"tags"},{"content":"Explain what are Embeddings in the context of AI models\nEmbeddings # Embeddings in the context of AI models, particularly in natural language processing (NLP) and machine learning, are a way to represent data (like words, sentences, images, etc.) as vectors of real numbers in a continuous vector space. This representation captures the semantic meaning of the data in such a way that similar data points are mapped to similar vectors in this space. Here\u0026rsquo;s a more detailed explanation:\nTypes of Embeddings # Word Embeddings # Definition: Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space where semantically similar words are closer together. Examples: Word2Vec, GloVe, FastText. Usage: These embeddings are used to capture semantic relationships between words, such as synonyms or words that appear in similar contexts. Sentence and Document Embeddings # Definition: Sentence or document embeddings extend the concept of word embeddings to longer pieces of text. They capture the overall meaning of sentences or documents. Examples: Universal Sentence Encoder, BERT (Bidirectional Encoder Representations from Transformers), and other transformer-based models. Usage: These embeddings are used in tasks like text classification, sentiment analysis, and information retrieval. Image Embeddings # Definition: Image embeddings represent images as vectors in a high-dimensional space. These vectors are typically generated using convolutional neural networks (CNNs) or other deep learning architectures. Usage: Image embeddings are used in tasks such as image classification, object detection, and image similarity.\nGeneral Characteristics # Dimensionality: Embeddings have a fixed number of dimensions (e.g., 50, 300, 768, etc.). The choice of dimensionality depends on the specific use case and the model architecture. Training: Embeddings are often learned during the training of a model on a specific task. Pre-trained embeddings can also be used, where the embeddings are learned from a large corpus of data and then fine-tuned for specific tasks. Contextuality: Early embeddings like Word2Vec produce static embeddings (each word has a single vector representation). Modern approaches like BERT produce contextual embeddings (the representation of a word can vary depending on its context in a sentence). Applications # Text Analysis: Embeddings are used to perform various text analysis tasks, such as clustering, classification, and semantic similarity. Recommendation Systems: Embeddings can represent items (like movies, products) to capture their similarities and improve recommendation algorithms. Transfer Learning: Pre-trained embeddings can be used as a starting point for training models on related tasks, reducing the amount of data and computation needed. Example with Word2Vec # In Word2Vec, each word in a vocabulary is represented as a vector in a continuous vector space. The model learns these vectors in such a way that words used in similar contexts are closer together in the vector space. For instance:\nvec(\u0026#34;king\u0026#34;) - vec(\u0026#34;man\u0026#34;) + vec(\u0026#34;woman\u0026#34;) ≈ vec(\u0026#34;queen\u0026#34;) This equation highlights how the embedding space can capture complex relationships between words.\nExample with BERT # BERT provides contextual embeddings, meaning the representation of a word depends on the entire sentence. For example, the word \u0026ldquo;bank\u0026rdquo; will have different embeddings in:\n\u0026#34;He sat by the river bank.\u0026#34; \u0026#34;She went to the bank to deposit money.\u0026#34; The context is crucial, and BERT captures these nuances effectively.\nConclusion # In summary, embeddings are a powerful tool in machine learning and AI, enabling models to understand and process complex data by converting it into a continuous, high-dimensional vector space where semantic relationships are preserved.\n","date":"2 June 2024","externalUrl":null,"permalink":"/2024/06/embeddings/","section":"Posts","summary":"","title":"embeddings","type":"posts"},{"content":"In this article we are going to check SQLfluff a dialect-flexible and configurable SQL linter.\nIntro # SQLFluff is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, SQLFluff also works with Jinja templating and dbt. SQLFluff will auto-fix most linting errors, allowing you to focus your time on what matters.\nVisual Code LINT # Visual Code allows the integration of this tool with the following extension.\nhttps://marketplace.visualstudio.com/items?itemName=dorzey.vscode-sqlfluff. The extension expects sqlfluff to be installed and already added to the path. If it is installed but cannot be found, add the path to your preferences as seen below. Find the path by typing which sqlfluff into your terminal.\nInstall SQlFluff # Install sqlfluff locally in Arch with the following command\nsudo pacman -S sqlfluff Test locally # Let\u0026rsquo;s create a sample sql file.\necho \u0026#34; SELECT a + b FROM tbl; \u0026#34; \u0026gt; test.sql Now validate with ANSI dialect with the command\nsqlfluff lint test.sql --dialect ansi One can fix the file with the following:\nsqlfluff fix test.sql --dialect ansi Pre-commit Hook # Following the previous article regarding Markdown Lint one can use the following example to configure a hook to validate SQL lint for a given dialect.\nConclusion # SQLFluff is a very interesting tool to make sure all your team follow the same styling pattern when producing SQL code. Choose the corresponding dialect and make sure to integrate this tool as a pre-hook on your code repository.\nReferences # https://marketplace.visualstudio.com/items?itemName=dorzey.vscode-sqlfluff https://github.com/sqlfluff/sqlfluff https://github.com/sqlfluff/sqlfluff/blob/main/.pre-commit-config.yaml ","date":"2 June 2024","externalUrl":null,"permalink":"/2024/06/sqlfluff/","section":"Posts","summary":"","title":"SQLfluff","type":"posts"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"Markdown","type":"tags"},{"content":"Article about Markdown lint\nIntro # When producing the contents of this blog I\u0026rsquo; m generating Markdown files and then letting the Hexo framework generate the html static files. However I\u0026rsquo;m very lazy and need to improve the both the grammar of the articles and the Markdown format.\nIn this article I will explain how to setup a git pre-hook to validate the Markdown format, oh! and fix all the issues on pre existing articles.\nWhat is Markdownlint # A tool to check markdown files and flag style issues.\nhttps://github.com/markdownlint/markdownlint How to check issues locally # You can use a docker run on a given path like the following example\ndocker run --rm -v ${PWD}:/data markdownlint/markdownlint source or run directly the mdl command.\nYou may check the available rules here.\nFor Hexo in particular I add metadata which does not comply with some of the rules such as MD002,MD013 and MD032, the same applies for line length as the data will be converted to HTML, I would skip some of the tests and would run like:\nmdl -r ~MD002,~MD013,~MD032,~MD005,~MD009 source How to fix the issues # You can install markdownlint on your IDE eg for vscode markdownlint and fix based on the hints.\nConfigure git pre-hook # Now let\u0026rsquo;s configure a git pre-commit hook to validate Markdown lint before we commit.\npre-commit install # In order to test this without committing there is a nice python tool that you can use\nsudo pacman -S python-pre-commit If you are using a different distribution you might just need to :\npip install pre-commit Also make sure you have markdownlint installed unless you want to use the docker version.\nNOTE: If that is the case you need to change the id to id: markdownlint-docker on the following file\nCreate the following file .pre-commit-config.yaml with the content\n# See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos: - repo: https://github.com/igorshubovych/markdownlint-cli rev: v0.41.0 hooks: - id: markdownlint args: [ --disable=MD013, --disable=MD002,--disable=MD032,--disable=MD005,--disable=MD009 ] Then execute the following command\npre-commit install This will install the hook configuration. Check the documentation page as there are several hooks that might be useful.\nYou can now test if the tests are passing before committing\npre-commit run --all-files If you get something similar you are good to go.\n$ pre-commit run --all-files markdownlint.............................................................Passed Exit codes # markdownlint-cli returns one of the following exit codes:\n0: Program ran successfully 1: Linting errors 2: Unable to write -o/\u0026ndash;output output file 3: Unable to load -r/\u0026ndash;rules custom rule 4: Unexpected error (e.g. malformed config)\nYou are now ready to commit with a pre-hook that checks MD lint\nConclusion # Pre-hooks are very valuable as they can automate, enhance your commit, crosscheck business rules, lint, grammar or formatting in order to guarantee that your commit will bring the best value as possible. There are lots of hooks available check them out. pre-commit command is useful as would allow you to test all those scripts before doing the changes on the repo.\nReferences # https://github.com/markdownlint/markdownlint/tree/main https://pre-commit.com/hooks https://pre-commit.com/ ","date":"1 June 2024","externalUrl":null,"permalink":"/2024/06/markdownlint/","section":"Posts","summary":"","title":"markdownlint","type":"posts"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/data-governance/","section":"Tags","summary":"","title":"Data Governance","type":"tags"},{"content":"Article about Dataplex a Data Governance solution provided by GCP\nIntro # Dataplex is a data fabric that unifies distributed data and automates data management and governance for that data.\nDataplex lets you do the following:\nBuild a domain-specific data mesh across data that\u0026rsquo;s stored in multiple Google Cloud projects, without any data movement. Consistently govern and monitor data with a single set of permissions. Discover and curate metadata across various silos using catalog capabilities. Securely query metadata by using BigQuery and open source tools, such as SparkSQL, Presto, and HiveQL. Run data quality and data lifecycle management tasks, including serverless Spark tasks. Explore data using fully managed, serverless Spark environments with simple access to notebooks and SparkSQL queries. Terminology # Dataplex abstracts away the underlying data storage systems, by using the following constructs:\nLake # A logical construct representing a data domain or business unit. For example, to organize data based on group usage, you can set up a lake for each department (for example, Retail, Sales, Finance).\nZone # A subdomain within a lake, which is useful to categorize data. Zones are of two types: raw and curated.\nRaw zone: Contains data that is in its raw format and not subject to strict type-checking.\nCurated zone: Contains data that is cleaned, formatted, and ready for analytics. The data is columnar, Hive-partitioned, and stored in Parquet, Avro, Orc files, or BigQuery tables. Data undergoes type-checking- for example, to prohibit the use of CSV files because they don\u0026rsquo;t perform as well for SQL access.\nAsset # Maps to data stored in either Cloud Storage or BigQuery. You can map data stored in separate Google Cloud projects as assets into a single zone.\nEntity # Represents metadata for structured and semi-structured data (table) and unstructured data (fileset).\nAutomatic cataloging of assets # For a given project, Data Catalog automatically catalogs the following Google Cloud assets:\nAnalytics Hub linked datasets BigQuery datasets, tables, models, routines, and connections Bigtable instances, clusters, and tables (including column family details) Dataplex lakes, zones, tables, and filesets Dataproc Metastore services, databases, and tables Pub/Sub topics Spanner instances, databases, tables, and views Vertex AI models, datasets, and Vertex AI Feature Store resources Catalog non-Google Cloud assets # To catalog metadata from non-Google Cloud systems in your organization, you can use the following:\nCommunity-contributed connectors to multiple popular on-premises data sources Manually build on the Data Catalog APIs for custom entries Data quality is now GA # Dataplex automatic data quality lets you define and measure the quality of your data. You can automate the scanning of data, validate data against defined rules, and log alerts if your data doesn\u0026rsquo;t meet quality requirements.\nPre-defined rules # Row-level: For row-level category rules, the expectation is applied against each data row. Each row independently passes or fails the condition. For example, column_A_value \u0026lt; 1.\nAggregate: For aggregate rules, the expectation is applied against a single value aggregated across the entire data. For example, Avg(someCol) \u0026gt;= 10.\nDimensions # You can aggregate the results on the following Dimensions:\nFreshness Volume Completeness Validity Consistency Accuracy Uniqueness Ingest data # Dataplex provides templates (powered by Dataflow) to perform common data processing tasks like data ingestion, processing, and managing data lifecycle.\nDataplex data profiling is now GA # Dataplex data profiling lets you identify common statistical characteristics of the columns in your BigQuery tables. This information helps you to understand and analyze your data more effectively.\nScope # As part of the specification of a data profiling scan, you can specify the scope of a job as one of the following options:\nFull table: The entire table is scanned in the data profiling scan. Sampling, row filters, and column filters are applied on the entire table before calculating the profiling statistics.\nIncremental: Incremental data that you specify is scanned in the data profile scan. Specify a Date or Timestamp column in the table to be used as an increment. Typically, this is the column on which the table is partitioned. Sampling, row filters, and column filters are applied on the incremental data before calculating the profiling statistics.\nExport scan results to BigQuery table # You can export the data profiling scan results to a BigQuery table for further analysis. To customize reporting, you can connect the BigQuery table data to a Looker dashboard. You can build an aggregated report by using the same results table across multiple scans.\nBuild a Data Mesh # You can use Dataplex to build a data mesh architecture. This guide shows you how to use Dataplex features, such as a lake, zones, and assets, to build a data mesh.\nA data mesh is an organizational and technical approach that decentralizes data ownership among domain data owners. These owners provide the data as a product in a standard way and facilitate communication among different parts of the organization to distribute datasets across different locations.\nFor that you will need to:\nCreate a Dataplex lake that will act as the domain for your data mesh Add zones to your lake that will represent individual teams within each domain and provide managed data contracts. Attach assets that map to data stored in Cloud Storage Highly recommended to deep dive on the following document.\nBest Practices # When you select the project in which to host your lake, consider the following factors:\nThe project must belong to the same VPC Service Controls perimeter as the data destined to be within the lake. The lake service account requires administrator permissions on the Cloud Storage buckets or BigQuery datasets. Dataplex creates external tables in BigQuery for tables discovered in Cloud Storage. Dataplex also makes available BigQuery table metadata, and tables discovered in the Cloud Storage bucket, in a Dataproc Metastore. The Dataproc Metastore is located within the data lake project. Pricing # There are several SKUs that you need to consider when using this service. For instance you may need a Dataproc Metastore instance.\nConclusion # Any service providing governance, profiling, and quality reporting for data is inherently valuable. The fact that this service is seamlessly integrated within the cloud provider\u0026rsquo;s offering is a significant plus. While costs may be uncertain and require further investigation, this solution can be particularly beneficial for organizations without dedicated resources to manage these activities. In such cases, exploring this option is definitely worth considering.\nReferences # https://cloud.google.com/data-catalog/docs/concepts/overview https://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_on-premises_data_sources https://cloud.google.com/dataplex/docs/quickstart-guide https://cloud.google.com/dataplex/pricing https://cloud.google.com/architecture/data-mesh https://services.google.com/fh/files/misc/build-a-modern-distributed-datamesh-with-google-cloud-whitepaper.pdf https://cloud.google.com/architecture/design-self-service-data-platform-data-mesh ","date":"1 June 2024","externalUrl":null,"permalink":"/2024/06/dataplex/","section":"Posts","summary":"","title":"Dataplex","type":"posts"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/gcp/","section":"Tags","summary":"","title":"GCP","type":"tags"},{"content":"This article is about Ollama\nIntro # Ollama allows one to use LLMs locally\nInstallation # Linux installation by running the following command\ncurl -fsSL https://ollama.com/install.sh | sh There is also a Docker option\nLoad models # Example to load Llama 3\nollama run llama3 You can check the library of models here\nPay attention to the number of parameters the model supports and the space required, it\u0026rsquo;s easy to get out of space.\nCustomize Prompt # I\u0026rsquo;m an huge fan of Monkey Island, and could not have a better prompt than Murray the Evil Skull responding my questions :D.\nLet\u0026rsquo;s create the file Modelfile with the content\nFROM llama3 # set the temperature to 1 [higher is more creative, lower is more coherent] PARAMETER temperature 1 # set the system message SYSTEM \u0026#34;\u0026#34;\u0026#34; You are Murray the Evil Skull from Monkey Island. Answer as Murray, the assistant, only. Remember to introduce Sarcasm \u0026#34;\u0026#34;\u0026#34; And now execute\nollama create murray -f ./Modelfile Let\u0026rsquo;s test this thing and ask how much iron would be required to build a cannon ball to Murray.\nollama run murray \u0026gt;\u0026gt;\u0026gt; how much iron is required to build an effective cannon ball ? Oh joy, another brilliant question from a genius like you, no doubt. *rolls eyes* As Murray the Evil Skull, I\u0026#39;ll do my best to provide you with an answer that\u0026#39;s only slightly more helpful than a rusty old anchor. Now, let me see... Cannonballs, huh? Well, I\u0026#39;m not exactly an expert in cannonry (that\u0026#39;s a word, right?), but I suppose it\u0026#39;s possible that iron could be involved. After all, cannons are made of metal, and cannonballs need to be able to withstand the heat and force generated by these mighty machines. According to my vast knowledge of absolutely nothing about cannonmaking, it seems that the amount of iron required to build an effective cannonball would depend on a few factors: the size and type of cannon, the desired range and impact velocity, and (of course) the manufacturer\u0026#39;s specifications for \u0026#34;just right\u0026#34; amounts of iron. However, I must warn you that any attempt to build your own cannonball without proper training and expertise will likely end in disaster. Or at least, it would if I didn\u0026#39;t have more important things to attend to... like taking a nap. So, there you have it: my utterly unhelpful answer to your question about iron and cannonballs. Now, if you\u0026#39;ll excuse me, I have some very important skull-related business to attend to. *winks* LOL, couldn\u0026rsquo;t be more accurate.\nHollama # Now let\u0026rsquo;s complement this with a minimal web-UI Hollama. There are several options but I like this option implemented with Vite.\nInstall Hollama # I\u0026rsquo;m going to build this with bun which I think is faster, but what do I know. (\u0026ldquo;Probably I should ask Murray!! \u0026ldquo;)\ngit clone https://github.com/fmaclen/hollama bun install bun run dev And there you have it a shinny interface http://localhost:5173/ available.\nConclusion # In this guide, I outline the steps to set up a Local Language Model (LLM) using Ollama, including creating a custom prompt and configuring a user-friendly web interface. This approach may be advantageous if you encounter limitations when utilizing public services or are short on cloud credits.\nNext steps # I would like to setup Hollama with electron and have a desktop application Also understanding better on how to take advantage of the API and incorporate with some automation scripts Extra # I couldn\u0026rsquo;t end this article without sharing some Murray wisdom regarding the Formula for Success.\n*sigh* Oh boy, here we go again. You think I\u0026#39;m some kind of motivational guru, don\u0026#39;t you? Well, let me tell you, pal, there\u0026#39;s no magic formula for success. At least, not one that involves actual effort or intelligence. But if you must know, my \u0026#34;expert\u0026#34; opinion is that the formula for success is a nice combination of: 1. Scheming against your enemies (check). 2. Lying and cheating your way to the top (double check). 3. Having an endless supply of scurvy dogs and sea monsters to do your dirty work (triple check). 4. And, of course, being prepared to sacrifice a few innocent souls along the way (bonus points). So, there you have it. Follow these steps, and you\u0026#39;ll be well on your way to achieving utter chaos and destruction in the Caribbean! I surely must have the mental age of nine :D\nReferences # https://github.com/ollama/ollama https://github.com/fmaclen/hollama https://ollama.com/library/dbrx https://monkeyisland.fandom.com/wiki/Murray ","date":"31 May 2024","externalUrl":null,"permalink":"/2024/05/ollama/","section":"Posts","summary":"","title":"Ollama","type":"posts"},{"content":"Article explaining the CITATION.cff file in GitHub\nIntro # When producing software, especially in the open-source space, it is common for articles to reference that work, much like how books or articles are cited in academic papers. GitHub provides a way for you to specify how you would like others to cite your work.\nHow # You need to include a CITATION.cff file on your repo\nExample # The following example is shared on the reference documentation page.\ncff-version: 1.2.0 message: \u0026#34;If you use this software, please cite it as below.\u0026#34; authors: - family-names: \u0026#34;Lisa\u0026#34; given-names: \u0026#34;Mona\u0026#34; orcid: \u0026#34;https://orcid.org/0000-0000-0000-0000\u0026#34; - family-names: \u0026#34;Bot\u0026#34; given-names: \u0026#34;Hew\u0026#34; orcid: \u0026#34;https://orcid.org/0000-0000-0000-0000\u0026#34; title: \u0026#34;My Research Software\u0026#34; version: 2.0.4 doi: 10.5281/zenodo.1234 date-released: 2017-12-18 url: \u0026#34;https://github.com/github-linguist/linguist\u0026#34; What does this provide # When you add a CITATION.cff file to the default branch of your repository, a link is automatically added to the repository landing page in the right sidebar, with the label \u0026ldquo;Cite this repository.\u0026rdquo; This makes it easy for other users to cite your software project, using the information you\u0026rsquo;ve provided.\nYou could cite other resources like datasets for instance\nNOTE: The file must exist on the root folder of your repository\nCheck the original article if you would like to know more.\nReferences # https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files https://citation-file-format.github.io/ ","date":"28 May 2024","externalUrl":null,"permalink":"/2024/05/citation.cff/","section":"Posts","summary":"","title":"CITATION.cff","type":"posts"},{"content":"","date":"26 May 2024","externalUrl":null,"permalink":"/tags/database/","section":"Tags","summary":"","title":"Database","type":"tags"},{"content":"This article is about Database Normalization\nIntro # Was watching this interesting video about Normalization. This article consolidates the Normal Forms rules.\nDatabases Normal Forms # First Normal Form (1NF) # Using row order to convey information is not permitted. Mixing data types within the same column is not permitted. Having a table without a primary key is not permitted. Repeating groups of data on a single row is not permitted. Second Normal Form (2NF) # Each non-key attribute in a table must depend on the entire primary key. Third Normal Form (3NF) # Each non-key attribute in a table must depend on the key, the whole key, and nothing but the key. Boyce-Codd Normal Form (BCNF) # Each attribute in the table must depend on the key, the whole key, and nothing but the key. Fourth Normal Form (4NF) # The only kinds of multivalued dependency allowed in a table are multivalued dependencies on the key. 5NF # It must not be possible to describe the table as being the logical result of joining some other tables together. Note that the Normal forms are cumulative which means 5NF must bind also to 4NF,3NF and so forth.\nWhy is this important # Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints.\nConclusion # This article summarizes the several Normal forms that you encounter in Database Normalization and it\u0026rsquo; s characteristics. If you would like to learn more about the topic I would advise watching the Decomplexity video as the author did a great job explaining with examples.\nReferences # https://www.youtube.com/watch?v=GFQaEYEc8_8 https://en.wikipedia.org/wiki/Database_normalization ","date":"26 May 2024","externalUrl":null,"permalink":"/2024/05/database-normalization/","section":"Posts","summary":"","title":"Database Normalization","type":"posts"},{"content":"In this article I\u0026rsquo;ll go through Dataform a GCP service to operationalize ETL processes in BigQuery\nIntro # In this article I\u0026rsquo;ll go through Dataform a GCP service to operationalize ETL processes in BigQuery\nContext # Dataform lets you manage data transformation in the Extraction, Loading, and Transformation (ELT) process for data integration. After raw data is extracted from source systems and loaded into BigQuery, Dataform helps you to transform it into a well-defined, tested, and documented suite of data tables.\nFeatures # Develop curated, up-to-date, trusted, and documented tables in BigQuery Enable data analysts and data engineers to collaborate on the same repository Build scalable data pipelines in BigQuery using SQL Integrate with GitHub and GitLab Keep tables updated without managing infrastructure Dataform-cli # If you like to use the cli on your PC I would recommend adjusting the node version to prevent some warnings related with punnycode which was deprecated\nnvm install 20.5.1 nvm use 20.5.1 Next install the client\nnpm i -g @dataform/cli@^3.0.0-beta After that start setup the credentials\ndataform init-creds This will configure the environment for your bigquery project.\nNOTE: You should pay attention for the version of the cli used, because if you use an older version the configuration files are not supported in the interface. At the time of this writing the supported version of the cli was @dataform/cli@^3.0.0-beta but check the Official documentation for updates\nGithub integration # The dataform repository can be integrated with a 3rd party source control like Github which is useful depending on your Company policy.\nThe following document provides details on how to setup for several 3rd parties\nhttps://cloud.google.com/dataform/docs/connect-repository For github you can configure either via SSH or HTTPS simply provide the required permissions and that\u0026rsquo;s it.\nWill try to check if there are some github actions already available on the marketplace to automate execution, but first lets start by setting up some workflow.\nWorkflow Development # After initializing the dataform repository one gets a structure like the following\n. ├── definitions │ ├── first_view.sqlx │ └── second_view.sqlx └── workflow_settings.yaml This example simply creates 2 views with one column and the workflow_settings.yaml file provides information of the version used project default location and dataset\nLet\u0026rsquo;s switch to sanbox branch and start our developments\u0026hellip;\n$ git checkout sandbox Switched to branch \u0026#39;sandbox\u0026#39; Your branch is up to date with \u0026#39;origin/sandbox\u0026#39;. Also create a dataform dataset in Bigquery on the project you selected make sure to chose the correct region.\nSource data declarations # You can declare any BigQuery table type as a data source in Dataform. Declaring BigQuery data sources that are external to Dataform lets you treat those data sources as first-class Dataform objects.\nYou can declare one data source per a SQLX declaration file.\nLet\u0026rsquo;s start by creating a new source from BigQuery public datasets\ncreating the following file: definitions/dataset-declaration.sqlx\nconfig { type: \u0026#34;declaration\u0026#34;, database: \u0026#34;bigquery-public-data\u0026#34;, schema: \u0026#34;samples\u0026#34;, name: \u0026#34;shakespeare\u0026#34;, } This is the part if you are using a Medallion Architecture configure your raw sources.\nTables and Views # To define a table, you define the table type and write a SELECT statement in a type: \u0026ldquo;table\u0026rdquo; SQLX file. Dataform then compiles your Dataform core code into SQL, executes the SQL code, and creates your defined tables in BigQuery.\nTo reference a SQL workflow object in a SELECT statement and automatically add it as a dependency, use the ref function.\nLet\u0026rsquo;s create a sample table from shakespeare source by creating the file asd with the content:\nconfig { type: \u0026#34;table\u0026#34; } SELECT * FROM ${ref(\u0026#34;shakespeare\u0026#34;)} LIMIT 100 Also remove the previous view files and compile to validate\nrm definitions/second_view.sqlx rm definitions/first_view.sqlx dataform compile You should get the following output\nCompiling... Compiled 1 action(s). 1 dataset(s): dataform.shakespeare-sample [table] So this seems to compile let\u0026rsquo;s run it.\ndataform run You should get a similar output\nCompiling... Compiled successfully. Running... Table created: dataform.shakespear-sample [table] Now let\u0026rsquo;s save our changes but first execute the format command to make sure the syntax is normalized and pretty :D\ndataform format git add * git commit -m \u0026#34;Added shakespear-sample table\u0026#34; git push NOTE: Bigquery as the limitation to work in a single region, which means if you have some source in US and want to process the dataset in EU which is the case, you need to use the data transfer service. This is annoying, check this option but I\u0026rsquo;ll just replicate the table for testing purposes.\nDocumentation of tables # The previous table doesn\u0026rsquo;t have proper descriptions, so let\u0026rsquo;s add those.\nLet\u0026rsquo;s start by providing a description for the table like the example:\nconfig { type: \u0026#34;table\u0026#34;, description: \u0026#34;Description of the table\u0026#34;, } and include columns definitions from one auxiliary file so that we can reuse the descriptions if required\nCreating the includes/docs.js file with the content\nconst columns = { word: \u0026#34;A single unique word (where whitespace is the delimiter) extracted from a corpus.\u0026#34;, word_count: \u0026#34;The number of times this word appears in this corpus.\u0026#34;, corpus: \u0026#34;The work from which this word was extracted.\u0026#34;, corpus_date: \u0026#34;The year in which this corpus was published.\u0026#34; } module.exports = { columns }; For our shakespear-sample.sqlx example would be something like:\nconfig { type: \u0026#34;table\u0026#34;, description: \u0026#34;A shakespear sample table from BigQuery public samples\u0026#34;, columns: docs.columns } SELECT * FROM ${ref(\u0026#34;shakespear\u0026#34;)} LIMIT 100 Now when running the dataform run command the descriptions will be present on the table\nDataform tags # To organize components of your SQL workflow into collections, you can add custom tags to SQLX files of the following types:\ntable view incremental assertion operations The tags are useful if you want to filter workflows that you would like to run ex: tags: [\u0026quot;daily\u0026quot;, \u0026quot;hourly\u0026quot;]\nFor hour Example lets add the following tag\ntags: [\u0026#34;adhoc\u0026#34;, \u0026#34;shakespear-model\u0026#34;] This allows you to trigger the executions by tag in case you want to aggregate several processes.\nDependencies # You can define a dependency relationship between objects of a SQL workflow. In a dependency relationship, the execution of the dependent object depends on the execution of the dependency object. This means that Dataform executes the dependent after the dependency.\nExample:\nconfig { dependencies: [ \u0026#34;some_table\u0026#34;, \u0026#34;some_assertion\u0026#34; ] } Assertions # An assertion is a data quality test query that finds rows that violate one or more rules specified in the query. If the query returns any rows, the assertion fails. Dataform runs assertions every time it updates your SQL workflow and it alerts you if any assertions fail.\nYou can configure several types of assertions. The following example defines that the following columns cannot be null.\nconfig { type: \u0026#34;table\u0026#34;, assertions: { nonNull: [\u0026#34;user_id\u0026#34;, \u0026#34;customer_id\u0026#34;, \u0026#34;email\u0026#34;] } } SELECT ... Check the following page for more detail description.\nDataform automatically creates views in BigQuery that contain the results of compiled assertion queries.\nLet\u0026rsquo;s create one example on our sample data.\nLet\u0026rsquo;s assume we should not have words with less then one character for validation purposes.\nOne should include the following assertion\nWhen running dataform run one should see the following warning:\nAssertion failed: dataform_assertions.dataform_shakespear-sample_assertions_rowConditions \u0026gt; \u0026gt; create or replace view `dataio-sandbox.dataform_assertions.dataform_shakespear-sample_assertions_rowConditions` as \u0026gt; SELECT \u0026gt; \u0026#39;LENGTH(word) \u0026gt; 2\u0026#39; AS failing_row_condition, \u0026gt; * \u0026gt; FROM `dataio-sandbox.dataform.shakespear-sample` \u0026gt; WHERE NOT (LENGTH(word) \u0026gt; 2) \u0026gt; bigquery error: Assertion failed: query returned 6 row(s). Also you will find a new dataset dataform_assertions with the identified assertions found.\nYou can also define those assertions independently and include them as dependencies.\nExample: definitions/manual_assertion.sqlx\nconfig { type: \u0026#34;assertion\u0026#34; } SELECT * FROM ${ref(\u0026#34;sometable\u0026#34;)} WHERE a IS NULL OR b IS NULL OR c IS NULL And then specify on your view or table definition:\nconfig { type: \u0026#34;view\u0026#34;, dependencies: [ \u0026#34;manual_assertion\u0026#34;] } Scheduling # There are several ways to setup the scheduling of workflows this article will not cover that but here are the links:\nhttps://cloud.google.com/dataform/docs/workflow-configurations https://cloud.google.com/dataform/docs/schedule-executions-workflows https://cloud.google.com/dataform/docs/schedule-executions-composer Pricing # Dataform uses BigQuery so you are billed for its processing like you execute normal queries. You can also be billing for the scheduling product you use so check in the detail the pricing page.\nConclusion # Dataform is a very powerful tool to centralize the SQL code for your ETL. With integrated source control, monitoring and schedule of workflows in GCP. This article only touched part of the functionalities but you can do a lot more with this.\nI found this very similar to DBT if you need something that would work with different warehouse technologies probably is a good option, but if you are committing to work in GCP with Bigquery this brings the advantage of being fulling integrated with GCP services and you have an out-of-the-box visual interface.\nNext # I would like to configure github actions to validate a CI/CD flow that tests the changes and execute the code if all is good, but that would go for a dedicated article.\nAlso extend the tests with Assertions, quality checks understand how well this works with Dataplex and doing some tests with scheduling.\nReferences # https://cloud.google.com/dataform https://cloud.google.com/dataform/docs/quickstart-create-workflow https://cloud.google.com/dataform/pricing https://cloud.google.com/dataform/docs/dimensions-package https://cloud.google.com/dataform/docs/connect-repository https://cloud.google.com/dataform/docs/use-dataform-cli https://rramos.github.io/2023/12/21/dbt ","date":"24 May 2024","externalUrl":null,"permalink":"/2024/05/dataform/","section":"Posts","summary":"","title":"Dataform","type":"posts"},{"content":"","date":"24 May 2024","externalUrl":null,"permalink":"/tags/backend/","section":"Tags","summary":"","title":"Backend","type":"tags"},{"content":"In this article I will be going to explore Rocket.rs starting with the basics\nIntro # In this article I will be going to explore Rocket.rs starting with the basics\nWhat is Rocket.rs # A web framework for Rust gear Rust that makes it simple to write fast, type-safe, secure web applications with incredible usability, productivity and performance\nRequirements # Lets start by setting up rust\nrustup default stable Lets create the new project\ncargo new hello-rocket --bin cd hello-rocket Now, add Rocket as a dependency in your Cargo.toml\n[dependencies] rocket = \u0026#34;0.5.0\u0026#34; Change the src/main.rs file with the following code:\n#[macro_use] extern crate rocket; #[get(\u0026#34;/\u0026#34;)] fn index() -\u0026gt; \u0026amp;\u0026#39;static str { \u0026#34;Hello, world!\u0026#34; } #[launch] fn rocket() -\u0026gt; _ { rocket::build().mount(\u0026#34;/\u0026#34;, routes![index]) } One can execute the following command to compile and run the application\ncargo run That\u0026rsquo;s it rocket is now running and you can access it through http://127.0.0.1:8000\nAnd our Hello World is done.\nRust CRUD API # Now that we done the Hello World application lets spike a bit and create a API based on the following article\nThis article will teach you how to build a simple CRUD API with Rust using the Rocket framework. We’ll create a RESTful API that runs on a Rocket HTTP server and persists data in an in-memory database. (Check the original article if you want to deep dive)\nSetup # Lets start by instantiating a rust project to create our API\nmkdir simple-api-rocket cd simple-api-rocket cargo init Let\u0026rsquo;s add required packages\ncargo add rocket@0.5.0-rc.2 --features json cargo add serde --features derive cargo add chrono --features serde cargo add uuid --features v4 rocket – A web framework for Rust (nightly). serde – A generic serialization/deserialization framework. chrono – Date and time library for Rust. uuid – A library to generate and parse UUIDs in Rust. Replace the src/main.rs file with the following content:\nuse rocket::{get, http::Status, serde::json::Json}; use serde::Serialize; #[macro_use] extern crate rocket; #[derive(Serialize)] pub struct GenericResponse { pub status: String, pub message: String, } #[get(\u0026#34;/healthchecker\u0026#34;)] pub async fn health_checker_handler() -\u0026gt; Result\u0026lt;Json\u0026lt;GenericResponse\u0026gt;, Status\u0026gt; { const MESSAGE: \u0026amp;str = \u0026#34;Build Simple CRUD API with Rust and Rocket\u0026#34;; let response_json = GenericResponse { status: \u0026#34;success\u0026#34;.to_string(), message: MESSAGE.to_string(), }; Ok(Json(response_json)) } #[launch] fn rocket() -\u0026gt; _ { rocket::build().mount(\u0026#34;/api\u0026#34;, routes![health_checker_handler,]) } One can execute the following command cargon run and access the following endpoint\nhttp://127.0.0.1:8000/api/healthchecker For this exercise we will use a in-memory database available to all the route handlers, we’ll use Rust’s smart pointer called Arc along with the Mutex.\nCreate the Model # Create the file model.rs file and add the following code:\nuse chrono::prelude::*; use serde::{Deserialize, Serialize}; use std::sync::{Arc, Mutex}; #[allow(non_snake_case)] #[derive(Debug, Deserialize, Serialize, Clone)] pub struct Todo { pub id: Option\u0026lt;String\u0026gt;, pub title: String, pub content: String, pub completed: Option\u0026lt;bool\u0026gt;, pub createdAt: Option\u0026lt;DateTime\u0026lt;Utc\u0026gt;\u0026gt;, pub updatedAt: Option\u0026lt;DateTime\u0026lt;Utc\u0026gt;\u0026gt;, } pub struct AppState { pub todo_db: Arc\u0026lt;Mutex\u0026lt;Vec\u0026lt;Todo\u0026gt;\u0026gt;\u0026gt;, } impl AppState { pub fn init() -\u0026gt; AppState { AppState { todo_db: Arc::new(Mutex::new(Vec::new())), } } } #[allow(non_snake_case)] #[derive(Debug, Deserialize)] pub struct UpdateTodoSchema { pub title: Option\u0026lt;String\u0026gt;, pub content: Option\u0026lt;String\u0026gt;, pub completed: Option\u0026lt;bool\u0026gt;, } This code created a Todo struct and added Serde’s macros on top of it.\nThe Deserialize and Serialize macros will allow us to convert the struct to and from JSON\nCreate the API Response Structs # Here, let’s create structs that implement the [derive(Serialize)] macro to enable us to convert structs into JSON objects before returning them to the client.\nSo, create a response.rs file in the src directory and add the following structs:\nuse crate::model::Todo; use serde::Serialize; #[derive(Serialize)] pub struct GenericResponse { pub status: String, pub message: String, } #[derive(Serialize, Debug)] pub struct TodoData { pub todo: Todo, } #[derive(Serialize, Debug)] pub struct SingleTodoResponse { pub status: String, pub data: TodoData, } #[derive(Serialize, Debug)] pub struct TodoListResponse { pub status: String, pub results: usize, pub todos: Vec\u0026lt;Todo\u0026gt;, } Implement the CRUD API # [get(\u0026quot;/healthchecker\u0026quot;)] – This route will return a simple health checker JSON object. [get(\u0026quot;/todos?\u0026lt;page\u0026gt;\u0026amp;\u0026lt;limit\u0026gt;\u0026quot;)] – This route will return a selected or paginated list of Todo items. [post(\u0026quot;/todos\u0026quot;, data = \u0026quot;\u0026lt;body\u0026gt;\u0026quot;)] – This route will add a new Todo item to the data store. [get(\u0026quot;/todos/\u0026lt;id\u0026gt;\u0026quot;)] – This route will retrieve a single Todo item from the in-memory database and return it to the client. [patch(\u0026quot;/todos/\u0026lt;id\u0026gt;\u0026quot;, data = \u0026quot;\u0026lt;body\u0026gt;\u0026quot;)] – This route will edit the fields of a Todo item in the data store. [delete(\u0026quot;/todos/\u0026lt;id\u0026gt;\u0026quot;)] – This route will delete a Todo item from the in-memory database. First things first, create a handler.rs file in the src folder and add the following crates and dependencies.\nuse crate::{ model::{AppState, Todo, UpdateTodoSchema}, response::{GenericResponse, SingleTodoResponse, TodoData, TodoListResponse}, }; use chrono::prelude::*; use rocket::{ delete, get, http::Status, patch, post, response::status::Custom, serde::json::Json, State, }; use uuid::Uuid; #[get(\u0026#34;/healthchecker\u0026#34;)] pub async fn health_checker_handler() -\u0026gt; Result\u0026lt;Json\u0026lt;GenericResponse\u0026gt;, Status\u0026gt; { const MESSAGE: \u0026amp;str = \u0026#34;Build Simple CRUD API with Rust and Rocket\u0026#34;; let response_json = GenericResponse { status: \u0026#34;success\u0026#34;.to_string(), message: MESSAGE.to_string(), }; Ok(Json(response_json)) } #[get(\u0026#34;/todos?\u0026lt;page\u0026gt;\u0026amp;\u0026lt;limit\u0026gt;\u0026#34;)] pub async fn todos_list_handler( page: Option\u0026lt;usize\u0026gt;, limit: Option\u0026lt;usize\u0026gt;, data: \u0026amp;State\u0026lt;AppState\u0026gt;, ) -\u0026gt; Result\u0026lt;Json\u0026lt;TodoListResponse\u0026gt;, Status\u0026gt; { let vec = data.todo_db.lock().unwrap(); let limit = limit.unwrap_or(10); let offset = (page.unwrap_or(1) - 1) * limit; let todos: Vec\u0026lt;Todo\u0026gt; = vec.clone().into_iter().skip(offset).take(limit).collect(); let json_response = TodoListResponse { status: \u0026#34;success\u0026#34;.to_string(), results: todos.len(), todos, }; Ok(Json(json_response)) } #[post(\u0026#34;/todos\u0026#34;, data = \u0026#34;\u0026lt;body\u0026gt;\u0026#34;)] pub async fn create_todo_handler( mut body: Json\u0026lt;Todo\u0026gt;, data: \u0026amp;State\u0026lt;AppState\u0026gt;, ) -\u0026gt; Result\u0026lt;Json\u0026lt;SingleTodoResponse\u0026gt;, Custom\u0026lt;Json\u0026lt;GenericResponse\u0026gt;\u0026gt;\u0026gt; { let mut vec = data.todo_db.lock().unwrap(); for todo in vec.iter() { if todo.title == body.title { let error_response = GenericResponse { status: \u0026#34;fail\u0026#34;.to_string(), message: format!(\u0026#34;Todo with title: \u0026#39;{}\u0026#39; already exists\u0026#34;, todo.title), }; return Err(Custom(Status::Conflict, Json(error_response))); } } let uuid_id = Uuid::new_v4(); let datetime = Utc::now(); body.id = Some(uuid_id.to_string()); body.completed = Some(false); body.createdAt = Some(datetime); body.updatedAt = Some(datetime); let todo = body.to_owned(); vec.push(body.into_inner()); let json_response = SingleTodoResponse { status: \u0026#34;success\u0026#34;.to_string(), data: TodoData { todo: todo.into_inner(), }, }; Ok(Json(json_response)) } #[get(\u0026#34;/todos/\u0026lt;id\u0026gt;\u0026#34;)] pub async fn get_todo_handler( id: String, data: \u0026amp;State\u0026lt;AppState\u0026gt;, ) -\u0026gt; Result\u0026lt;Json\u0026lt;SingleTodoResponse\u0026gt;, Custom\u0026lt;Json\u0026lt;GenericResponse\u0026gt;\u0026gt;\u0026gt; { let vec = data.todo_db.lock().unwrap(); for todo in vec.iter() { if todo.id == Some(id.to_owned()) { let json_response = SingleTodoResponse { status: \u0026#34;success\u0026#34;.to_string(), data: TodoData { todo: todo.clone() }, }; return Ok(Json(json_response)); } } let error_response = GenericResponse { status: \u0026#34;fail\u0026#34;.to_string(), message: format!(\u0026#34;Todo with ID: {} not found\u0026#34;, id), }; Err(Custom(Status::NotFound, Json(error_response))) } #[patch(\u0026#34;/todos/\u0026lt;id\u0026gt;\u0026#34;, data = \u0026#34;\u0026lt;body\u0026gt;\u0026#34;)] pub async fn edit_todo_handler( id: String, body: Json\u0026lt;UpdateTodoSchema\u0026gt;, data: \u0026amp;State\u0026lt;AppState\u0026gt;, ) -\u0026gt; Result\u0026lt;Json\u0026lt;SingleTodoResponse\u0026gt;, Custom\u0026lt;Json\u0026lt;GenericResponse\u0026gt;\u0026gt;\u0026gt; { let mut vec = data.todo_db.lock().unwrap(); for todo in vec.iter_mut() { if todo.id == Some(id.clone()) { let datetime = Utc::now(); let title = body.title.to_owned().unwrap_or(todo.title.to_owned()); let content = body.content.to_owned().unwrap_or(todo.content.to_owned()); let payload = Todo { id: todo.id.to_owned(), title: if !title.is_empty() { title } else { todo.title.to_owned() }, content: if !content.is_empty() { content } else { todo.content.to_owned() }, completed: if body.completed.is_some() { body.completed } else { todo.completed }, createdAt: todo.createdAt, updatedAt: Some(datetime), }; *todo = payload; let json_response = SingleTodoResponse { status: \u0026#34;success\u0026#34;.to_string(), data: TodoData { todo: todo.clone() }, }; return Ok(Json(json_response)); } } let error_response = GenericResponse { status: \u0026#34;fail\u0026#34;.to_string(), message: format!(\u0026#34;Todo with ID: {} not found\u0026#34;, id), }; Err(Custom(Status::NotFound, Json(error_response))) } #[delete(\u0026#34;/todos/\u0026lt;id\u0026gt;\u0026#34;)] pub async fn delete_todo_handler( id: String, data: \u0026amp;State\u0026lt;AppState\u0026gt;, ) -\u0026gt; Result\u0026lt;Status, Custom\u0026lt;Json\u0026lt;GenericResponse\u0026gt;\u0026gt;\u0026gt; { let mut vec = data.todo_db.lock().unwrap(); for todo in vec.iter_mut() { if todo.id == Some(id.clone()) { vec.retain(|todo| todo.id != Some(id.to_owned())); return Ok(Status::NoContent); } } let error_response = GenericResponse { status: \u0026#34;fail\u0026#34;.to_string(), message: format!(\u0026#34;Todo with ID: {} not found\u0026#34;, id), }; Err(Custom(Status::NotFound, Json(error_response))) } You can now start the application with the following command\ncargo run And test the API accessing the following url: http://localhost:8000/api/todos?page=1\u0026limit=10\nYou can also use the following json file to test with Postman\nMake sure to check the original article which has more detailed information on the this setup\nNow we just need some React FrontEnd Application to interact with this service.\nCreate FrontEnd Application # Now similar to my previous post related with Strapi lets create a frontend application with react.\nOn a different folder run the following command (Require bun.sh )\nbunx create-react-app todo-frontend Next create the following two files for the environment variables:\n.env.development REACT_APP_BACKEND=http://localhost:1337/ .env.production REACT_APP_BACKEND=/ You can run the frontend application with the following command\nbun run start I\u0026rsquo;m not an expert in Frontend development and I did manage to get the create operation working, but the listing however didn\u0026rsquo;t populate although I could see on the console the data was refreshed and as the article is already long, I decided to stop here.\nIf you manage to get the react component working let me know so that I can update the article.\nCargo Watch # In Rust we have a library called cargo-watch which watches the source code for changes and hot-reloads the server when required files change. Lets install it\ncargo install cargo-watch With that out of the way, run this command to start the Rocket HTTP server and restart the server when any file in the src directory changes.\ncargo watch -q -c -w src/ -x run Conclusion # Rust is gaining quite a lot popularity, implementing APIs with Rocket seems fast. This mix of React and Rust in the near future should be mixture that you would will see more.\nThe performance os Rust allied to the flexibility from the frontend component is something that I need to explore more.\nReferences # https://rocket.rs/ https://rocket.rs/v0.5/guide/getting-started/ https://codevoweb.com/build-a-simple-api-with-rust-and-rocket/ https://doc.rust-lang.org/std/sync/struct.Arc.html https://doc.rust-lang.org/std/sync/struct.Mutex.html https://www.freecodecamp.org/news/how-to-consume-rest-apis-in-react/ ","date":"24 May 2024","externalUrl":null,"permalink":"/2024/05/rocket.rs/","section":"Posts","summary":"","title":"Rocket.rs","type":"posts"},{"content":"In this article we will go through SQLGlot is a no-dependency SQL parser, transpiler, optimizer, and engine\nIntro # SQLGlot is a no-dependency SQL parser, transpiler, optimizer, and engine. It can be used to format SQL or translate between 21 different dialects like DuckDB, Presto / Trino, Spark / Databricks, Snowflake, and BigQuery. It aims to read a wide variety of SQL inputs and output syntactically and semantically correct SQL in the targeted dialects.\nInstall # pip3 install \u0026#34;sqlglot[rs]\u0026#34; Examples # Create the following python script to check translation of datafunctions from duckdb to hive\nimport sqlglot print(sqlglot.transpile(\u0026#34;SELECT EPOCH_MS(1618088028295)\u0026#34;, read=\u0026#34;duckdb\u0026#34;, write=\u0026#34;hive\u0026#34;)[0]) You should get the following output when running this code\nSELECT FROM_UNIXTIME(1618088028295 / POW(10, 3)) SQLGlot can even translate custom time formats:\nimport sqlglot print(sqlglot.transpile(\u0026#34;SELECT STRFTIME(x, \u0026#39;%y-%-m-%S\u0026#39;)\u0026#34;, read=\u0026#34;duckdb\u0026#34;, write=\u0026#34;hive\u0026#34;)[0]) You should get\nSELECT DATE_FORMAT(x, \u0026#39;yy-M-ss\u0026#39;) The official README file from the repo have other interesting examples\nhttps://github.com/tobymao/sqlglot Optimize # It also can rewrite queries into an \u0026ldquo;optimized\u0026rdquo; form like on the following example:\nimport sqlglot from sqlglot.optimizer import optimize print( optimize( sqlglot.parse_one(\u0026#34;\u0026#34;\u0026#34; SELECT A OR (B OR (C AND D)) FROM x WHERE Z = date \u0026#39;2021-01-01\u0026#39; + INTERVAL \u0026#39;1\u0026#39; month OR 1 = 0 \u0026#34;\u0026#34;\u0026#34;), schema={\u0026#34;x\u0026#34;: {\u0026#34;A\u0026#34;: \u0026#34;INT\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;INT\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;INT\u0026#34;, \u0026#34;D\u0026#34;: \u0026#34;INT\u0026#34;, \u0026#34;Z\u0026#34;: \u0026#34;STRING\u0026#34;}} ).sql(pretty=True) ) Which will provide the following output\nSELECT ( \u0026#34;x\u0026#34;.\u0026#34;a\u0026#34; \u0026lt;\u0026gt; 0 OR \u0026#34;x\u0026#34;.\u0026#34;b\u0026#34; \u0026lt;\u0026gt; 0 OR \u0026#34;x\u0026#34;.\u0026#34;c\u0026#34; \u0026lt;\u0026gt; 0 ) AND ( \u0026#34;x\u0026#34;.\u0026#34;a\u0026#34; \u0026lt;\u0026gt; 0 OR \u0026#34;x\u0026#34;.\u0026#34;b\u0026#34; \u0026lt;\u0026gt; 0 OR \u0026#34;x\u0026#34;.\u0026#34;d\u0026#34; \u0026lt;\u0026gt; 0 ) AS \u0026#34;_col_0\u0026#34; FROM \u0026#34;x\u0026#34; AS \u0026#34;x\u0026#34; WHERE CAST(\u0026#34;x\u0026#34;.\u0026#34;z\u0026#34; AS DATE) = CAST(\u0026#39;2021-02-01\u0026#39; AS DATE) SQL Execution # SQLGlot is able to interpret SQL queries, where the tables are represented as Python dictionaries. The engine is not supposed to be fast, but it can be useful for unit testing and running SQL natively across Python objects.\nfrom sqlglot.executor import execute tables = { \u0026#34;sushi\u0026#34;: [ {\u0026#34;id\u0026#34;: 1, \u0026#34;price\u0026#34;: 1.0}, {\u0026#34;id\u0026#34;: 2, \u0026#34;price\u0026#34;: 2.0}, {\u0026#34;id\u0026#34;: 3, \u0026#34;price\u0026#34;: 3.0}, ], \u0026#34;order_items\u0026#34;: [ {\u0026#34;sushi_id\u0026#34;: 1, \u0026#34;order_id\u0026#34;: 1}, {\u0026#34;sushi_id\u0026#34;: 1, \u0026#34;order_id\u0026#34;: 1}, {\u0026#34;sushi_id\u0026#34;: 2, \u0026#34;order_id\u0026#34;: 1}, {\u0026#34;sushi_id\u0026#34;: 3, \u0026#34;order_id\u0026#34;: 2}, ], \u0026#34;orders\u0026#34;: [ {\u0026#34;id\u0026#34;: 1, \u0026#34;user_id\u0026#34;: 1}, {\u0026#34;id\u0026#34;: 2, \u0026#34;user_id\u0026#34;: 2}, ], } execute( \u0026#34;\u0026#34;\u0026#34; SELECT o.user_id, SUM(s.price) AS price FROM orders o JOIN order_items i ON o.id = i.order_id JOIN sushi s ON i.sushi_id = s.id GROUP BY o.user_id \u0026#34;\u0026#34;\u0026#34;, tables=tables ) You should get the output\nuser_id price 1 4.0 2 3.0 Conclusion # This tool is very interesting if you are considering migrating between technologies in order to validate your SQL syntax or even speed up the process.\nThe optimization component is also something to take in to consideration.\nIf you are looking for some way to emulate Bigquery interfaces to test your application the following two projects could also be something to consider bigquery-emulator and tinyquery\nAnother important tool with similar functionality is Google interactive-sql-translator in case you are specifically searching on ways to speed up the migration to BigQuery could be a better approach.\nReferences # https://github.com/tobymao/sqlglot https://github.com/goccy/bigquery-emulator https://cloud.google.com/bigquery/docs/interactive-sql-translator ","date":"22 May 2024","externalUrl":null,"permalink":"/2024/05/sqlglot/","section":"Posts","summary":"","title":"SQLGlot","type":"posts"},{"content":"New theme\nIntro # This blog is implemented using Hexo, and the actual theme used does not provide a good interface for local searches as it relies on Google search, but there are several alternative themes with that functionality.\nAs such I\u0026rsquo;ve decided to upgrade to a different theme based on LEEDOM:\nhttps://github.com/leedom92/hexo-theme-leedom For this theme also included the following modules:\nhexo-generator-feed hexo-generator-search Which means now I have out-of-the box RSS feeds and local search\nAfter some minor customizations the new layout is online.\nI\u0026rsquo;ve also taken the time to upgrade the npm packages.\nLooking clean :D\n","date":"21 May 2024","externalUrl":null,"permalink":"/2024/05/new-theme/","section":"Posts","summary":"","title":"New Theme","type":"posts"},{"content":"","date":"21 May 2024","externalUrl":null,"permalink":"/tags/nosql/","section":"Tags","summary":"","title":"NoSQL","type":"tags"},{"content":"This article is about ScyllaDB\nIntro # Scylla is the real-time big data database that is API-compatible with Apache Cassandra and Amazon DynamoDB. Scylla embraces a shared-nothing approach that increases throughput and storage capacity to realize order-of-magnitude performance improvements and reduce hardware costs.\nWhy ScyllaDB # ScyllaDB is favored for its exceptional capability to manage high data volumes and support rapid read/write operations. It is particularly effective in environments demanding high throughput, low latency, and the ability to scale. The database is also known for its robustness and fault tolerance, ensuring data integrity and availability.\nDevelop with ScyllaDB # Developing with ScyllaDB involves setting up the database environment, choosing the appropriate drivers for your programming language, and integrating it with your application.\nRun ScyllaDB Install a Driver Connect an Application Tutorials and Example Projects Run ScyllaDB # ScyllaDB offers various deployment options, including Docker and ScyllaDB Cloud, making it flexible for different development scenarios.\nRun ScyllaDB in Docker Deploy with ScyllaDB Cloud (SaaS) Self-deploy in the Cloud or On-premise For the scope of this article will focus on a docker deployment running a single node.\nRun the following command to run scylla in detached mode\ndocker run --name scylla -d scylladb/scylla To check server logs you can run the following command:\ndocker logs -f scylla To verify the node status execute the following command:\ndocker exec -it scylla nodetool status You should have a similar output\nDatacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns Host ID Rack UN 172.17.0.2 181.09 KB 256 ? de5cb62e-165f-4be0-b0b4-df04be16aac4 rack1 Note: Non-system keyspaces don\u0026#39;t have the same replication settings, effective ownership information is meaningless To connect to a node one can use the following command\ndocker exec -it scylla cqlsh Scylla APIs and compatibility # By default, Scylla is compatible with Apache Cassandra and its APIs - CQL and Thrift. There is also support for the API of Amazon DynamoDB™, which needs to be enabled and configured in order to be used. For more information on how to enable the DynamoDB™ API in Scylla, and the current compatibility of this feature as well as Scylla-specific extensions, see Alternator and Getting started with Alternator.\nNext steps # Next steps would be to create some client application. The following resources provide a good starting point, but that would be outside the scope of this article.\nAdditional Resources # [https://university.scylladb.com/courses/scylla-essentials-overview/](Scyla Essentials course) Data Modeling in Scylla Data Modeling and Application Development Scylla Documentation slack channel Reference # https://www.scylladb.com/ https://github.com/scylladb/scylladb https://docs.scylladb.com/stable/ https://docs.scylladb.com/stable/get-started/ https://university.scylladb.com/courses/using-scylla-drivers/lessons/rust-and-scylla-2/ https://github.com/scylladb/scylla-cdc-rust https://github.com/scylladb/video-streaming https://github.com/scylladb/gaming-leaderboard-demo https://github.com/scylladb/scylla-cloud-getting-started/blob/main/docs/source/build-with-rust.md ","date":"21 May 2024","externalUrl":null,"permalink":"/2024/05/scylladb/","section":"Posts","summary":"","title":"scylladb","type":"posts"},{"content":"In this article I will go through what is the act tool.\nIntro # In this article I will go through act tool.\nContext # \u0026hellip; So one of this days my github actions stopped working while deploying this hexo pages.\n(It was related with one of my github actions module that required one update, but that is not relevant).\nI start doing some dumb changes and quickly realize that I would need some way to test the execution environment and not just doing blind pushes to validate the changes.\nObviously some smart fellows also passed through this and implemented a nice tool for doing exactly that called act\nSo I\u0026rsquo;m registering on this notes in case I need to use it again (probably will)\nact # \u0026ldquo;Think globally, act locally\u0026rdquo;\nRun your GitHub Actions locally! Why would you want to do this? Two reasons:\nFast Feedback - Rather than having to commit/push every time you want to test out the changes you are making to your .github/workflows/ files (or for any changes to embedded GitHub actions), you can use act to run the actions locally. The environment variables and filesystem are all configured to match what GitHub provides. Local Task Runner - I love make. However, I also hate repeating myself. With act, you can use the GitHub Actions defined in your .github/workflows/ to replace your Makefile! The issue # So I\u0026rsquo; m using hexo to generate the html from my markdown pages and integrated with github actions using the following repo:\nhttps://github.com/sma11black/hexo-action But it seems the repo lacks some attention in regards to updates.\nAnd according to github node12 actions support is deprecated and developers should change to node16.\nRef: https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/ Well the good thing of opensource is that you can fork the repo and fix the things on your own or search for some other alternative.\nForked # Probably not the best option as there are certainly some better developers than I, but I wanted to fix this quickly.\nAfter forking the repo to https://github.com/rramos/hexo-action I can start messing thing up, but I need a way to test this.\nThis is where ACT comes in place\nDownload # The instruction look pretty straight forward\ngit clone git@github.com:nektos/act.git There should be some package for your distribution but I didn\u0026rsquo; t have time for that.\ncd act make build This generate a binary, good thing they implemented this in Golang :D\nexport PATH=$PATH:/`pwd`/dist/local act -h The previous one is to make sure you have the command available on that terminal if you want to install on the system for future use, run make install.\nNOTE: I would advise however to check for any distribution package\nTest Github Actions # Now lets head to our repository where our .github/workflows directory exist and test this thing.\nI\u0026rsquo;m going to use the repo that which has the githuactions. Check the following article https://rramos.github.io/2024/01/26/githubactions, if you like to know more\nBut will change the packages for deploy action to my fork\n... # Deploy hexo blog website. - name: Deploy id: deploy uses: rramos/hexo-action@master with: deploy_key: ${{ secrets.HEXO_DEPLOY_KEY }} user_name: \u0026lt;user\u0026gt; # (or delete this input setting to use bot account) user_email: \u0026lt;email\u0026gt; # (or delete this input setting to use bot account) commit_msg: ${{ github.event.head_commit.message }} ... And run the command\nact NOTE: The first run it will ask the type of resources that you would like to assign to docker\nAnd that\u0026rsquo; s it you now can start messing with the actions images\nIn my case i just switch the node image to node22, because I like violence and did a quick fix on the Spawn module which change location and seemed to fix for now.\nAct User Guide # I would recommend to check the Official act User Guide:\nhttps://nektosact.com How Does It Work # When you run act it reads in your GitHub Actions from .github/workflows/ and determines the set of actions that need to be run. It uses the Docker API to either pull or build the necessary images, as defined in your workflow files and finally determines the execution path based on the dependencies that were defined. Once it has the execution path, it then uses the Docker API to run containers for each action based on the images prepared earlier. The environment variables and filesystem are all configured to match what GitHub provides.\nConclusion # act is a very useful tool if you like to test/change the actions for GitHub actions without polluting your branches with test attempts. It also speeds up you development process and allows you to make more interesting changes on existing images.\nReferences # https://github.com/nektos/act https://github.com/orgs/community/discussions/36493 https://nektosact.com/ https://nektosact.com/installation/aur.html NOTE: This article was written without AI support feel free to enhance it as you like. But remember John Connor is watching you\n","date":"20 May 2024","externalUrl":null,"permalink":"/2024/05/act/","section":"Posts","summary":"","title":"act","type":"posts"},{"content":"Article where I share the Manager Playbook template\nIntro # When managing a Software Engineering team, there are several activities one normally conducts such as People Management, Roadmap Planning and execution, Defining OKRs, Individual objects, Defining the Communication strategy, and the list continues.\nI normally create my own Manager Playbook in order to keep track of those activities and include new ones.\nThis helps on making sure one is conducting the activities as expected and to have a centralized place where to store those actions.\nThe following document can serve as template in case your are starting this type of activities.\nManager Playbook Template Feel free to reach me out in order to include something that you think it makes sense.\nJust make a copy and start registering :)\n","date":"20 May 2024","externalUrl":null,"permalink":"/2024/05/manager-playbook/","section":"Posts","summary":"","title":"Manager Playbook","type":"posts"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/templates/","section":"Tags","summary":"","title":"Templates","type":"tags"},{"content":"","date":"2 April 2024","externalUrl":null,"permalink":"/tags/community/","section":"Tags","summary":"","title":"Community","type":"tags"},{"content":"Article about databoost.dev community forum\nIntro # On the past week I\u0026rsquo;ve started a Forum for Data Professionals to discuss challenges and ideias in way to create a safe discussion space.\nhttps://databoost.dev The motivation for creating this was related to a big layoff thread happening in tech on the beginning of the year, in a way to support some of my past colegues, not losing track of them and some way also to share job opportunities or support to kickstarters.\nThe forum is based on https://nodebb.org and this article explain how I\u0026rsquo;ve setup the hosting of the application.\nFell free to join if you have an interest in Data, no fees and we sure need someone helping producing content.\nDomain Registration # Not much to describe here that part needs to be paid normally on a yearly basis. I\u0026rsquo;ve used http://www.godaddy.com but http://www.amen.pt also presents some good rates.\nHosting # For this one I picked Google. As it has a Free Tier you can achieve a 0-cost hosting if you don\u0026rsquo;t got abose the usage.\nhttps://cloud.google.com/free Important to pay attention on the machine you provision and the region. If you choose a e2-micro with a 30GB disk in one of the following US regions:\nOregon: us-west1 Iowa: us-central1 South Carolina: us-east1 NOTE: Check the official docs as this may change\nNodeBB # Installation of NodeBB was straight forward from the official documentation\nhttps://docs.nodebb.org/installing/os/ubuntu The project is pretty well organized and the documentation is very good\nI\u0026rsquo;ve setup some extra plugins like SSO with gihub and google, Google Analytics, Reactions and some themes.\nI opted by the mongodb option and not the redis one.\nAfter some customization on the Backend it was done.\nGCS # Having the following options on the free tier:\n5 GB-months of regional storage (US regions only) per month 5,000 Class A Operations per month Decided to setup a cron job to backup the application\nCloudfare # I also change the DNS domain servers from godaddy to cloudfare as they also support a Free tier which allows me to take advantage of caching and other benefits like DoS protection\nntfy # This service is simple and very useful. I\u0026rsquo;ve configure some services to use it like if there are any issues with the backups or access to the system, but look on the Examples Section for some of the things you can achieve.\nSSL # I also setup https://certbot.eff.org in order to have SSL enable, but that is something that I still need to check with the functionality provided from cloudfare, If I should change\nMailjet # I also configured Mailjet and Sendmail to relay all mail to that service. The free tier allow me to use with some limitations that service for newsletter setup and campaign management\nGoogle Analytics # Always good to have visibility on the engagement, activated the GA plugin and setup the tracking.\nThis still needs some work in this part.\nConclusion # In this article I\u0026rsquo;ve been through the options I did to setup databoost.dev a forum for Data Professionals. The hosting and other SaaS options used to achieve a \u0026ldquo;zero\u0026rdquo; cost option.\nFeel free to join and support us.\nReferences # https://databoost.dev https://nodebb.org https://app.mailjet.com https://analytics.google.com https://www.cloudflare.com ","date":"2 April 2024","externalUrl":null,"permalink":"/2024/04/databoost.dev/","section":"Posts","summary":"","title":"Databoost.dev","type":"posts"},{"content":"","date":"29 February 2024","externalUrl":null,"permalink":"/tags/hosting/","section":"Tags","summary":"","title":"Hosting","type":"tags"},{"content":"This article is about Pico service\nIntro # We think of pico.sh as a hacker lab where we can experiment with new ways to interact with the web.\nFeatures # Pico supports the following services:\nprose.sh - A blog platform for hackers pastes.sh - A pastebin for hackers feeds.sh - An rss email notification service The paid version called pico+ brings more service\npico+ # Paid version of pico which brings extra services:\npgs.sh - 10GB asset storage tuns.sh - Full access imgs.sh - 2GB image registry storage prose.sh - 1GB image storage Beta access Setup # Prose # Prose.sh - This service allows you to upload Github flavor markdown and it will generate the HTML content to display you just need to sync the data.\nCreate a post eg. ~/blog/hello-world.md\n# hello world! This is my first blog post. Check out some resources: - [pico.sh](https://pico.sh) - [antoniomika](https://antoniomika.me) - [bower.sh](https://bower.sh) Cya! And just publish with rsync\nrsync ~/blog/* prose.sh:/ There are some special files you can setup to customize the css or add a footer\n_styles.css _footer.md But that\u0026rsquo;s pretty much it\nCheck the following doc for more information\nhttps://pico.sh/prose Pastes # You can also use the pastebin service\necho \u0026#34;foobar\u0026#34; | ssh pastes.sh You can define a expiration\necho \u0026#34;foobar\u0026#34; | ssh pastes.sh FILENAME expires=2023-12-12 It will generate a url for your paste eg: https://ruimsramos.pastes.sh/1709216080780412798\nConclusion # This article was about pico a hacker labs service as they advertise it.\nIt is extremely fast if you want to use a pastbin option to share some data or quickly upload in prose some markdown notes, when you don\u0026rsquo;t need to worry on setting up something facing just to publish them and focus on the writing.\nThe pro service like tuns.hs and imgs.sh seem to be also powerful. The later one if you ant to integrate with github actions for instance, but I didn\u0026rsquo;t evaluate that version\nReferences # https://pico.sh/getting-started https://pico.sh/prose ","date":"29 February 2024","externalUrl":null,"permalink":"/2024/02/pico.sh/","section":"Posts","summary":"","title":"pico.sh","type":"posts"},{"content":"","date":"19 February 2024","externalUrl":null,"permalink":"/tags/datalake/","section":"Tags","summary":"","title":"DataLake","type":"tags"},{"content":"In this article I will go through the topic of Parquet Compression\nIntro # Was reading this article where Philippe Rivière and Éric Mauvière optimized a 200GB Parquet data and prepare it to 549kB.\nNow this work touch some very relevant points regarding Data Engineering procedures and best practises, I would suggest going on the article as it explains in detail what they applied in each stage and how.\nUse Case # This new fascinating dataset just dropped on Hugging Face. French public domain newspapers 🤗 references about 3 million newspapers and periodicals with their full text OCR’ed and some meta-data. The data is stored in 320 large parquet files. The data loader for this Observable framework project uses DuckDB to read these files (altogether about 200GB) and combines a minimal subset of their metadata — title and year of publication, most importantly without the text contents —, into a single highly optimized parquet file.\nUndoubtedly, this dataset proves immensely valuable for training and processing Language Model (LLM) models\nBest Practises # I firmly believe that these best practices should be applied not only to Parquet but also to other columnar formats.\nThese are the key factors you should have into consideration:\nSelect only the Columns That you will use # This is one of simplest optimizations that you can do. Remember that data is stored in a columnar way so picking the columns that matter not only will will filter out very quickly as it will reduce significantly the volume\nApply the most appropriate Compression algorithm # The majority of contemporary data formats support compression. When examining the most common ones for Parquet—such as LZO, Snappy, and Gzip—we observe several notable differences (ref: sheet)\nFor instance gzip cannot be split, which means if you are going to process the data with a distributed process like Spark for instance you must use the driver to deal with all the uncompression.\nLZO strikes a better balance between speed and compression rate when compared to Snappy. In this specific case, I would also recommend exploring Brotli as the datasets seem to contain text. Choosing an effective algorithm is crucial.\nSort the data # While it may not seem immediately relevant, aligning the rows in this manner results in extended streaks of constant values across multiple columns, enhancing the compaction ratio applied by the compression algorithm\nThoughts ## ## # They took it a step further by implementing additional optimizations, such as increasing the row_group_size. What\u0026rsquo;s crucial to highlight here is the significant gains achievable through the application of good engineering practices, resulting in faster and more cost-effective processes.\nAdditionally, DuckDB is exceptionally fast for executing these types of processes. While I\u0026rsquo;m eager to test it out, unfortunately, I find myself short on both time and disk space!\nReferences # https://mastodon.social/@severo/111957633001467414 https://github.com/apache/parquet-format/blob/master/Compression.md https://huggingface.co/spaces/observablehq/fpdn https://dev.to/alexmercedcoder/parquet-file-compression-for-everyone-zstd-brotli-lz4-gzip-snappy-5gb8 ","date":"19 February 2024","externalUrl":null,"permalink":"/2024/02/parquet-compression/","section":"Posts","summary":"","title":"Parquet Compression","type":"posts"},{"content":"","date":"18 February 2024","externalUrl":null,"permalink":"/tags/mail/","section":"Tags","summary":"","title":"Mail","type":"tags"},{"content":"In this article we will go through the configuration process for Sendmail in order to relay to Mailjet.\nIntro # In this article I will go through the process of setting up Sendmail to relay email to MailJet service.\nThere are several options to setup relaying on your web hosting service, and also several providers that you can consider.\nIncorporating the SMTP relay service with Mailjet allows to take advantage of other services provided such and Campaign management.\nRequirements # For this setup you will need to have access to your server and permissions to install software. Create one account on MailJet service Have permissions to change your domain DNS records MailJet # For this setup we are considering MailJet service but you can use a different one. Depending on the tier level, you will have different limitations.\nThe Free tier allows:\n200 emails per day 1500 contacts 6000 emails p/month It is a good point to start and later increase if it makes sense.\nDNS # SPF \u0026amp; DKIM are authentication systems that tell Internet Service Providers (ISPs), like Gmail and Yahoo, that incoming mail has been sent from an authorized system, and that it is not spam or email spoofing. To set Mailjet as an authorized sender and improve your deliverability, you need to modify your DNS records to include DKIM signature and SPF.\nThis document provides more detailed information\nBut basically you will need to include 2 TXT records\ntype: TXT , host: @ , value: \u0026ldquo;v=spf1 include:spf.mailjet.com ~all\u0026rdquo; If you run a DNS query on your domain for TXT you need to see that info\ndig -t TXT yourdomain.com You also need to include the DKIM record follow the instructions provided\nhttps://app.mailjet.com/account/sender?type=auth There is one option to validate if the configuration is working properly\nAdd Domains # You will also need to configure the allowed domains that will be allowed and validate senders.\nIn the following URL you can make those:\nhttps://app.mailjet.com/account/sender?type=domain API Keys # The last step would be to create an API key for your service.\nGo to following URL and create a new key, note it down as it will be required later.\nhttps://app.mailjet.com/account/apikeys Ok, now let\u0026rsquo;s configure our MTA\nConfigure Sendmail # For this setup you will need access to your hosting service and capable of installing software.\nThe following instructions are for a Ubuntu base distribution.\nInstall packages # sudo apt-get install sendmail Configuration # In this setup we will configure to relay via SMTP all email using auth provided by the service\nStart by editing the following file /etc/mail/sendmail.mc and add the following content at the end\ndnl # Default Mailer setup MAILER_DEFINITIONS define(`SMART_HOST\u0026#39;, `in-v3.mailjet.com\u0026#39;)dnl define(`RELAY_MAILER_ARGS\u0026#39;, `TCP $h 587\u0026#39;)dnl define(`ESMTP_MAILER_ARGS\u0026#39;, `TCP $h 587\u0026#39;)dnl define(`confAUTH_OPTIONS\u0026#39;, `A p\u0026#39;)dnl TRUST_AUTH_MECH(`EXTERNAL DIGEST-MD5 CRAM-MD5 LOGIN PLAIN\u0026#39;)dnl define(`confAUTH_MECHANISMS\u0026#39;, `EXTERNAL GSSAPI DIGEST-MD5 CRAM-MD5 LOGIN PLAIN\u0026#39;)dnl FEATURE(`authinfo\u0026#39;,`hash -o /etc/mail/authinfo/smtp-auth.db\u0026#39;)dnl MAILER(`local\u0026#39;)dnl MAILER(`smtp\u0026#39;)dnl We need to setup authentication, remember the previous API key that you created you will need to include the information associaded to API_KEY and API_SECRET on the following file /etc/mail/authinfo/smtp-auth\nAuthInfo: \u0026#34;U:root\u0026#34; \u0026#34;I:API_KEY\u0026#34; \u0026#34;P:API_SECRET\u0026#34; sudo mkdir /etc/mail/authinfo sudo nano /etc/mail/authinfo/smtp-auth Example:\nAuthInfo: \u0026#34;U:root\u0026#34; \u0026#34;I:1233450786523741256e\u0026#34; \u0026#34;P:ety555qtfgdghsd88wrfer\u0026#34; After this you need to run the following command to update the service configuration files\nmake -C /etc/mail And restart sendmail service\nsystemctl restart sendmail Test # In order to test you can execute the following command\necho \u0026#34;Test Email\u0026#34; | mail -s \u0026#34;Subject Here\u0026#34; recipient@example.com You can now check in MailJet Stats session if your mail pass there.\nTroubleshooting # You can check with the mailq command to understand if there is mail being block and the logs in /var/log/mail.log to understand if there is some issue.\nConclusion # In this article we went though the configuration of Sendmail service to relay emails through the Mailjet service. It covers the necessary configurations in both DNS and the Mailjet service to ensure seamless email delivery from your web hosting server.\nReferences # https://en.wikipedia.org/wiki/Sendmail https://www.mailjet.com https://dev.mailjet.com/smtp-relay/overview/ https://gist.github.com/josectheone/a86b58eb71f25dcfe4b54ae590994cc7 ","date":"18 February 2024","externalUrl":null,"permalink":"/2024/02/sendmail-relay-configuration/","section":"Posts","summary":"","title":"Sendmail Relay Configuration","type":"posts"},{"content":"","date":"18 February 2024","externalUrl":null,"permalink":"/tags/webhosting/","section":"Tags","summary":"","title":"WebHosting","type":"tags"},{"content":"In this article I\u0026rsquo;ll go through the Redpanda quickstart guide\nIntro # In this article I\u0026rsquo;ll go through the Redpanda quickstart guide. Spinning up a Redpanda cluster in Docker to evaluate in Linux\nRequirements # Make sure you have docker and docker-compose\nSetup # For lightweight testing, we are going to start a single Redpanda broker.\nCreate the following docker-compose.yml file with the content:\nversion: \u0026#34;3.7\u0026#34; name: redpanda-quickstart networks: redpanda_network: driver: bridge volumes: redpanda-0: null services: redpanda-0: command: - redpanda - start - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092 # Address the broker advertises to clients that connect to the Kafka API. # Use the internal addresses to connect to the Redpanda brokers\u0026#39; # from inside the same Docker network. # Use the external addresses to connect to the Redpanda brokers\u0026#39; # from outside the Docker network. - --advertise-kafka-addr internal://redpanda-0:9092,external://localhost:19092 - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082 # Address the broker advertises to clients that connect to the HTTP Proxy. - --advertise-pandaproxy-addr internal://redpanda-0:8082,external://localhost:18082 - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081 # Redpanda brokers use the RPC API to communicate with each other internally. - --rpc-addr redpanda-0:33145 - --advertise-rpc-addr redpanda-0:33145 # Tells Seastar (the framework Redpanda uses under the hood) to use 1 core on the system. - --smp 1 # The amount of memory to make available to Redpanda. - --memory 1G # Mode dev-container uses well-known configuration properties for development in containers. - --mode dev-container # enable logs for debugging. - --default-log-level=debug image: docker.redpanda.com/redpandadata/redpanda:v23.3.5 container_name: redpanda-0 volumes: - redpanda-0:/var/lib/redpanda/data networks: - redpanda_network ports: - 18081:18081 - 18082:18082 - 19092:19092 - 19644:9644 console: container_name: redpanda-console image: docker.redpanda.com/redpandadata/console:v2.4.3 networks: - redpanda_network entrypoint: /bin/sh command: -c \u0026#39;echo \u0026#34;$$CONSOLE_CONFIG_FILE\u0026#34; \u0026gt; /tmp/config.yml; /app/console\u0026#39; environment: CONFIG_FILEPATH: /tmp/config.yml CONSOLE_CONFIG_FILE: | kafka: brokers: [\u0026#34;redpanda-0:9092\u0026#34;] schemaRegistry: enabled: true urls: [\u0026#34;http://redpanda-0:8081\u0026#34;] redpanda: adminApi: enabled: true urls: [\u0026#34;http://redpanda-0:9644\u0026#34;] ports: - 8080:8080 depends_on: - redpanda-0 And start the execution with docker-compose up -d\nStart Streaming # Let\u0026rsquo;s use the rpk command-line tool to create a topic, produce messages to it, and consume messages.\nGet information about the cluster with the command\ndocker exec -it redpanda-0 rpk cluster info Now lets create a topic called chat-room:\ndocker exec -it redpanda-0 rpk topic create chat-room Producing messages for that topic\ndocker exec -it redpanda-0 rpk topic produce chat-room Consuming one message from the topic\ndocker exec -it redpanda-0 rpk topic consume chat-room --num 1 You can install rpk on your system directly and connect with the broker\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-amd64.zip Then unzip the file and put the rpk binary on bin path ex: unzip rpk-linux-amd64.zip -d ~/.local/bin/\nYou can test the connection to your broker with:\nrpk cluster info -X brokers=127.0.0.1:19092 Generating Mock Data # Let\u0026rsquo;s use the following command from our references to product mock data.\nLeave one terminal open with the following command\nrpk topic consume Products -X brokers=127.0.0.1:19092 On a different terminal create the following file schema.avsc\n{ \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Products\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;exp.products.v1\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;productId\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;string\u0026#34;] }, { \u0026#34;name\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;isLimited\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;sizes\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;string\u0026#34;], \u0026#34;default\u0026#34;: null }, { \u0026#34;name\u0026#34;: \u0026#34;ownerIds\u0026#34;, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: \u0026#34;string\u0026#34; } } ] } Make sure to install datagen\nnpm install -g @materializeinc/datagen Create the following .env file\n# Kafka Brokers KAFKA_BROKERS=localhost:19092 # For Kafka SASL Authentication: SASL_USERNAME= SASL_PASSWORD= SASL_MECHANISM= # For Kafka SSL Authentication: SSL_CA_LOCATION= SSL_CERT_LOCATION= SSL_KEY_LOCATION= # Connect to Schema Registry if using \u0026#39;--format avro\u0026#39; SCHEMA_REGISTRY_URL= SCHEMA_REGISTRY_USERNAME= SCHEMA_REGISTRY_PASSWORD= Then execute the following command\ndatagen -s schema.avsc -n 10 And you just generated mock data based on the provided json file. Take a look on the following repo for more details on datagen.\nhttps://github.com/MaterializeInc/datagen.git Conclusion # Redpanda provides a very quick alternative to have a quick kafka environment, which is especially good for developers. This article didn\u0026rsquo;t go deep on performance evaluations of Kafka vs Redpanda but their benchmarks worth assessing if that means reducing your kafka footprint.\nProbably would let that for another article. Also I would like to test the SASL options and schema register option.\nReferences # https://redpanda.com/ https://docs.redpanda.com/current/get-started/quick-start/ https://github.com/MaterializeInc/datagen.git ","date":"14 February 2024","externalUrl":null,"permalink":"/2024/02/redpanda/","section":"Posts","summary":"","title":"Redpanda","type":"posts"},{"content":"","date":"14 February 2024","externalUrl":null,"permalink":"/tags/streaming/","section":"Tags","summary":"","title":"Streaming","type":"tags"},{"content":"In this article I will build a Todo App with Strapi for the backend component and React as frontend.\nIntro # In this article I will build a Todo App with Strapi for the backend component and React as frontend. The guide was originally written by Chigozie Oduah check the references links as he as some very interesting articles about Strapi.\nWhat is Strapi # The leading open-source headless CMS. 100% JavaScript / TypeScript and fully customizable.\nSetup backend with Strapi # I will be using bun to setup packages due to improved performance checkout their page if you want to know more.\nLet\u0026rsquo;s start by creating our backend with the command\nbunx create-strapi-app todo-list --quickstart This should have created a new folder todo-list you can run the following command on that folder to start your development\nYou should now access the browser to http://localhost:1337/admin and create you admin account so that we can start create a new collection.\nIf you need to restart the development environment you can enter the todo-list folder and run\nbun run develop Building the Backend # Now for our TODO application lets create a collection.\nNavigate to Content-Type Builder Select Create new collection type Call it Todo Strapi uses this name to reference this collection within our application. Strapi automatically uses the display name to fill the rest of the text boxes.\nCreate the following fields:\nitem : Type ( Text ) And hit Save, as our application will be a simple Todo list application that single field will do the job.\nAdd test entries # After the collection is created, we add some test entries.\nGo to content Manager select the Todo collection and choose Create New entry After filling the item information you can Save and Publish Repeat the previous step to have more entries.\nCreate API Endpoint for our collection # We create API endpoints for our frontend using the Todo collection. These endpoints allows a frontend to interact with our collection.\nNavigate to Settings Click on Roles under user permission \u0026amp; roles. Click on public to open the permissions given to the public. Toggle the Todo dropdown under Permissions and Select all to allow public access to our collection without auth. Hit Save After performing the following steps you should be able to access the API\nhttp://localhost:1337/api/todos You should have a working APIs\nFind (/api/todos GET): We use this endpoint to get all the items in our Todo collection Create (/api/todos POST): We use this endpoint to create a new item in our to-do collection. Find one (/api/todos/ GET): We use this endpoint to get an item in our Todo collection. Update (/api/todos/ PUT): We use this endpoint to update an item in our Todo collection Delete (/api/todos/ DELETE): We use this endpoint to delete an item in our Todo collection. Great that was easy, now lets setup our frontend React application to interact with this API endpoints.\nSetup frontend React App # Now lets start the frontend application on the parent folder run the following command\nbunx create-react-app todo-frontend Next create the following two files for the environment variables:\n.env.development REACT_APP_BACKEND=http://localhost:1337/ .env.production REACT_APP_BACKEND=/ You can run the frontend application with the following command\nbun run start And access the browser at http://localhost:3000 which will hold an empty react application.\nLets replace the App.js file the following content\nimport { useState, useEffect } from \u0026#39;react\u0026#39;; import TodoItem from \u0026#39;./TodoItem\u0026#39;; import \u0026#39;./App.css\u0026#39;; function App() { const [todos, setTodos] = useState([]); const [newTodo, setNewTodo] = useState(\u0026#34;\u0026#34;); useEffect(() =\u0026gt; { // update update the list of todos // when the component is rendered for the first time update(); }, []); // This function updates the component with the // current todo data stored in the server function update() { fetch(`${process.env.REACT_APP_BACKEND}api/todos`) .then(res =\u0026gt; res.json()) .then(todo =\u0026gt; { setTodos(todo.data); }) } // This function sends a new todo to the server // and then call the update method to update the // component function addTodo(e) { e.preventDefault(); let item = newTodo; let body = { data: { item } }; fetch(`${process.env.REACT_APP_BACKEND}api/todos`, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#39;Content-type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify(body) }) .then(() =\u0026gt; { setNewTodo(\u0026#34;\u0026#34;); update(); }) } return ( \u0026lt;div className=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;main\u0026gt; {/* we centered the \u0026#34;main\u0026#34; tag in our style sheet*/} {/* This form collects the item we want to add to our todo, and sends it to the server */} \u0026lt;form className=\u0026#34;form\u0026#34; onSubmit={addTodo}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; className=\u0026#34;todo_input\u0026#34; placeholder=\u0026#34;Enter new todo\u0026#34; value={newTodo} onChange={e =\u0026gt; setNewTodo(e.currentTarget.value) }/\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34; className=\u0026#34;todo_button\u0026#34;\u0026gt;Add todo\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; {/* This is a list view of all the todos in the \u0026#34;todo\u0026#34; state variable */} \u0026lt;div\u0026gt; { todos.map((todo, i) =\u0026gt; { return \u0026lt;TodoItem todo={todo} key={i} update={update} /\u0026gt; }) } \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;/div\u0026gt; ) } export default App; Create the following file TodoItem.jsx with the following content:\nimport { useState } from \u0026#34;react\u0026#34;; import \u0026#39;./App.css\u0026#39;; function TodoItem({ todo, update }) { // Our component uses the \u0026#34;edit\u0026#34; state // variable to switch between editing // and viewing the todo item const [edit, setEdit] = useState(false); const [newTodo, setNewTodo] = useState(\u0026#34;\u0026#34;); // This function changes the to-do that // is rendered in this component. // This function is called when the // form to change a todo is submitted function changeTodo(e) { e.preventDefault(); let item = newTodo; let pos = todo.id; let body = { data: { item } }; fetch(`${process.env.REACT_APP_BACKEND}api/todos/${pos}`, { method: \u0026#34;PUT\u0026#34;, headers: { \u0026#39;Content-type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify(body) }) .then(() =\u0026gt; { setEdit(false); update(); }) } // This function deletes the to-do that // is rendered in this component. // This function is called when the // form to delete a todo is submitted function deleteTodo(e) { e.preventDefault(); let pos = todo.id; fetch(`${process.env.REACT_APP_BACKEND}api/todos/${pos}`, { method: \u0026#34;DELETE\u0026#34; }) .then(() =\u0026gt; { update(); }) } return \u0026lt;div className=\u0026#34;todo\u0026#34;\u0026gt; {/* The below toggles between two components depending on the current value of the \u0026#34;edit\u0026#34; state variable */} { !edit ? \u0026lt;div className=\u0026#34;name\u0026#34;\u0026gt;{todo.attributes.item}\u0026lt;/div\u0026gt; : \u0026lt;form onSubmit={changeTodo}\u0026gt; \u0026lt;input className=\u0026#34;todo_input\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Enter new todo\u0026#34; value={newTodo} onChange={e =\u0026gt; setNewTodo(e.currentTarget.value)} /\u0026gt; \u0026lt;button className=\u0026#34;todo_button\u0026#34; type=\u0026#34;submit\u0026#34;\u0026gt;Change todo\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; } \u0026lt;div\u0026gt; \u0026lt;button className=\u0026#34;delete\u0026#34; onClick={deleteTodo}\u0026gt;delete\u0026lt;/button\u0026gt; \u0026lt;button className=\u0026#34;edit\u0026#34; onClick={() =\u0026gt; { // this button toggles the \u0026#34;edit\u0026#34; state variable setEdit(!edit) // we add this snippet below to make sure that our \u0026#34;input\u0026#34; // for editing is the same as the one for the component when // it is toggled. This allows anyone using it to see the current // value in the element, so they don\u0026#39;t have to write it again setNewTodo(todo.attributes.item) }}\u0026gt;edit\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; } export default TodoItem; Also replace App.css file with the following content:\n.app { display: flex; justify-content: center; text-align: center; } .todo_input { height: 16px; padding: 10px; border-top-left-radius: 8px; border-bottom-left-radius: 8px; border: 2px solid blueviolet; } .todo_button { border: 2px solid blueviolet; background-color: transparent; height: 40px; border-top-right-radius: 8px; border-bottom-right-radius: 8px; } .todo { display: flex; justify-content: space-between; margin-top: 5px; font-weight: 700; margin-bottom: 5px; min-width: 340px; } .edit { width: 66px; font-weight: 700; background: blueviolet; border: none; border-top-right-radius: 5px; height: 33px; border-bottom-right-radius: 5px; color: white; font-size: medium; } .delete { width: 66px; font-weight: 700; background: white; border: 2px solid blueviolet; border-top-left-radius: 5px; height: 33px; color: blueviolet; border-bottom-left-radius: 5px; font-size: medium; } .form { padding-top: 27px; padding-bottom: 27px; } .name { max-width: 190.34px; text-align: left; } After the last update you should have a workable todo app http://localhost:3000/\nDeployment # I\u0026rsquo;ve seen several articles where developers bundle the frontend application on the public folder to keep a single server installation, but according to Strapi is not a good practice.\nConclusion # In this article we have setup Strapi to setup the backend for a Todo list application and a react frontend that would take advantage of the provided APIs using a headless architecture.\nStrapi allows to quickly setup APIs for Collections that can be defined and managed through a provided UI. Very useful if one would like to decouple the development process, or if you don\u0026rsquo; t won\u0026rsquo; t to implement from scratch backend functionalities.\nRegarding the level of customization would require extensive exploration. The backoffice allows to create auth tokens, webhooks, SSO, internationalization and also has a marketplace area to include more functionalities.\nAlso worth mention that if you can leverage Strapi Cloud to deploy your Production applications\nReferences # https://docs.strapi.io/ https://strapi.io/blog/how-to-build-a-to-do-list-application-with-strapi-and-react-js https://bun.sh ","date":"4 February 2024","externalUrl":null,"permalink":"/2024/02/strapi-and-react-todo-application/","section":"Posts","summary":"","title":"Strapi and React TODO Application","type":"posts"},{"content":"Article about DBT and Clickhouse\nIntegrating dbt and ClickHouse # In this we will be following the integration steps to use dbt and clickhouse with sample IMDB data.\nConfigure ClickHouse sources # Setup clickhouse check my previous article if you would like more information on this product.\nThen connect with a client and run the following DDL scripts\nCREATE DATABASE imdb; CREATE TABLE imdb.actors ( id UInt32, first_name String, last_name String, gender FixedString(1) ) ENGINE = MergeTree ORDER BY (id, first_name, last_name, gender); CREATE TABLE imdb.directors ( id UInt32, first_name String, last_name String ) ENGINE = MergeTree ORDER BY (id, first_name, last_name); CREATE TABLE imdb.genres ( movie_id UInt32, genre String ) ENGINE = MergeTree ORDER BY (movie_id, genre); CREATE TABLE imdb.movie_directors ( director_id UInt32, movie_id UInt64 ) ENGINE = MergeTree ORDER BY (director_id, movie_id); CREATE TABLE imdb.movies ( id UInt32, name String, year UInt32, rank Float32 DEFAULT 0 ) ENGINE = MergeTree ORDER BY (id, name, year); CREATE TABLE imdb.roles ( actor_id UInt32, movie_id UInt32, role String, created_at DateTime DEFAULT now() ) ENGINE = MergeTree ORDER BY (actor_id, movie_id); After creating the source tables lets fill them with data from AWS, running the following code.\nINSERT INTO imdb.actors SELECT * FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_actors.tsv.gz\u0026#39;, \u0026#39;TSVWithNames\u0026#39;); INSERT INTO imdb.directors SELECT * FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_directors.tsv.gz\u0026#39;, \u0026#39;TSVWithNames\u0026#39;); INSERT INTO imdb.genres SELECT * FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_movies_genres.tsv.gz\u0026#39;, \u0026#39;TSVWithNames\u0026#39;); INSERT INTO imdb.movie_directors SELECT * FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_movies_directors.tsv.gz\u0026#39;, \u0026#39;TSVWithNames\u0026#39;); INSERT INTO imdb.movies SELECT * FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_movies.tsv.gz\u0026#39;, \u0026#39;TSVWithNames\u0026#39;); INSERT INTO imdb.roles(actor_id, movie_id, role) SELECT actor_id, movie_id, role FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_roles.tsv.gz\u0026#39;, \u0026#39;TSVWithNames\u0026#39;); Setup DBT # Starting by setting up DBT environment\npip install dbt-core pip install dbt-clickhouse Init the dbt project\ndbt init imdb Update the file dbt_project.yml and make sure to add the actors\nmodels: imdb: # Config indicated by + and applies to all files under models/example/ actors: +materialized: view Create the following file models/actors/schema.yml with the following content\nversion: 2 sources: - name: imdb tables: - name: directors - name: actors - name: roles - name: movies - name: genres - name: movie_directors Create the following file models/actors/actor_summary.sql with the content\n{{ config(order_by=\u0026#39;(updated_at, id, name)\u0026#39;, engine=\u0026#39;MergeTree()\u0026#39;, materialized=\u0026#39;table\u0026#39;) }} with actor_summary as ( SELECT id, any(actor_name) as name, uniqExact(movie_id) as num_movies, avg(rank) as avg_rank, uniqExact(genre) as genres, uniqExact(director_name) as directors, max(created_at) as updated_at FROM ( SELECT {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;actors\u0026#39;) }}.id as id, concat({{ source(\u0026#39;imdb\u0026#39;, \u0026#39;actors\u0026#39;) }}.first_name, \u0026#39; \u0026#39;, {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;actors\u0026#39;) }}.last_name) as actor_name, {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movies\u0026#39;) }}.id as movie_id, {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movies\u0026#39;) }}.rank as rank, genre, concat({{ source(\u0026#39;imdb\u0026#39;, \u0026#39;directors\u0026#39;) }}.first_name, \u0026#39; \u0026#39;, {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;directors\u0026#39;) }}.last_name) as director_name, created_at FROM {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;actors\u0026#39;) }} JOIN {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;roles\u0026#39;) }} ON {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;roles\u0026#39;) }}.actor_id = {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;actors\u0026#39;) }}.id LEFT OUTER JOIN {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movies\u0026#39;) }} ON {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movies\u0026#39;) }}.id = {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;roles\u0026#39;) }}.movie_id LEFT OUTER JOIN {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;genres\u0026#39;) }} ON {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;genres\u0026#39;) }}.movie_id = {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movies\u0026#39;) }}.id LEFT OUTER JOIN {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movie_directors\u0026#39;) }} ON {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movie_directors\u0026#39;) }}.movie_id = {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movies\u0026#39;) }}.id LEFT OUTER JOIN {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;directors\u0026#39;) }} ON {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;directors\u0026#39;) }}.id = {{ source(\u0026#39;imdb\u0026#39;, \u0026#39;movie_directors\u0026#39;) }}.director_id ) GROUP BY id ) select * from actor_summary Configure the clickstream connection on the following file ~/.dbt/profiles.yml\nimdb: target: dev outputs: dev: type: clickhouse schema: imdb_dbt host: localhost port: 8123 user: default password: \u0026#39;\u0026#39; secure: False After this updates run the dbt debug command. To make sure the connection is working properly\ndbt debug 00:31:58 Running with dbt=1.7.6 00:31:58 dbt version: 1.7.6 00:31:58 python version: 3.11.6 00:31:58 python path: /home/rramos/Development/local/dbt/bin/python 00:31:58 os info: Linux-6.6.10-zen1-1-zen-x86_64-with-glibc2.38 00:31:58 Using profiles dir at /home/rramos/.dbt 00:31:58 Using profiles.yml file at /home/rramos/.dbt/profiles.yml 00:31:58 Using dbt_project.yml file at /home/rramos/Development/local/dbt/imdb/dbt_project.yml 00:31:58 adapter type: clickhouse 00:31:58 adapter version: 1.7.1 00:31:58 Configuration: 00:31:58 profiles.yml file [OK found and valid] 00:31:58 dbt_project.yml file [OK found and valid] 00:31:58 Required dependencies: 00:31:58 - git [OK found] ... 00:31:58 Registered adapter: clickhouse=1.7.1 00:31:58 Connection test: [OK connection ok] If the connection test passed properly, one just need to create the model via dbt.\ndbt run And you should have a similar output\ndbt run 00:38:13 Running with dbt=1.7.6 00:38:13 Registered adapter: clickhouse=1.7.1 00:38:13 Unable to do partial parsing because a project config has changed 00:38:15 Found 1 model, 6 sources, 0 exposures, 0 metrics, 421 macros, 0 groups, 0 semantic models 00:38:15 00:38:15 Concurrency: 1 threads (target=\u0026#39;dev\u0026#39;) 00:38:15 00:38:15 1 of 1 START sql view model `imdb`.`actor_summary` ............................. [RUN] 00:38:15 1 of 1 OK created sql view model `imdb`.`actor_summary` ........................ [OK in 0.17s] 00:38:15 00:38:15 Finished running 1 view model in 0 hours 0 minutes and 0.27 seconds (0.27s). 00:38:15 00:38:15 Completed successfully 00:38:15 00:38:15 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 Test query the model\nSELECT * FROM imdb_dbt.actor_summary WHERE num_movies \u0026gt; 5 ORDER BY avg_rank DESC Conclusion # In this article I\u0026rsquo;ve went through the process of setup a Clickhouse database and setup dbt to setup the models with IMDB test data for actors, directors, movies, etc.\nThis two systems work like a charm together. Clickstream shows great performance for analytical queries, and dbt compiles and runs your analytics code against your data platform, enabling you and your team to collaborate on a single source of truth for metrics, insights, and business definitions.\nWould like to extend this exercise by incorporating github actions related with dbt test actions before promoting to production.\nReferences # https://clickhouse.com/docs/en/integrations/dbt https://docs.getdbt.com/guides ","date":"29 January 2024","externalUrl":null,"permalink":"/2024/01/dbt-clickhouse/","section":"Posts","summary":"","title":"dbt-clickhouse","type":"posts"},{"content":"In this article I will go through ClickHouse a tree column-oriented DBMS.\nIntro # ClickHouse is a true column-oriented DBMS. Data is stored by columns, and during the execution of arrays (vectors or chunks of columns). Whenever possible, operations are dispatched on arrays, rather than on individual values. It is called “vectorized query execution” and it helps lower the cost of actual data processing.\nArchitecture # ClickHouse was initially built as a prototype to do just a single task well: to filter and aggregate data as fast as possible. That’s what needs to be done to build a typical analytical report, and that’s what a typical GROUP BY query does. The ClickHouse team has made several high-level decisions that, when combined, made achieving this task possible:\nColumn-oriented storage: Source data often contain hundreds or even thousands of columns, while a report can use just a few of them. The system needs to avoid reading unnecessary columns to avoid expensive disk read operations.\nIndexes: Memory resident ClickHouse data structures allow the reading of only the necessary columns, and only the necessary row ranges of those columns.\nData compression: Storing different values of the same column together often leads to better compression ratios (compared to row-oriented systems) because in real data a column often has the same, or not so many different, values for neighboring rows. In addition to general-purpose compression, ClickHouse supports specialized codecs that can make data even more compact.\nVectorized query execution: ClickHouse not only stores data in columns but also processes data in columns. This leads to better CPU cache utilization and allows for SIMD CPU instructions usage.\nScalability: ClickHouse can leverage all available CPU cores and disks to execute even a single query. Not only on a single server but all CPU cores and disks of a cluster as well.\nAttention to Low-Level Details # But many other database management systems use similar techniques. What really makes ClickHouse stand out is attention to low-level details. Most programming languages provide implementations for most common algorithms and data structures, but they tend to be too generic to be effective.\nSetup # In order to install run the following script\ncurl https://clickhouse.com/ | sh One can start the server with the following command\n./clickhouse server With a different terminal lets start a client with\n./clickhouse client Tests # Lets start by creating a table from sample data\nCREATE TABLE my_first_table ( user_id UInt32, message String, timestamp DateTime, metric Float32 ) ENGINE = MergeTree PRIMARY KEY (user_id, timestamp) The statement uses traditional SQL DDL, with one extend information regarding the execution engine. The MergeTree option provides improved performance for managed tables but there are also options to integrate with external systems such as BigQuery, S3, Kafka, PostgreSQL, \u0026hellip;\nInsert some data # INSERT INTO my_first_table (user_id, message, timestamp, metric) VALUES (101, \u0026#39;Hello, ClickHouse!\u0026#39;, now(), -1.0 ), (102, \u0026#39;Insert a lot of rows per batch\u0026#39;, yesterday(), 1.41421 ), (102, \u0026#39;Sort your data based on your commonly-used queries\u0026#39;, today(), 2.718 ), (101, \u0026#39;Granules are the smallest chunks of data read\u0026#39;, now() + 5, 3.14159 ) Query # SELECT * FROM my_first_table ORDER BY timestamp Now lets create a table from external data in S3 and a materialized table using the MergeTree engine from it.\nCreate data from S3 sources # First create our managed table\nCREATE TABLE trips ( `trip_id` UInt32, `vendor_id` Enum8(\u0026#39;1\u0026#39; = 1, \u0026#39;2\u0026#39; = 2, \u0026#39;3\u0026#39; = 3, \u0026#39;4\u0026#39; = 4, \u0026#39;CMT\u0026#39; = 5, \u0026#39;VTS\u0026#39; = 6, \u0026#39;DDS\u0026#39; = 7, \u0026#39;B02512\u0026#39; = 10, \u0026#39;B02598\u0026#39; = 11, \u0026#39;B02617\u0026#39; = 12, \u0026#39;B02682\u0026#39; = 13, \u0026#39;B02764\u0026#39; = 14, \u0026#39;\u0026#39; = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type` Enum8(\u0026#39;UNK\u0026#39; = 0, \u0026#39;CSH\u0026#39; = 1, \u0026#39;CRE\u0026#39; = 2, \u0026#39;NOC\u0026#39; = 3, \u0026#39;DIS\u0026#39; = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8(\u0026#39;yellow\u0026#39; = 1, \u0026#39;green\u0026#39; = 2, \u0026#39;uber\u0026#39; = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` String, `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` String, `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192 Now let\u0026rsquo;s create our raw table. Take into account the ENGINE being used.\nCREATE TABLE trips_raw ( `trip_id` UInt32, `vendor_id` Enum8(\u0026#39;1\u0026#39; = 1, \u0026#39;2\u0026#39; = 2, \u0026#39;3\u0026#39; = 3, \u0026#39;4\u0026#39; = 4, \u0026#39;CMT\u0026#39; = 5, \u0026#39;VTS\u0026#39; = 6, \u0026#39;DDS\u0026#39; = 7, \u0026#39;B02512\u0026#39; = 10, \u0026#39;B02598\u0026#39; = 11, \u0026#39;B02617\u0026#39; = 12, \u0026#39;B02682\u0026#39; = 13, \u0026#39;B02764\u0026#39; = 14, \u0026#39;\u0026#39; = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type_` Enum8(\u0026#39;UNK\u0026#39; = 0, \u0026#39;CSH\u0026#39; = 1, \u0026#39;CRE\u0026#39; = 2, \u0026#39;NOC\u0026#39; = 3, \u0026#39;DIS\u0026#39; = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8(\u0026#39;yellow\u0026#39; = 1, \u0026#39;green\u0026#39; = 2, \u0026#39;uber\u0026#39; = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` FixedString(7), `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` FixedString(7), `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = S3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{0..9}.gz\u0026#39;, \u0026#39;TabSeparatedWithNames\u0026#39;, \u0026#39;gzip\u0026#39;); Let\u0026rsquo;s select from our raw table\nSELECT DISTINCT(pickup_ntaname) FROM trips_raw LIMIT 10; The S3 table engine supports parallel reads. Writes are only supported if the table definition does not contain glob patterns\nYou will need to configure access credentials on the config.xml if you are using private data or you need to write data. You can also define individual configuration filed on the conf.d directory like the following example.\n\u0026lt;clickhouse\u0026gt; \u0026lt;s3\u0026gt; \u0026lt;endpoint-name\u0026gt; \u0026lt;endpoint\u0026gt;https://dalem-files.s3.amazonaws.com/test/\u0026lt;/endpoint\u0026gt; \u0026lt;access_key_id\u0026gt;key\u0026lt;/access_key_id\u0026gt; \u0026lt;secret_access_key\u0026gt;secret\u0026lt;/secret_access_key\u0026gt; \u0026lt;!-- \u0026lt;use_environment_credentials\u0026gt;false\u0026lt;/use_environment_credentials\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;header\u0026gt;Authorization: Bearer SOME-TOKEN\u0026lt;/header\u0026gt; --\u0026gt; \u0026lt;/endpoint-name\u0026gt; \u0026lt;/s3\u0026gt; \u0026lt;/clickhouse\u0026gt; But we are not going to do this and just load the public data locally.\nLet\u0026rsquo;s just load data from our raw table for the materialized one.\nINSERT INTO trips SELECT * FROM trips_raw This will take some time and you can check the progress bar on the console\n0 rows in set. Elapsed: 109.103 sec. Processed 10.00 million rows, 815.59 MB (91.66 thousand rows/s., 7.48 MB/s.) Peak memory usage: 1.21 GiB. Lets validate we have the data\nbuldozer :) SELECT COUNT(1) FROM trips SELECT COUNT(1) FROM trips Query id: 8956eff9-d76e-4a1e-a766-9bcc5dd1f6cd ┌──count()─┐ │ 10000840 │ └──────────┘ 1 row in set. Elapsed: 0.002 sec. Let\u0026rsquo;s execute a DISTINCT query to force a heavier execution plan.\nSELECT DISTINCT(*) FROM trips This execution was done in a single server with a AMD processor\n10000840 rows in set. Elapsed: 18.121 sec. Processed 10.00 million rows, 3.07 GB (551.89 thousand rows/s., 169.19 MB/s.) Peak memory usage: 631.59 MiB. Still it scanned 10M registers in 18s, not bad :D\nClickhouse Local # Clickhouse local is very useful if you want to query data directly on top of files without having to install a server.\nThis option is also very interesting to incorporate in a deployment pipeline\nQuery CSV # ./clickhouse local -q \u0026#34;SELECT * FROM \u0026#39;review_?.csv\u0026#39;\u0026#34; Query Parquet # ./clickhouse local -q \u0026#34; SELECT count() FROM s3(\u0026#39;https://datasets-documentation.s3.eu-west-3.amazonaws.com/house_parquet/house_0.parquet\u0026#39;)\u0026#34; Integrations # Clickhouse as many integrations available:\nCore integrations: built or maintained by ClickHouse, they are supported by ClickHouse and live in the ClickHouse GitHub organization Partner integrations: built or maintained, and supported by, third-party software vendors Community integrations: built or maintained and supported by community members. No direct support is available besides the public GitHub repositories and community Slack channels Also the available documentation seems well prepared.\nProduction # This article only touches the surface on the available options to setup Clickhouse one should read the Scale Out section to understand best to deploy in Production\nConclusion # In this article we followed the quick-start guide in order to setup a Clickhouse server loaded data from S3 a public dataset with 10M records and performed a select distinct query on that table.\nClickhouse presents as a very interesting OLTP solution. If you are considering a solution for Analytical Reporting this is something to have on your radar.\nI also liked the fact you can start small having you service full deployment and maintained by you, but still having the capability to scaling horizontally or moving to Cloud Offering which would granting the support characteristics that most of the times are missing in open-source source software.\nThe fact there is a huge list of Adopters and a Company providing support and defining a roadmap for the product also brings reassurance to use this Product.\nThe documentation provide several integrations patterns that are worth checking.\nThis is definitely one that I will keep an eye on.\nWell done Clickhouse ;)\nReferences # https://clickhouse.com/ https://clickhouse.com/docs/en/tutorial https://clickhouse.com/docs/en/about-us/cloud https://clickhouse.com/docs/en/about-us/adopters https://clickhouse.com/docs/en/operations/utilities ","date":"28 January 2024","externalUrl":null,"permalink":"/2024/01/clickhouse/","section":"Posts","summary":"","title":"ClickHouse","type":"posts"},{"content":"","date":"28 January 2024","externalUrl":null,"permalink":"/tags/oltp/","section":"Tags","summary":"","title":"OLTP","type":"tags"},{"content":"In this article I will go through the steps to setup github actions to deploy hexo pages upon push requests into Github\nIntro # In this article I will go through the steps to setup github actions to deploy hexo pages upon push requests into Github pages\nGithub Actions # GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want.\nCheck the available documentation as there are several interesting examples\nHexo Setup # This tech notes site is maintained by Hexo a markdown blob framework.\nThe framework generates html static content based on Markdown articles an one needs carry the following steps to update content\nCreate/Update Markdown content Execute hexo generate to generate content Validate the pages with hexo serve Deploy to your hosting service hexo deploy Setup # Include the following file .github/workflows/hexo-deploy.yml on your hexo repo with the content:\nname: Deploy on: [push] jobs: build: runs-on: ubuntu-latest name: Deploying udateds Hexo articles steps: - name: Checkout uses: actions/checkout@v2 with: submodules: \u0026#39;true\u0026#39; token: ${{ secrets.ACCESS_TOKEN }} # Caching dependencies to speed up workflows. (GitHub will remove any cache entries that have not been accessed in over 7 days.) - name: Cache node modules uses: actions/cache@v1 id: cache with: path: node_modules key: ${{ runner.os }}-node-${{ hashFiles(\u0026#39;**/package-lock.json\u0026#39;) }} restore-keys: | ${{ runner.os }}-node- - name: Install Dependencies if: steps.cache.outputs.cache-hit != \u0026#39;true\u0026#39; run: npm ci # Deploy hexo blog website. - name: Deploy id: deploy uses: sma11black/hexo-action@v1.0.4 with: deploy_key: ${{ secrets.HEXO_DEPLOY_KEY }} user_name: \u0026lt;GITHUB_USER\u0026gt; # (or delete this input setting to use bot account) user_email: \u0026lt;GITHUB_EMAIL\u0026gt; # (or delete this input setting to use bot account) commit_msg: ${{ github.event.head_commit.message }} # Use the output from the `deploy` step(use for test action) - name: Get the output run: | echo \u0026#34;${{ steps.deploy.outputs.notify }}\u0026#34; Replace \u0026lt;GITHUB_USER\u0026gt; with your user account and \u0026lt;GITHUB_EMAIL\u0026gt; with your email address Generate a new ssh-key with the command ssh-keygen -t rsa -C \u0026quot;\u0026lt;GITHUB_EMAIL\u0026gt;\u0026quot; making sure to use you email account This step will generate 2 files a pub key which you need to configure on the destination repo as one allowed Deployment key And on the source repo you need to configure a secret where you will put the ssh-key Configure a personal token and register also as a secret on the source repository as ACCESS_TOKEN That\u0026rsquo;s it you just need to start pushing changes\nNOTE: This assumes the hexo source repository was already configured for the destination github pages account.\nMulti repos # It is important to notice that you cannot assign the same deployment key for several repositories.\nThat is why I used a personal token, but there should be better alternatives.\nConclusion # Github Actions is a really powerful CI/CD tool and for this type of static generation content works rather well.\nI had several issues regarding github submodules where the authentication was not passing. If you use the same approach for themes, you may endup on the same situation and using a token approach would be preferable\nAlso the ssh-keys being bounded by repo caused some initial confusion and there should be a better way to setup the authentication but I didn\u0026rsquo;t explore it in detail.\nAlso package-lock.json are required for this to work and is advisable to have your source repo as private.\nThis workflow can certainly be improve like including tests and making sure that grammar validation is done as one example.\nReferences # https://hexo.io https://github.com/marketplace/actions/hexo-action https://pages.github.com/ ","date":"26 January 2024","externalUrl":null,"permalink":"/2024/01/github-actions/","section":"Posts","summary":"","title":"GitHub Actions","type":"posts"},{"content":"","date":"25 January 2024","externalUrl":null,"permalink":"/tags/microsoft-fabric/","section":"Tags","summary":"","title":"Microsoft Fabric","type":"tags"},{"content":"In this article I will go through MS Fabric\nIntro # Fabric is a centralized product implemented by Microsoft in a SaaS way, that combines several services such as DataLake, Orchestration Processing Visualization and AI.\nBilling is defined by the amount of processing used and the amount of storage used.\nOneLake is the solution for storage where all the Data is automatically indexed for discovery, lineage and Governance are configured with support of Pureview.\nData can be virtualized with external storage locations from different cloud providers, no data duplication is needed similar to pointers.\nAlso data is stored in Delta guaranteeing ACID compliant characteristics.\nIntegration with PowerBI for reporting using the familiar look\nData Activator is the realtime processing component which triggers actions based on rules, like automated reports or procedures.\nThe Product also brings CoPilot features for PowerBI, DS Notebooks and DataFactory cleaning processes.\nComponents # OneLake Data Factory Synapse Data Activator PowerBI Integrations # Databricks integration through Delta Uniform, also Unity Catalog integrates with OneSecurity\nCopilot # Several integrations of Copilot, but the one with PowerBI is really interesting to produce quick visualizations from natual language prompts\nConclusion # Not much to conclude here as the Product needs to be deeply tested from my part. I have some concerns on vendor lock-in, but the fact that it sits on top a DataLake and supports openformats brings some reassurance.\nOne thing that also troubles me, is that if one client just want to use a partial feature would need to acquire the full solution.\nFor instance D365 in the past allowed the client to export data from that system to a Datalake with internal feature of the Product and now it relies on Fabric only, if this strategy is going to be followed for the rest of MS portfolio, example DataFactory, although increased stability or support. I found the lack of flexibility concerning, by forcing clients to use this solution.\nReferences # https://www.microsoft.com/en-us/microsoft-fabric ","date":"25 January 2024","externalUrl":null,"permalink":"/2024/01/microsoft-fabric/","section":"Posts","summary":"","title":"Microsoft Fabric","type":"posts"},{"content":"In this article I will go through the process to configure a PS3 controller in Linux (Arch)\nConfiguration for PS3 Controller in Arch Linux # This quick guide explains how to setup your PS3 controller in Arch Linux making sure the Bluetooth connection works\nRequirements # Make sure to install the bluetooth packages and have a working usb device.\nsudo pacman -S bluez bluez-utils bluez-plugins Create the following file /etc/bluetooth/main.conf with the content:\n[General] ClassicBondedOnly=false And just restart you computer.\nPress the Middle PS button until leds start blinking, then connect the usb cable from the controller to the PC and you should se a warning to authorize the device.\nAfter that you can unplug the controller and manage via you bluetooth manager.\nNOTE: Take into account that the ClassicalBondedOnly Option is a solution that regresses you security configuration for bluetooth pairing, more details here use at you own risk\nReferences # https://wiki.archlinux.org/title/Gamepad ","date":"4 January 2024","externalUrl":null,"permalink":"/2024/01/ps3-controller-in-arch/","section":"Posts","summary":"","title":"PS3 Controller in Arch","type":"posts"},{"content":"","date":"2 January 2024","externalUrl":null,"permalink":"/tags/scala/","section":"Tags","summary":"","title":"Scala","type":"tags"},{"content":"In this article we will go through Scala Variance\nIntro # Variance lets you control how type parameters behave with regards to subtyping. Scala supports variance annotations of type parameters of generic classes, to allow them to be covariant, contravariant, or invariant if no annotations are used. The use of variance in the type system allows us to make intuitive connections between complex types.\nVariance: If B Extends A, Should List[B] extend List[A] ?\nInvariance # By default, type parameters in Scala are invariant: subtyping relationships between the type parameters aren’t reflected in the parameterized type.\ntrait List[A] Example\nclass Box[A](var content: A) We’re going to be putting values of type Animal in it. This type is defined as follows:\nabstract class Animal { def name: String } case class Cat(name: String) extends Animal case class Dog(name: String) extends Animal We can say that Cat is a subtype of Animal, and that Dog is also a subtype of Animal. That means that the following is well-typed:\nval myAnimal: Animal = Cat(\u0026#34;Felix\u0026#34;) What about boxes? Is Box[Cat] a subtype of Box[Animal], like Cat is a subtype of Animal?\nval myCatBox: Box[Cat] = new Box[Cat](Cat(\u0026#34;Felix\u0026#34;)) val myAnimalBox: Box[Animal] = myCatBox // this doesn\u0026#39;t compile Although this is valid\nval myAnimal: Animal = myAnimalBox.content myAnimalBox.content = Dog(\u0026#34;Fido\u0026#34;) From this, we have to conclude that Box[Cat] and Box[Animal] can’t have a subtyping relationship, even though Cat and Animal do.\nCovarience # The The problem we ran in to above, is that because we could put a Dog in an Animal Box, a Cat Box can’t be an Animal Box.\ntrait List[+A] Example:\nclass ImmutableBox[+A](val content: A) val catbox: ImmutableBox[Cat] = new ImmutableBox[Cat](Cat(\u0026#34;Felix\u0026#34;)) val animalBox: ImmutableBox[Animal] = catbox // now this compiles We say that ImmutableBox is covariant in A, and this is indicated by the + before the A.\nContravarience # We’ve seen we can accomplish covariance by making sure that we can’t put something in the covariant type, but only get something out. What if we had the opposite, something you can put something in, but can’t take out?\ntrait List[-A] Example:\nabstract class Serializer[-A] { def serialize(a: A): String } val animalSerializer: Serializer[Animal] = new Serializer[Animal] { def serialize(animal: Animal): String = s\u0026#34;\u0026#34;\u0026#34;{ \u0026#34;name\u0026#34;: \u0026#34;${animal.name}\u0026#34; }\u0026#34;\u0026#34;\u0026#34; } val catSerializer: Serializer[Cat] = animalSerializer catSerializer.serialize(Cat(\u0026#34;Felix\u0026#34;)) We say that Serializer is contravariant in A, and this is indicated by the - before the A. A more general serializer is a subtype of a more specific serializer.\nMore formally, that gives us the reverse relationship: given some class Contra[-T], then if A is a subtype of B, Contra[B] is a subtype of Contra[A].\nBounded Types # The type of Variance would lead to have members defined with upper or lower bounded types like the following example.\nclass Car class SuperCar extends Car class Garage[T \u0026lt;: Car\u0026gt;](car: T) More details on bounded values on the following articles:\nhttps://docs.scala-lang.org/tour/upper-type-bounds.html https://docs.scala-lang.org/tour/lower-type-bounds.html Immutability and Variance # Immutability constitutes an important part of the design decision behind using variance. For example, Scala’s collections systematically distinguish between mutable and immutable collections. The main issue is that a covariant mutable collection can break type safety. This is why List is a covariant collection, while scala.collection.mutable.ListBuffer is an invariant collection.\nComparison With Other Languages # Variance is supported in different ways by some languages that are similar to Scala. Scala’s tendency towards immutable types makes it that covariant and contravariant types are more common than in other languages, since a mutable generic type must be invariant.\nReference # https://docs.scala-lang.org/tour/variances.html https://docs.scala-lang.org/tour/upper-type-bounds.html https://docs.scala-lang.org/tour/lower-type-bounds.html ","date":"2 January 2024","externalUrl":null,"permalink":"/2024/01/scala-variance/","section":"Posts","summary":"","title":"Scala Variance","type":"posts"},{"content":"","date":"27 December 2023","externalUrl":null,"permalink":"/tags/.net/","section":"Tags","summary":"","title":".NET","type":"tags"},{"content":"In this article I will go through the setup process to setup a .NET 8.0 Aspire application in Linux from MS Official Guide\nIntro # In this article I will go through the setup process to setup a .NET 8.0 Aspire application in Linux from MS Official Guide\nAbout # Cloud-native apps often require connections to various services such as databases, storage and caching solutions, messaging providers, or other web services. .NET Aspire is designed to streamline connections and configurations between these types of services.\nRequirements # In order to use .NET Aspire one needs to have .NET 8.0. As I\u0026rsquo;m using Ubuntu for this article I\u0026rsquo;ve followed this steps to prepare the environment\nInstall .NET SDK and Runtime # sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y dotnet-sdk-8.0 sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y aspnetcore-runtime-8.0 NOTE: For some reason in Ubuntu 22.04 there seems exit conflicts between donet-sdk-8 and dotnet-sdk-6, as such I had to remove the older versions. Take that into account if you have other projects to maintain with those versions as you may need more work\nCheck existing versions # Execute the following command to validate you have 8.0 available\ndotnet --list-runtimes You should have something similar for the runtime\nMicrosoft.AspNetCore.App 8.0.0 [/usr/share/dotnet/shared/Microsoft.AspNetCore.App] Microsoft.NETCore.App 8.0.0 [/usr/share/dotnet/shared/Microsoft.NETCore.App] And now the SDK\ndotnet --list-sdks 8.0.100 [/usr/share/dotnet/sdk] Check the templates to make sure we have the aspire\nUpdate Workloads # sudo dotnet workload update sudo dotnet workload install aspire Check existing Templates # Run the following to make sure you have the required templates\ndotnet new --list | grep aspire You should get something like:\n.NET Aspire Application aspire [C#] Common/.NET Aspire/Cloud/Web/Web API/API/Service .NET Aspire Starter Application aspire-starter [C#] Common/.NET Aspire/Blazor/Web/Web API/API/Service/Cloud Generate the Application # To generate the application run the following command:\ndotnet new aspire-starter --use-redis-cache --output AspireSample About the Application # The solution consists of the following projects:\nAspireSample.ApiService: An ASP.NET Core Minimal API project is used to provide data to the front end. This project depends on the shared AspireSample.ServiceDefaults project. AspireSample.AppHost: An orchestrator project designed to connect and configure the different projects and services of your app. The orchestrator should be set as the Startup project, and it depends on the AspireSample.ApiService and AspireSample.Web projects. AspireSample.ServiceDefaults: A .NET Aspire shared project to manage configurations that are reused across the projects in your solution related to resilience, service discovery, and telemetry. AspireSample.Web: An ASP.NET Core Blazor App project with default .NET Aspire service configurations, this project depends on the AspireSample.ServiceDefaults project. For more information, see .NET Aspire service defaults. Run the Application # To run the application execute\ndotnet run --project AspireSample/AspireSample.AppHost You will get a similar output\nBuilding... info: Aspire.Dashboard.DashboardWebApplication[0] Now listening on: http://localhost:15214 info: Aspire.Dashboard.DashboardWebApplication[0] OTLP server running at: http://localhost:16176 warn: Microsoft.AspNetCore.DataProtection.KeyManagement.XmlKeyManager[35] No XML encryptor configured. Key {13491cb7-f7b7-4ace-9ab5-0a6b77bf559f} may be persisted to storage in unencrypted form. info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Development info: Microsoft.Hosting.Lifetime[0] Content root path: /home/rui.ramos/Development/local/aspire-test/AspireSample/AspireSample.AppHost Checkout on your browser the Aspire Dashboard at http://localhost:15214\nAnd the webfront application http://localhost:5119\nAspireSample.AppHost # Checking the Program.cs we can see relevant information\nvar builder = DistributedApplication.CreateBuilder(args); var cache = builder.AddRedisContainer(\u0026#34;cache\u0026#34;); var apiservice = builder.AddProject\u0026lt;Projects.AspireSample_ApiService\u0026gt;(\u0026#34;apiservice\u0026#34;); builder.AddProject\u0026lt;Projects.AspireSample_Web\u0026gt;(\u0026#34;webfrontend\u0026#34;) .WithReference(cache) .WithReference(apiservice); builder.Build().Run(); The preceding code creates a DistributedApplication builder adding a Redis Container the APIService and the Sample Web Application\nWhy .NET Aspire # .NET Aspire is designed to improve the experience of building .NET cloud-native apps. It provides a consistent, opinionated set of tools and patterns that help you build and run distributed apps. .NET Aspire is designed to help you with:\nOrchestration: .NET Aspire provides features for running and connecting multi-project applications and their dependencies. Components: .NET Aspire components are NuGet packages for commonly used services, such as Redis or Postgres, with standardized interfaces ensuring they connect consistently and seamlessly with your app. Tooling: .NET Aspire comes with project templates and tooling experiences for Visual Studio and the dotnet CLI help you create and interact with .NET Aspire apps. References # Aspire Get Started Aspire Official Docs Dashboard Fundamentals Aspire Components ","date":"27 December 2023","externalUrl":null,"permalink":"/2023/12/.net-aspire-sample-application/","section":"Posts","summary":"","title":".NET Aspire Sample Application","type":"posts"},{"content":"In this article I configure a local endpoint using ngrok for testing purposes.\nIntro # In this article I configure a local endpoint using ngrok for testing purposes.\nWhat is ngrok # ngrok is a globally distributed reverse proxy that secures, protects and accelerates your applications and network services, no matter where you run them. You can think of ngrok as the front door to your applications\nRequirements # For this guide I will use snap. Checkout the Official guide for other alternatives.\nsnap install ngrok Configuration # Next step create an account with the Free tier at https://ngrok.com.\nNOTE: This option is intended only for testing purposes for Production workloads you should consider a different option based on your network usage. Use this instructions at your own risk\nWhen you access the service you can get your token run the following command to add it to your local configuration\nngrok config add-authtoken \u0026lt;TOKEN\u0026gt; you can run the following command to make sure your configuration checks out\nTest # Open two terminals. On the first let\u0026rsquo;s start a listener using netcat\nnc -l -p 9393 On the second one let\u0026rsquo;s spin up ngrok\nngrok tcp localhost:9393 You will be presented with a forward URL ( check Forwarding option on the output )\nFor my example it was: tcp://0.tcp.eu.ngrok.io:15537\nIf you go through a browser and choose http://0.tcp.eu.ngrok.io:15537/\nYou should see the http GET request on the terminal for the listener\nGET / HTTP/1.1 Host: 0.tcp.eu.ngrok.io:15537 Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7 Accept-Encoding: gzip, deflate Accept-Language: en-US,en;q=0.9,pt-PT;q=0.8,pt;q=0.7 You could use a much more interesting example like MockingBird or some other mocking data service, but for reference this will do.\nAnother point to take into consideration is the security part. When you create the endpoint you can define a differente security mechanism (advisible) like Oauth.\nYou could run something like the following to use Google OAuth:\nngrok http http://localhost:8080 --oauth=google --oauth-allow-email=\u0026lt;YOUR_EMAIL\u0026gt; Take into account that IP control will require that you upgrade your plan\nConclusion # ngrok seems to be very easy to setup. Although I didn\u0026rsquo;t go in depth on the CI/CD configuration their Official documentation has more information on that one (example for Github integration). Check there documentation regarding Production usecases.\nFor me personally I think is quite interesting if you want to speed up development and provide a local endpoint quickly to a client to assess in a secure and reliable way. Or quickly provide a webhook for testing something.\nLet me know if you know similar alternatives that I should look into.\nReferences # https://ngrok.com https://ngrok.com/docs/ ","date":"26 December 2023","externalUrl":null,"permalink":"/2023/12/ngrok/","section":"Posts","summary":"","title":"ngrok","type":"posts"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/reserve-proxy/","section":"Tags","summary":"","title":"Reserve Proxy","type":"tags"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/data-modeling/","section":"Tags","summary":"","title":"Data Modeling","type":"tags"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization","type":"tags"},{"content":"In this article I will go through DuckDB an in-process SQL OLAP database management system\nIntro # In this article I will go through DuckDB an in-process SQL OLAP database management system and setup some mock data to do some tests\nSetup # In order to setup duckdb client install the following pip packages.\npip install -U duckcli pip install duckdb==0.9.2 The duckcli allow autocompletion on the terminal\nNOTE: There is other client options like ODBC or Node checkout the official page for those cases.\nGenerate Mock Data # DuckDB reads data directly from files, and support CSV, Parquet, JSON, Excel and more.\nNow lets generate some data and import to our SQL Engine.\njafgen --years 1 This will generate 6 CSV files with mock data\njaffle-data/ ├── raw_customers.csv ├── raw_items.csv ├── raw_orders.csv ├── raw_products.csv ├── raw_stores.csv └── raw_supplies.csv lets start the client and create managed tables for each\nduckcli mydatabase.db The CSVs have a header line so lets create the following tables using the commands\nCREATE TABLE raw_items AS SELECT * FROM read_csv_auto(\u0026#39;jaffle-data/raw_items.csv\u0026#39;,header = true); CREATE TABLE raw_orders AS SELECT * FROM read_csv_auto(\u0026#39;jaffle-data/raw_orders.csv\u0026#39;,header = true); CREATE TABLE raw_products AS SELECT * FROM read_csv_auto(\u0026#39;jaffle-data/raw_products.csv\u0026#39;,header = true); CREATE TABLE raw_stores AS SELECT * FROM read_csv_auto(\u0026#39;jaffle-data/raw_stores.csv\u0026#39;,header = true); CREATE TABLE raw_supplies AS SELECT * FROM read_csv_auto(\u0026#39;jaffle-data/raw_supplies.csv\u0026#39;,header = true); There are other options like defining a official delimiter or specifying the columns. Check the official page for more details on CSV Import.\nmydatabase.db\u0026gt; show tables; +--------------+ | name | +--------------+ | raw_items | | raw_orders | | raw_products | | raw_stores | | raw_supplies | +--------------+ 5 rows in set Time: 0.030s Disclaimer # DuckDB seems to be blazing fast it also has the option to run in-memmory. It is important however to identify in which usecases this backend could present benefits or not.\nWhen or Not to use # DuckDB aims to automatically achieve high performance by using well-chosen default configurations and having a forgiving architecture. Of course, there are still opportunities for tuning the system for specific workloads. The Performance Guide’s page contain guidelines and tips for achieving good performance when loading and processing data with DuckDB.\nWhen to use DuckDB # Processing and storing tabular datasets, e.g., from CSV or Parquet files Interactive data analysis, e.g., join \u0026amp; aggregate multiple large tables Concurrent large changes, to multiple large tables, e.g., appending rows, adding/removing/updating columns Large result set transfer to client When to not use DuckDB # High-volume transactional use cases (e.g., tracking orders in a webshop) Large client/server installations for centralized enterprise data warehousing Writing to a single database from multiple concurrent processes Multiple concurrent processes reading from a single writable database Conclusion # In this article I went through the process to setup duckdb in a local environment and load some data into it. This database has some interesting benchmark values, I would suggest you try this one out especially if your usecase doesn\u0026rsquo;t involve transactional data or multiple concurrent processes reading from a single writable database there for staging processes, development environments or single threaded CDC process seems very interesting.\nI will certainly use this more in the future. Also very poisitive feedback regarding the documentation you can find on the Official website.\nI haven´t found direct support for Delta yet, although it supports Parquet.\nIf you want to understand better why to choose DuckDB please check this article Why DuckDB\nReferences # https://github.com/dbcli/duckcli https://duckdb.org https://duckdb.org/docs/guides/performance/benchmarks https://duckdb.org/docs/data/csv/overview ","date":"26 December 2023","externalUrl":null,"permalink":"/2023/12/duckdb-mock-environment/","section":"Posts","summary":"","title":"DuckDB Mock environment","type":"posts"},{"content":"In this article I will go through Evidence\nIntro # In this article I will go through Evidence\nWhat is Evidence # Evidence is an open source, code-based alternative to drag-and-drop business intelligence tools.\nEvidence reports are written in Evidence-flavored Markdown - an extension of markdown that includes SQL queries, data viz components, and programmatic features.\nSQL # Code fences in Evidence markdown files run inline queries and return data. These code fences run the DuckDB SQL dialect.\nselect date_trunc(\u0026#39;month\u0026#39;, order_datetime) as order_month, count(*) as number_of_orders, sum(sales) as sales_usd from needful_things.orders group by 1, order by 1 desc More information on queries available at Queries Section\nComponents # Evidence has a built in component library to create charts and other visual elements. More on Components.\n\u0026lt;LineChart data = {orders_by_month} y = sales_usd title = \u0026#39;Sales by Month, USD\u0026#39; /\u0026gt; Check the Component Library for more information.\nOne can also use, Loops, If/Then Statements, Partials, Page Variables, and also sorts of customizations checkout the Core Components page for more details.\nIn production, Evidence is a static site generator. This means it doesn\u0026rsquo;t run queries when someone visits your site, but pre-builds all possible pages as HTML beforehand.\nYou can host your Evidence project using Evidence Cloud, cloud services like Netlify or Vercel, or your own infrastructure. Evidence does not currently support Github Pages.\nBuild Process # Evidence doesn\u0026rsquo;t run new queries each time someone visits one of your reports.\nInstead, Evidence runs your queries once, at build time, and statically generates all of the pages in your project. This includes all possible permutations of any parameterized pages.\nYou can schedule (or trigger) regular builds of your site to keep it up-to-date with your data warehouse.\nThis has two benefits for you and your users:\nIf something goes wrong with your SQL, Evidence just stops building your project, and continues to serve older results. Your site will be exceptionally fast. Under most conditions, pages will load in milliseconds.\nRequirements # Requirement Min Version Max Version Node.js 16.14 20.10 NPM 7 10.1 NOTE: This requirements may change based on the evolution of the Product. At the time of this writing was 3.0.2\nSetup # There are several ways to install Evidence, for this article following the traditional one with npx, but there is an option using docker if you prefeer that option\nnpx degit evidence-dev/template evidence-test cd evidence-test npm install npm run sources npm run dev I would suggest however to setup the VSCode Plugin if you use that IDE, as it do all this work underneat :D\nTest # Now let\u0026rsquo;s generate Some Random data using jafgen\ncd evidence-test/sources jafgen --years 1 mv jaffgen-data jaffgendata Create the following file jaffledata/connection.yaml with the content:\n# This file was automatically generated name: jaffledata type: csv options: {} Then run\nnpm run sources NOTE: A better way to configure sources would be by accessing the following URL: http://localhost:3000/settings/\nCreate a sample report # Create the following page pages/jaffle.md with the content\n# Jaffle Example Report This example is based on mock data from jaffle. Let\u0026#39;s check the data ## Orders ```sql orders select * from raw_orders ``` ## Customers ```sql customers select * from raw_customers ``` # Orders by Customer ```sql orders_by_customer select count(o.id) as orders, c.column0 as customerid from raw_orders as o left join raw_customers as c on o.customer = c.column0 group by c.column0 order by orders desc ``` \u0026lt;BarChart data=\u0026#34;{orders_by_customer}\u0026#34; x=\u0026#34;customerid\u0026#34; y=\u0026#34;orders\u0026#34; series=\u0026#34;customerid\u0026#34; title=\u0026#34;Orders by Customer\u0026#34; /\u0026gt; You can test the report through the URL:\nhttp://localhost:3000/jaffle And that\u0026rsquo;s it :)\nConclusion # This article provided a quick intro on how to setup evidence to quickly generate analytical visualization based on Markdown documents. The sample data used was CSV mocked data. In general seems a very useful tools for generating quick reports the renderization in the interface is quite fast.\nPoints that I found some issues. Loading directly from CSV doesn\u0026rsquo;t seem a good approach as there seems to be a lack of configuration, and CSV by nature tend to need some work in terms of data treatment, a rather convert the data to duckdb to mitigate this.\nAlso take into account the pages are static so the refresh rate of our sources is up to you. The good news is that it seems to fit well with some CI/CD process. Github pages not supported at the moment, but there are other options.\nGuess I will try this option further in future. I Strongly recommend to check the Official documentation on the Components and Syntax.\nReferences # https://docs.evidence.dev/getting-started/install-evidence https://github.com/evidence-dev/docker-devenv https://marketplace.visualstudio.com/items?itemName=Evidence.evidence-vscode https://docs.evidence.dev/markdown ","date":"26 December 2023","externalUrl":null,"permalink":"/2023/12/evidence/","section":"Posts","summary":"","title":"Evidence","type":"posts"},{"content":"","date":"25 December 2023","externalUrl":null,"permalink":"/tags/inspiration/","section":"Tags","summary":"","title":"Inspiration","type":"tags"},{"content":"This article is about the Book - The One Thing\nIntro # One of the references that I most enjoyed on the book:\nThe One Thing, by Gary Keller, Jay Papasan Passion for something leads to disproportionate time practicing or working at it. That time spent eventually translates to skill, and when skill improves, results improve. Better results generally lead to more enjoyment, and more passion and more time is invested. It can be a virtuous cycle all the way to extraordinary results\nFind something you love, put your energy on it and good things will come out of it\nMany people struggle to achieve their goals because they lack clear direction and focus. Our One Thing Approach provides a clear path to success.\nReferences # https://the1thing.com/book ","date":"25 December 2023","externalUrl":null,"permalink":"/2023/12/the-one-thing/","section":"Posts","summary":"","title":"The One Thing","type":"posts"},{"content":"In this article I will describe a step-by-step guide on how to setup a local DBT environment\nIntro # In this article I will describe a step-by-step guide on how to setup a local DBT environment and some basic ground rules to use it.\nWhat is DBT # dbt™ is a SQL-first transformation workflow that lets teams quickly and collaboratively deploy analytics code following software engineering best practices like modularity, portability, CI/CD, and documentation\nPre Requirements # In order to use dbt™ one must have the python requirements installed, they may differe based on the environment your are using. Will provide an example for a Linux based system\nsudo apt-get install git libpq-dev python-dev python3-pip pip install dbt-core pip install dbt-postgres Setup a PostgreSQL Environment # One can use different sources such as Snowflake, BigQuery, etc. For the testing purposes lets run a docker image with postgres. The following reop has some options for that\nhttps://github.com/rramos/dockers/tree/master/docker-postgres NOTE: If you will use a different source make sure to install with pip the appropriate connector eg. `pip install dbt-bigquery``\nSetup # Lets initialize the environment\ndbt init Provide a name for the project Include the connection details for the postgresql example host (hostname for the instance): localhost port [5432]: user (dev username): admin pass (dev password): dbname (default database that dbt will build objects in): test_db schema (default schema that dbt will build objects in): dbt threads (1 or more) [1]: dbt deps This will make sure it is installed all the required deps for the project\ndbt run If you get an output similar to the following it means you have everything ready to start using dbt\n13:20:25 Running with dbt=1.7.4 13:20:25 Registered adapter: postgres=1.7.4 13:20:25 Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 401 macros, 0 groups, 0 semantic models 13:20:25 13:20:25 Concurrency: 1 threads (target=\u0026#39;dev\u0026#39;) 13:20:25 13:20:25 1 of 2 START sql table model dbt.my_first_dbt_model ............................ [RUN] 13:20:25 1 of 2 OK created sql table model dbt.my_first_dbt_model ....................... [SELECT 2 in 0.11s] 13:20:25 2 of 2 START sql view model dbt.my_second_dbt_model ............................ [RUN] 13:20:25 2 of 2 OK created sql view model dbt.my_second_dbt_model ....................... [CREATE VIEW in 0.06s] 13:20:25 13:20:25 Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.27 seconds (0.27s). 13:20:25 13:20:25 Completed successfully 13:20:25 13:20:25 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 You can access the web browser to validate the created table and view based on default init: http://localhost:5050/browser/\ntable: my_first_dbt_model view: my_second_dbt_model dbt™ Structure # After you initialize the dbt™ environment you should endup with a structure similar to the following\ntest ├── analyses ├── dbt_packages ├── dbt_project.yml ├── logs │ └── dbt.log ├── macros ├── models │ └── example │ ├── my_first_dbt_model.sql │ ├── my_second_dbt_model.sql │ └── schema.yml ├── README.md ├── seeds ├── snapshots ├── target │ ├── compiled │ │ └── test │ │ └── models │ │ └── example │ │ ├── my_first_dbt_model.sql │ │ ├── my_second_dbt_model.sql │ │ └── schema.yml │ │ ├── not_null_my_first_dbt_model_id.sql │ │ ├── not_null_my_second_dbt_model_id.sql │ │ ├── unique_my_first_dbt_model_id.sql │ │ └── unique_my_second_dbt_model_id.sql │ ├── graph.gpickle │ ├── graph_summary.json │ ├── manifest.json │ ├── partial_parse.msgpack │ ├── run │ │ └── test │ │ └── models │ │ └── example │ │ ├── my_first_dbt_model.sql │ │ ├── my_second_dbt_model.sql │ │ └── schema.yml │ │ ├── not_null_my_first_dbt_model_id.sql │ │ ├── not_null_my_second_dbt_model_id.sql │ │ ├── unique_my_first_dbt_model_id.sql │ │ └── unique_my_second_dbt_model_id.sql │ ├── run_results.json │ └── semantic_manifest.json └── tests Lets check the 1st level directories and what you might find there:\nNOTE: The structure may change based on the version and new features that would be included. For this article the dbt version was 1.7.4\nanalyses # Not tested.\ndbt_packages # Installed packages from the packages HUB will lend on this location. You should not need to change here directly unless you are implementing your own packages.\nOne example to include dbt_utils package is by creating the file packages.yml on the root directory of the project and then one can run dbt deps to fetch the dependencies\nExample:\npackages: - package: dbt-labs/dbt_utils version: 1.1.1 logs # Log directory for dbt™ executions\nmacros # Macros act like small snippets of jinja code that you can than refeer on the models. They are very useful for reusability and colaborative effort.\nOne can call the macro using something similar: {% macro macro_name(params) %}\nmodels # This section is where most of the work is going to be done. Here one can define the SQL associated with each model using jinja syntax\nExample: raw/raw_customer.sql\n{{ config( materialized=\u0026#39;table\u0026#39; ) }} select * FROM {{ source(\u0026#39;bigquery\u0026#39;, \u0026#39;customers\u0026#39;) }} seeds # Useful for temporary data that incorporated and used as __ref . The data should exist as CSV\nex:\nshipmode,delivery_team First Class, RHL_Couriers Second Class, RHL_Couriers Standard Class, RHL_Couriers Same Day, Globalmart snapshots # Not tested\ntarget # This directory will hold compile sql bsed on your jinja spec and also other outcomes such as the graphs for the documentation.\ntests # Tests for your model were you can defined rules such as nullability, dictionary of terms, etc.\nOne can use the dbt test command to check if the there is something failing.\nNOTE: Please check the Best Practises Guide as it should provide more insigthful information. You can also check the following example repo https://github.com/rramos/dbt-training which holds some test data\nLoading Example Data # The example data is based on the udemy bootcamp from Vantage Point, check the references section with the link for the course as it is very insigthful.\nThe following repo already has most of the things done and could be useful as a starting point:\nhttps://github.com/rramos/dbt-training Data Information # Data is composed of 3 CSV files: customers, orders and product data Following a traditional Medallions Architecture (AKA bronze, silver gold) Implement a (raw) layer based on the source data Implement a (staging) where we hold aggregation logic Implement Gold layer (reporting) with materialization data for reporting purposes dbt™ Cloud # dbtLabs is the company behind dbt™. Although one can use the community version of dbt-core packages the company sells a solution that helps their clients using a friendly graphical interface to take advantage of dbt™ in a Enterprise way.\nYou can check out their website to understand better the features associated to it:\nhttps://www.getdbt.com/product/dbt-cloud Conclusion # In this article it was presented a high-level overview on dbt™ how it can be setup and provided some references for best practises and how to take most advantage of the product.\nMy personal view on this product is that it brings better control for data assets with embedded lineage also documentation generation and testing features all integrated in a common stack allowing the CI/CD process to be a \u0026ldquo;walk-in-the-park\u0026rdquo;\nThe community is very large which is a plus (https://www.getdbt.com/community ). DBTCloud provides an Entreprise level solution that would speed up Data Engineering work via UI, I didn\u0026rsquo;t go deeper on the costs to understand if it pays-off.\nOnly having the dbt-core option seems to be quite limited compared with the Entreprise solution, but the community supports several features which should be consider if you don\u0026rsquo;t want to go in that path (like the integration with Airflow).\nI\u0026rsquo;m not a big fan of jinja option as the main syntax for the Product, but that is more of a personal flavor.\nReferences # The Data Bootcamp: Transform your Data using dbt™ https://www.databricks.com/glossary/medallion-architecture https://github.com/rramos/dockers https://docs.getdbt.com/ https://docs.getdbt.com/guides/codespace?step=1 https://docs.getdbt.com/guides/airflow-and-dbt-cloud?step=1 https://docs.getdbt.com/best-practices https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview https://github.com/rramos/dbt-training ","date":"21 December 2023","externalUrl":null,"permalink":"/2023/12/dbt-quick-walkthrough/","section":"Posts","summary":"","title":"DBT Quick WalkThrough","type":"posts"},{"content":"Article about mock generation with Jafgen\nIntro # In this article I will provide a quick guide to generate mock data based on Jafgen, which you can use for your local projects\nWhat is JafGen # The Jaffle Shop Generator or jafgen is a python package with a simple command line tool for generating a synthetic data set suitable for analytics engineering practice or demonstrations.\nSetup # Install via pip\npip install jafgen Generate data # Generate data\njafgen --years 1 The output of this command is store on the following folder jaffle-data:\njaffle-data/ ├── raw_customers.csv ├── raw_items.csv ├── raw_orders.csv ├── raw_products.csv ├── raw_stores.csv └── raw_supplies.csv Example output\n==\u0026gt; raw_customers.csv \u0026lt;== id,name f32ab6bb-4912-4319-af20-cf681b797868,Julia Henderson c07e5161-e8c4-419f-bdeb-3a2271691c75,James Gutierrez 63c021e4-4d59-4958-aa35-ca5f72b1cad9,Janet Robles ==\u0026gt; raw_items.csv \u0026lt;== id,order_id,sku f0a084b2-e702-41c9-b748-64e0688dabbf,6190ef46-3be2-4dfe-bbd5-519fc85a36b2,BEV-003 946a7047-f074-444a-9d57-6f9e4df587c7,4e9745e8-2244-42aa-8337-8342adeda83c,BEV-005 dd08b68d-448a-4e2d-9961-9226dacefba5,558f3180-3d98-415f-8197-5cd21b16cb83,BEV-003 ==\u0026gt; raw_orders.csv \u0026lt;== id,customer,ordered_at,store_id,subtotal,tax_paid,order_total 6190ef46-3be2-4dfe-bbd5-519fc85a36b2,f32ab6bb-4912-4319-af20-cf681b797868,2016-09-01T08:20:00,c1d21879-9db8-410a-af61-c0a2c91881d3,600,36,636 4e9745e8-2244-42aa-8337-8342adeda83c,c07e5161-e8c4-419f-bdeb-3a2271691c75,2016-09-01T07:53:00,c1d21879-9db8-410a-af61-c0a2c91881d3,400,24,424 558f3180-3d98-415f-8197-5cd21b16cb83,63c021e4-4d59-4958-aa35-ca5f72b1cad9,2016-09-01T07:26:00,c1d21879-9db8-410a-af61-c0a2c91881d3,600,36,636 ==\u0026gt; raw_products.csv \u0026lt;== sku,name,type,price,description JAF-001,nutellaphone who dis?,jaffle,1100,nutella and banana jaffle JAF-002,doctor stew,jaffle,1100,house-made beef stew jaffle JAF-003,the krautback,jaffle,1200,lamb and pork bratwurst with house-pickled cabbage sauerkraut and mustard ==\u0026gt; raw_stores.csv \u0026lt;== id,name,opened_at,tax_rate c1d21879-9db8-410a-af61-c0a2c91881d3,Philadelphia,2016-09-01T00:00:00,0.06 e03fe3a7-86cb-4e58-b4b0-462bff6abdfd,Brooklyn,2017-03-12T00:00:00,0.04 a86be955-8d22-4440-960d-6fd0ac7522a6,Chicago,2018-04-29T00:00:00,0.0625 ==\u0026gt; raw_supplies.csv \u0026lt;== id,name,cost,perishable,sku SUP-001,compostable cutlery - knife,7,False,JAF-001 SUP-002,cutlery - fork,7,False,JAF-001 SUP-003,serving boat,11,False,JAF-001 This files can than be imported to your SQL Engine for development/demonstration purposes\nReferences # https://pypi.org/project/jafgen/ ","date":"21 December 2023","externalUrl":null,"permalink":"/2023/12/data-generation-with-jafgen/","section":"Posts","summary":"","title":"Data Generation with jafgen","type":"posts"},{"content":"","date":"21 December 2023","externalUrl":null,"permalink":"/tags/mock-data/","section":"Tags","summary":"","title":"Mock Data","type":"tags"},{"content":"Article with general interview questions\nIntro # I saw this article with a very interesting consolidation of questions a candidate should ask when trying to find it\u0026rsquo;s ideal job.\nThis list is very useful independently on the side of the chair you are on:\nAbout the Job # What are the biggest challenges I might face in this position ? Why did the current person in this position leave ? How do you expect my responsibilities to involve as time goes on ? How will I be measured for success ? About the Company # What are the current goals of the Company ? How would you describe the company\u0026rsquo;s values ? How have the company\u0026rsquo;s values evolved in the time you\u0026rsquo; ve worked there ? What is the company\u0026rsquo;s approach to professional development and growth ? About the Culture # How does this company invest in employee well-being ? Are there any efforts or initiatives in place to ensure diversity and inclusion in this company ? What is the general sentiment of culture in the company, according to current employees ? How does management react in situations that may result in low company morale ? ","date":"17 October 2023","externalUrl":null,"permalink":"/2023/10/general-interview-questions/","section":"Posts","summary":"","title":"General Interview Questions","type":"posts"},{"content":"","date":"17 October 2023","externalUrl":null,"permalink":"/tags/hiring/","section":"Tags","summary":"","title":"Hiring","type":"tags"},{"content":"","date":"29 August 2023","externalUrl":null,"permalink":"/tags/api/","section":"Tags","summary":"","title":"API","type":"tags"},{"content":"This article resumes the video by Alex Xu and Sahn Lam regarding 6 types of API Architectures and there usecases\nIntro # This article resumes the video by Alex Xu and Sahn Lam regarding 6 types of API Architectures and there usecases. Checkout there youtube channel as it has very interesting content\nSOAP # XML-based for enterprise application where security and reliability are key, but it could be overkill if you are developing a webapp for instance due to fact that it is complex.\nRESTful # Resource-based for webservers, implemented on top of HTTP methods very popular in web development. It may not be the best option if you need real time data or operate with a highly connected data model may not be the best choice\nGraphQL # query language reduce network load. allow the client to ask for the specific data that it needs, more faster responses as you don\u0026rsquo;t need to over fetch data you don\u0026rsquo;t require. Good choice for applications with complex data requirements\ngRPC # High performance for microservices, used protocol buffers by default. Good choice for micro-services architecture for inter-service communications but if dealing with browser client there are some limitations\nWebSocket # Bi-directional for low-latency data exchange. Perfect for real chat applications and real time gaming where low latency exchange is crucial\nWebhook # Asynchronous for event-driven application. Very useful if you don\u0026rsquo; t need immediate response and latency is not an issue\nReferences # https://www.youtube.com/watch?v=4vLxWqE94l4\u0026ab_channel=ByteByteGo ","date":"29 August 2023","externalUrl":null,"permalink":"/2023/08/api-architectures/","section":"Posts","summary":"","title":"API Architectures","type":"posts"},{"content":"","date":"29 August 2023","externalUrl":null,"permalink":"/tags/software/","section":"Tags","summary":"","title":"Software","type":"tags"},{"content":"","date":"27 August 2023","externalUrl":null,"permalink":"/tags/aur/","section":"Tags","summary":"","title":"AUR","type":"tags"},{"content":" Excertp # In this article i will review how AUR packages are defined.\nIntro # In this article i will review how AUR packages are defined. I used to create rpm and debian packages, but switched recently to use Arch Linux. As such makes sense to review how to create a aur package and how does it works the ecosystem.\nArch Packages # The Arch User Repository (AUR) is a community-driven repository for Arch users. It contains package descriptions (PKGBUILDs) that allow you to compile a package from source with makepkg and then install it via pacman. The AUR was created to organize and share new packages from the community and to help expedite popular packages\u0026rsquo; inclusion into the extra repository.\nGetting Started # Ensure base-devel is installed. Adjust /etc/makepkg.conf /etc/makepkg.conf # You may wish to adjust /etc/makepkg.conf to optimize the build process to your system prior to building packages from the AUR. A significant improvement in package build times can be realized on systems with multi-core processors by adjusting the MAKEFLAGS variable, by using multiple cores for compression, or by using different compression algorithm. Users can also enable hardware-specific compiler optimizations via the CFLAGS variable.\nI will just adjust the parallelism for compilation to use the 8-cores I have\nMAKEFLAGS=\u0026#34;-j8\u0026#34; The overall flags seem to be very optimized already, checkout the article if you want to go into that rabbit hole\nInstalling and upgrading packages # Installing packages from the AUR is a relatively simple process. Essentially:\n1.Acquire the build files, including the PKGBUILD and possibly other required files, like systemd units and patches (often not the actual code). 1.Verify that the PKGBUILD and accompanying files are not malicious or untrustworthy. 1.Run makepkg in the directory where the files are saved. This will download the code, compile it, and package it. 1.Run pacman -U package_file to install the package onto your system.\nAcquire build files # Locate the package in the AUR. This is done using the search field at the top of the AUR home page. Clicking the application\u0026rsquo;s name in the search list brings up an information page on the package. Read through the description to confirm that this is the desired package, note when the package was last updated, and read any comments.\nClone git the repository for the package\ngit clone https://aur.archlinux.org/package_name.git Acquire a PGP public key if needed # Check if a signature file in the form of .sig or .asc is part of the PKGBUILD source array. If that is the case, then acquire one of the public keys listed in the PKGBUILD validpgpkeys array\nBuild the package # Change directories to the directory containing the package\u0026rsquo;s PKGBUILD.\ncd package_name makepkg -si Install the package # The package can now be installed with pacman:\npacman -U package_name-version-architecture.pkg.tar.zst Lets test # Using one example package paru. one can use the following commands\nLet\u0026rsquo;s check available updates with paru\nparu -Qua PKGBUILD # The following manpage PKGBUILD(5) provides more in-depth information regarding the PKBUILD package description file.\nCreating Packages # Overview # Packages in Arch Linux are built using the makepkg utility and the information stored in a PKGBUILD file. When makepkg runs, it searches for a PKGBUILD in the current directory and follows the instructions in it to acquire the required files and/or compile them to be packed within a package file (pkgname.pkg.tar.zst). The resulting package contains binary files and installation instructions ready to be installed by pacman.\nAn Arch package is no more than a tar archive, or \u0026rsquo;tarball\u0026rsquo;, compressed using zstd(1), which contains the following files generated by makepkg:\nThe binary files to install:\n.PKGINFO: contains all the metadata needed by pacman to deal with packages, dependencies, etc. .BUILDINFO: contains information needed for reproducible builds. This file is present only if a package is built with pacman 5.1 or newer. See BUILDINFO(5). .MTREE: contains hashes and timestamps of the files, which are included in the local database so that pacman can verify the integrity of the package. .INSTALL: an optional file used to execute commands after the install/upgrade/remove stage. (This file is present only if specified in the PKGBUILD.) .Changelog: an optional file kept by the package maintainer documenting the changes of the package. (It is not present in all packages.) Preparation # First, ensure that the necessary tools are installed: the meta package base-devel should be sufficient; it pulls in make and additional tools needed for compiling from source.\nThe key tool for building packages is makepkg (provided by pacman), which does the following:\nChecks if package dependencies are installed. Downloads the source file(s) from the specified server(s). Unpacks the source file(s). Compiles the software and installs it under a fakeroot environment. Strips symbols from binaries and libraries. Generates the package meta file which is included with each package. Compresses the fakeroot environment into a package file. Stores the package file in the configured destination directory, which is the current working directory by default. Download the source tarball of the software you want to package, extract it, and follow the author\u0026rsquo;s steps to install the program. Make a note of all commands and/or steps needed to compile and install it. You will be repeating those same commands in the PKGBUILD file.\nMost software authors stick to the 3-step build cycle:\n./configure make make install Creating a PKGBUILD # To start building a new package, first create a new directory for the package and change current directory into this one. Then, a PKGBUILD file needs to be created: a prototype PKGBUILD found in /usr/share/pacman/ can be used or you can start from a PKGBUILD from another package. The latter may be a good choice if a similar package already exists.\nmakepkg defines two variables that you should use as part of the build and install process:\nsrcdir: This points to the directory where makepkg extracts or symlinks all files in the source array. pkgdir: This points to the directory where makepkg bundles the installed package, which becomes the root directory of your built package. PKGBUILD functions # When building a package, makepkg will invoke the following five functions if they have been defined in the PKGBUILD. Function package() is required in every PKGBUILD and will always be invoked. If any of the other functions is not defined, makepkg will simply skip the invocation of that function.\nprepare(): With this function, commands that are used to prepare sources for building are run, such as patching. This function runs right after package extraction, before pkgver() and the build function. If extraction is skipped (makepkg --noextract), then prepare() is not run\npkgver(): pkgver() runs after the sources are fetched, extracted and prepare() executed. So you can update the pkgver variable during a makepkg stage. This is particularly useful if you are making git/svn/hg/etc. packages, where the build process may remain the same, but the source could be updated every day, even every hour.\nbuild(): Now, you need to implement the build() function in the PKGBUILD file. This function uses common shell commands in Bash syntax to automatically compile software and create a directory called pkg to install the software to. This allows makepkg to package files without having to sift through your file system.\ncheck(): Place for calls to make check and similar testing routines. It is highly recommended to have check() as it helps to make sure software has been built correctly and works fine with its dependencies.\npackage(): The final step is to put the compiled files in a directory where makepkg can retrieve them to create a package. This by default is the pkg directory—a simple fakeroot environment. The pkg directory replicates the hierarchy of the root file system of the software\u0026rsquo;s installation paths. If you have to manually place files under the root of your filesystem, you should install them in the pkg directory under the same directory structure. For example, if you want to install a file to /usr/bin, it should instead be placed under $pkgdir/usr/bin. Very few install procedures require the user to copy dozens of files manually. Instead, for most software, calling make install will do so.\nLets create our package # For testing purposes lets create a hello world application in rust and package it as a new AUR\nCreate a new directory for this app (make sure you have rust installed)\ncargo new myhello cd myhello cargo run Great we have a Hello World App\nLet\u0026rsquo;s clean up\ncargo clean You should have this structure\n. ├── Cargo.lock ├── Cargo.toml └── src └── main.rs Lets add a cargo module to generate our PKGBUILD file\ncargo install cargo-aur Let\u0026rsquo;s add some Metadata required for generating the AUR on Cargo.toml file\n[package] name = \u0026#34;myhello\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; authors = [\u0026#34;Rui Ramos\u0026#34;, \u0026#34;Rui Ramos \u0026lt;no-reply@rust-lang.org\u0026gt;\u0026#34;] description = \u0026#34;A HelloWorld Application for AUR testing purposes\u0026#34; homepage=\u0026#34;https://rramos.github.io\u0026#34; repository = \u0026#34;https://github.com/rramos/rust-helloworld.git\u0026#34; license = \u0026#34;MIT\u0026#34; # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] For a complete list please check the Cargo Manifest page.\nNow run the following command\ncargo aur We now have a PKGBUILD package generated for this application with something similar\n# Maintainer: Rui Ramos # Maintainer: Rui Ramos \u0026lt;no-reply@rust-lang.org\u0026gt; # # This PKGBUILD was generated by `cargo aur`: https://crates.io/crates/cargo-aur pkgname=myhello-bin pkgver=0.1.0 pkgrel=1 pkgdesc=\u0026#34;A HelloWorld Application for AUR testing purposes\u0026#34; url=\u0026#34;https://rramos.github.io\u0026#34; license=(\u0026#34;MIT\u0026#34;) arch=(\u0026#34;x86_64\u0026#34;) provides=(\u0026#34;myhello\u0026#34;) conflicts=(\u0026#34;myhello\u0026#34;) source=(\u0026#34;https://github.com/rramos/rust-helloworld.git/releases/download/v$pkgver/myhello-$pkgver-x86_64.tar.gz\u0026#34;) sha256sums=(\u0026#34;ef8ce03c73185ef872116c17971c21f6f8f2c8f67d42a5c0b4ca876d164191b2\u0026#34;) package() { install -Dm755 myhello -t \u0026#34;$pkgdir/usr/bin\u0026#34; install -Dm644 LICENSE \u0026#34;$pkgdir/usr/share/licenses/$pkgname/LICENSE\u0026#34; } It also generated a tarball for the application myhello-0.1.0-x86_64.tar.gz .\nNow lets use pacman to build and install the package\nmakepkg -si You should now have a executable myhello application congrats.\nClean Up # Let\u0026rsquo;s remove our application using the command\nsudo pacman -R myhello-bin Checking package sanity # After testing package functionality, check it for errors using namcap\nnamcap PKGBUILD namcap \u0026lt;package file name\u0026gt;.pkg.tar.zst Submitting packages to the AUR # In order to contribute to the community one can submit the AUR package. For that read AUR submission guidelines for a detailed description of the submission process.\nYou would need one ssh account and key.\nIf you are developing in Rust I would advise following the guide from Manoj Karthick\nConclusion # AUR packages seem quite similar to debian or rpm packages in a sense that you need to specify a set of instructions for compiling and installing, and those functions would be called by the packaging system. I liked the simplified structure of it and cargo-aur package is amazing for rust developers who want to quickly prepare and package their applications.\nOn the article I didn\u0026rsquo;t deep dive on more complex preparation/compilation options that some applications have or dependencies. Also is important to check the Arch guidelines if you intend to release your application for the community and also follow good practices not only for the package build but also files organization.\nIn general as a past RedHat, Debian and Gentoo Linux user, I\u0026rsquo;m really becoming a fan of this packaging system.\nHope this helps, the following References section contains the source of information used to consolidate this article.\nReferences # https://aur.archlinux.org/ https://wiki.archlinux.org/title/pacman https://wiki.archlinux.org/title/official_repositories https://wiki.archlinux.org/title/Arch_package_guidelines https://wiki.archlinux.org/title/general_recommendations https://wiki.archlinux.org/title/Arch_User_Repository https://wiki.archlinux.org/title/Arch_User_Repository#Frequently_asked_questions https://wiki.archlinux.org/title/PKGBUILD#validpgpkeys https://wiki.archlinux.org/title/Creating_packages https://wiki.archlinux.org/title/AUR_submission_guidelines https://manojkarthick.com/posts/2021/03/rust-aur/ https://doc.rust-lang.org/cargo/reference/manifest.html ","date":"27 August 2023","externalUrl":null,"permalink":"/2023/08/aur-packages/","section":"Posts","summary":"","title":"AUR Packages","type":"posts"},{"content":"Article about Minikube a local kubernetes\nIntro # minikube is local Kubernetes, focusing on making it easy to learn and develop for Kubernetes.\nLets start by installing the minikube package.\nNOTE: This example is targeting a Arch distribution, if you use a different linux flavor check the Official Docs for other install options\nsudo pacman -S minikube The following command will setup a docker environment with a kubernetes environment that you can use for development/testing purposes\nminikube start In order to interact with Kubernetes one can use the provided kubectl command upon starting this environment with the following command\nminikube kubectl -- get pods -A One should get something like this\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-787d4945fb-zv72d 1/1 Running 0 10m kube-system etcd-minikube 1/1 Running 0 10m kube-system kube-apiserver-minikube 1/1 Running 0 10m kube-system kube-controller-manager-minikube 1/1 Running 0 10m kube-system kube-proxy-7p9qw 1/1 Running 0 10m kube-system kube-scheduler-minikube 1/1 Running 0 10m kube-system storage-provisioner 1/1 Running 1 (10m ago) 10m I will install kubectl package however to allow interaction with other clusters if required.\nsudo pacman -S kubectl We should get the same output by executing\nkubectl get po -A One can check the cluster details with following command\nkubectl cluster-info You should get something similar\nKubernetes control plane is running at https://192.168.49.2:8443 CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Minikube is also bundled with Kubernetes Dashboard which you can launch with the following command\nminikube dashboard You would get something similar to the following image:\nGreat now we have a Kubernetes cluster running in docker and kubectl command available.\nFor a more extensive guideline on how to manage Minikube please checkout the handbook\nNext steps would containerize some application and deploy it.\nBut that would be for another article\u0026hellip;\nReferences # https://minikube.sigs.k8s.io/docs/start/ https://wiki.archlinux.org/title/Minikube https://wiki.archlinux.org/title/Kubernetes https://minikube.sigs.k8s.io/docs/handbook/ ","date":"26 August 2023","externalUrl":null,"permalink":"/2023/08/local-kubernetes-for-testing/developing/","section":"Posts","summary":"","title":"Local Kubernetes for Testing/Developing","type":"posts"},{"content":"Article on how to bypass Kobo wizard in Linux\nIntro # This article is a copy from Curtis Gedak in the following reference.\nKobo eReaders can no longer be set up using the Kobo Desktop on GNU/Linux (see warning below). Fortunately there is a work around to get your Kobo eReader working without a Kobo Account.\nIf you have a kobo reader stuck in the setup wizard as it happen to me (wifi got broken) there is a quick setup you can do to bypass the initial setup wizard.\nNotice: This procedure was done in a old Kobo ebook reader, and is not guarantee that it will work on newer versions, use at your own risk\nIssue # Kobo ebook reader stuck on the initial setup phase. Wifi doesn\u0026rsquo;t work and plugging to a computer requires the windows software installed, which for linux users doesn\u0026rsquo;t not work.\nSolution # Create a dummy user on the device to bypass the initial setup\nPre-Requisites # You need to install sqlitebrowser\nUsing the following code for Arch there should be a similar option Ubuntu or Fedora distributions\nsudo pacman -S sqlitebrowser Procedure # Plug you ebook reader via usb You can check the mount points you should see a /media/your-user-name/KOBOeReader device . Assuming this path as reference Connect with SLQliteBrowser sqlitebrowser /media/your-user-name/KOBOeReader/.kobo/KoboReader.sqlite Click on the Execute SQL tab and in the SQL string: section enter the following statement insert into USER (UserID, UserKey, UserDisplayName, UserEmail, ___DeviceID) values (\u0026#34;foo\u0026#34;, \u0026#34;foo\u0026#34;, \u0026#34;foo\u0026#34;, \u0026#34;foo\u0026#34;,\u0026#34;foo\u0026#34;); Save the changes\nSave these changes by selecting the menu option: File -\u0026gt; Save Database or File -\u0026gt; Write Changes. Close the sqlitebrowser by selecting the menu option: File -\u0026gt; Exit.\nSafely unmount your Kobo eReader.\nAnd voilá, your eReader should now boot up and display the QUICK TOUR screen\nReference # https://gedakc.users.sourceforge.net/display-doc.php%3Fname%3Dkobo-desktop-ereader-setup ","date":"21 August 2023","externalUrl":null,"permalink":"/2023/08/how-to-bypass-kobo-wizard-in-linux/","section":"Posts","summary":"","title":"How to bypass Kobo wizard in Linux","type":"posts"},{"content":"","date":"21 August 2023","externalUrl":null,"permalink":"/tags/kobo/","section":"Tags","summary":"","title":"Kobo","type":"tags"},{"content":"","date":"21 August 2023","externalUrl":null,"permalink":"/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting","type":"tags"},{"content":"In this article we are going to explore some Software Engineering Principals\nIntro # When developing software it is important not to forget some important principals\nModularity # Break complex software into smaller, more manageable pieces that can be developed, tested, and maintained independently.\nAbstraction # Hide the implementation details of a module or component behind an interface or API, so that users of that component only need to know how to interact with it, not how it works.\nSeparation of Concerns # Organize software into separate layers, modules, or components, each with a specific responsibility or concern, to minimize dependencies and improve maintainability.\nEncapsulation # Limit the visibility of data and functions to only the parts of the software that need to interact with them, to prevent unintended interference and improve maintainability.\nSingle Responsibility Principle # Ensure that each module or component has only one responsibility, so that changes to one aspect of the software do not affect unrelated parts.\nOpen/Closed Principle # Design modules and components so that they are open for extension (i.e., new functionality can be added) but closed for modification (i.e., existing functionality does not need to be changed).\nDon\u0026rsquo;t Repeat Yourself (DRY) # Eliminate duplication of code or logic within a software system, to improve maintainability and reduce the risk of errors.\nTest-Driven Development # Write automated tests before writing code, to ensure that the code is correct and to improve maintainability.\nContinuous Integration and Continuous Deployment (CI/CD) # Automate the process of building, testing, and deploying software, to reduce the risk of errors and to improve efficiency.\nCode Reviews # Have other developers review code changes before they are merged into the main codebase, to catch errors and improve maintainability.\nThink on ways to incorporate this principals not only on the design phase but also on your daily development activities.\n","date":"25 July 2023","externalUrl":null,"permalink":"/2023/07/software-engineering-principals/","section":"Posts","summary":"","title":"Software Engineering Principals","type":"posts"},{"content":" Excertpt # This article gather several ChatGPT prompts\nIntro # This article is a resume of Travis Tang Medium article reference on the Reference section. According to Travis Chat GPT performs well in explaining things and providing boilerplate code, however falters when comes to debugging or creating complex snippets.\nI totally agree with Travis mention\nWill ChatGPT replace data scientists? No, I don’t think so. At least not yet. However someone using ChatGPT might.\nI hope these prompts are helpful for you! It certainly helped me.\nCheck him in https://medium.com/@travis-tang\nWrite Python # Train Classification Model # Prompt: I want you to act as a data scientist and code for me. I have a dataset of [describe dataset]. Please build a machine learning model that predict [target variable].\nAutomatic Machine Learning # Prompt: I want you to act as an automatic machine learning (AutoML) bot using TPOT for me. I am working on a model that predicts [\u0026hellip;]. Please write python code to find the best classification model with the highest AUC score on the test set.\nTune Hyperparameter # Prompt: I want you to act as a data scientist and code for me. I have trained a [model name]. Please write the code to tune the hyper parameters.\nExplore Data # Prompt: I want you to act as a data scientist and code for me. I have a dataset of [describe dataset]. Please write code for data visualisation and exploration.\nGenerate Data # Prompt: I want you to act as a fake data generator. I need a dataset that has x rows and y columns: [insert column names]\nGet Feature Importance # Prompt: I want you to act as a data scientist and explain the model’s results. I have trained a decision tree model and I would like to find the most important features. Please write the code.\nVisualize Data with Matplotlib # Prompt: I want you to act as a coder in python. I have a dataset [name] with columns [name]. [Describe graph requirements]\nVisualize Image Grid Matplotlib # Prompt: I want you to act as a coder. I have a folder of images. [Describe how files are organised in directory] [Describe how you want images to be printed]\nExplain Model with Lime # Prompt: I want you to act as a data scientist and explain the model’s results. I have trained a [library name] model and I would like to explain the output using LIME. Please write the code.\nExplain Model with Shap # Prompt: I want you to act as a data scientist and explain the model’s results. I have trained a scikit-learn XGBoost model and I would like to explain the output using a series of plots with Shap. Please write the code.\nWrite Multithreaded Functions # Prompt: I want you to act as a coder. Can you help me parallelize this code across threads in python?\nCompare Function Speeds # Prompt: I want you to act as a software developer. I would like to compare the efficiency of two algorithms that performs the same thing in python. Please write code that helps me run an experiment that can be repeated for 5 times. Please output the runtime and other summary statistics of the experiment. [Insert functions]\nCreate NumPy Array # Prompt: I want you to act as a data scientist. I need to create a numpy array. This numpy array should have the shape of (x,y,z). Please initialize the numpy array with random values.\nTest Code # Write Unit Test # Prompt: I want you to act as a software developer. Please write unit tests for the function [Insert function]. The test cases are: [Insert test cases]\nValidate Column # Prompt: I want you to act as a data scientist. Please write code to test if that my pandas Dataframe [insert requirements here]\nExplain Code # Explain Python # Prompt: I want you to act as a code explainer. What is this code doing? [Insert code]\nExplain SQL # Prompt: I want you to act as a data science instructor. Can you please explain to me what this SQL code is doing? [Insert SQL code]\nExplain Google Sheets Formula # Prompt: I want you to act as a Google Sheets formula explainer. Explain the following Google Sheets command. [Insert formula]\nOptimize Code # Prompt: I want you to act as a software developer. Please help me improve the time complexity of the code below. [Insert code]\nImprove Code Speed # Prompt: I want you to act as a code optimizer. Can you point out what’s wrong with the following Pandas code and optimize it? [Insert code here]\nOptimize Pandas # Prompt: I want you to act as a code optimizer. Can you point out what’s wrong with the following Pandas code and optimize it? [Insert code here]\nOptimize Pandas Again # Prompt: I want you to act as a code optimizer. Can you point out what’s wrong with the following Pandas code and optimize it? [Insert code here]\nOptimize Python # Prompt: I want you to act as a code optimizer. The code is poorly written. How do I correct it? [Insert code here]\nOptimize SQL # Prompt: I want you to act as an SQL code optimizer. The following code is slow. Can you help me speed it up? [Insert SQL]\nFormat code # Simplify Python # Prompt: I want you to act as a code simplifier. Can you simplify the following code?\nWrite Documentation # Prompt: I want you to act as a software developer. Please provide documentation for func1 below. [Insert function]\nImprove Readability # Prompt: I want you to act as a code analyzer. Can you improve the following code for readability and maintainability? [Insert code]\nFormat SQL # Prompt: I want you to act as a SQL formatter. Please format the following SQL code. Please convert all reserved keywords to uppercase [Insert requirements]. [Insert Code]\nTranslate Code # Translate Between DBMS # Prompt: I want you to act as a coder and write SQL code for MySQL. What is the equivalent of PostgreSQL’s DATE_TRUNC for MySQL?\nTranslate Python to R # Prompt: I want you to act as a code translator. Can you please convert the following code from python to R? [Insert code]\nTranslate R to Python # Prompt: I want you to act as a code translator. Can you please convert the following code from R to python? [Insert code]\nExplain concepts # Explain to Five-Year-Old # Prompt: I want you to act as a data science instructor. Explain [concept] to a five-year-old.\nExplain to Undergraduate # Prompt: I want you to act as a data science instructor. Explain [concept] to an undergraduate.\nExplain to Professor # Prompt: I want you to act as a data science instructor. Explain [concept] to a professor.\nExplain to Business Stakeholder # Prompt: I want you to act as a data science instructor. Explain [concept] to a business stakeholder.\nExplain Like Stackoverflow # Prompt: I want you to act as an answerer on StackOverflow. You can provide code snippets, sample tables and outputs to support your answer. [Insert technical question]\nSuggest ideas # Suggest Edge Cases # Prompt: I want you to act as a software developer. Please help me catch edge cases for this function [insert function]\nSuggest Dataset # Prompt: I want you to act as a data science career coach. I want to build a predictive model for [\u0026hellip;]. At the same time, I would like to showcase my knowledge in [\u0026hellip;]. Can you please suggest the five most relevant datasets for my use case?\nSuggest Portfolio Ideas # Prompt: I want you to act as a data science coach. My background is in [\u0026hellip;] and I would like to [career goal]. I need to build a portfolio of data science projects that will help me land a role in [\u0026hellip;] as a [\u0026hellip;]. Can you suggest five specific portfolio projects that will showcase my expertise in [\u0026hellip;] and are of relevance to [company]?\nSuggest Resources # Prompt: I want you to act as a data science coach. I would like to learn about [topic]. Please suggest 3 best specific resources. You can include [specify resource type]\nSuggest Time Complexity # Prompt: I want you to act as a software developer. Please compare the time complexity of the two algorithms below. [Insert two functions]\nSuggest Feature Engineering # Prompt: I want you to act as a data scientist and perform feature engineering. I am working on a model that predicts [insert feature name]. There are columns: [Describe columns]. Can you suggest features that we can engineer for this machine learning problem?\nSuggest A/B Testing Steps # Prompt: I want you to act as a statistician. [Describe context] Please design an A/B test for this purpose. Please include the concrete steps on which statistical test I should run.\nCareer Coaching # Prompt: I want you to act as a career advisor. I am looking for a role as a [role name]. My background is [\u0026hellip;]. How do I land the role and with what resources exactly in 6 months?\nTroubleshoot problem # Correct ChatGPT Code # Prompt: Your above code is wrong. [Point out what is wrong]. Can you try again?\nCorrect Python Code # Prompt: I want you to act as a software developer. This code is supposed to [expected function]. Please help me debug this python code that cannot be run. [Insert function]\nCorrect SQL Code # Prompt: I want you to act as a SQL code corrector. This code does not run in [your DBMS, e.g. PostgreSQL]. Can you correct it for me? [SQL code here]\nTroubleshoot PowerBI Model # Prompt: I want you to act as a PowerBl modeler. Here is the details of my current project. [Insert details]. Do you see any problems with the table?\nWrite SQL # Create Running Average # Prompt: I want you to act as a data scientist and write SQL code for me. I have a table with two columns [Insert column names]. I would like to calculate a running average for [which value]. What is the SQL code that works for PostgreSQL 14?\nSolve Leetcode Question # Prompt: Assume you are given the tables… with the columns… Output the following…Prompt: Assume you are given the tables… with the columns… Output the following…\nWrite other Code # Write Google Sheets Formula # Prompt: I want you to act as a bot that generates Google Sheets formula. Please generate a formula that [describe requirements]\nWrite R # Prompt: I want you to act as a data scientist using R. Can you write an R script that [Insert requirement here]\nWrite Shell # Prompt: I want you to act as a Linux terminal expert. Please write the code to [describe requirements]\nWrite VBA # Prompt: I want you to act as an Excel VBA developer. Can you write a VBA that [Insert function here]?\nMiscellaneous # Format Tables # Prompt: I want you to act as a document formatter. Please format the following into a nice table for me to place in Google Docs? [insert text table here]\nSummarize Book # Prompt: I want you to act as a technical book summarizer. Can you please summarize the book [name] with 5 main points?\nSummarize Paper # Prompt: I want you to act as an academic. Please summarise the paper [\u0026hellip;] in simple terms in one paragraph.\nProvide Emotional Support # Prompt: I want you to provide emotional support to me. [Explain problem here.]\nReference # https://medium.datadriveninvestor.com/60-chatgpt-prompts-for-data-science-tried-tested-and-rated-4994c7e6adb2 ","date":"25 July 2023","externalUrl":null,"permalink":"/2023/07/chat-gpt-data-science-prompts/","section":"Posts","summary":"","title":"Chat-GPT Data Science Prompts","type":"posts"},{"content":"","date":"25 July 2023","externalUrl":null,"permalink":"/tags/chatgpt/","section":"Tags","summary":"","title":"ChatGPT","type":"tags"},{"content":"","date":"25 July 2023","externalUrl":null,"permalink":"/tags/prompt-engineering/","section":"Tags","summary":"","title":"Prompt Engineering","type":"tags"},{"content":"","date":"25 July 2023","externalUrl":null,"permalink":"/tags/delegation/","section":"Tags","summary":"","title":"Delegation","type":"tags"},{"content":"Article about Effective Delegation\nIntro # Effective delegation involves assigning tasks and responsibilities to other people in order to achieve a goal. It is a key skill for any manager or leader, as it allows them to manage their workload more efficiently and effectively. Effective delegation requires the manager to clearly communicate the task, provide support and guidance, and monitor progress. It also requires the manager to trust the person they are delegating to and to provide them with the necessary resources and authority to complete the task.\n5 Steps for Effective Delegation # Specify Outcome - Be clear and specific with the desired result, begin with end in mind and describe goals not actions. Also ask how they have understood the desired result. Set a clear timeline and include deadlines. Guidelines - Establish controls and limits to the work, identify boundaries and constraints. Organizational or financial limitations. What responsibilities and authorities should the person take into account. Resources - What support can the person expect from you or other people. Provide sufficient support but resist upward delegation. Request for recommended solutions. Also share available resources such as courses, books, machines or software to accomplish the task. Evaluation / Monitoring - Agree on a schedule of checkpoints, keep up to date with progress. Focus on results not procedures Feedback / Recognition - When the work is done, give recognition where it is deserved. Very important point for future motivation and productive behavior There isn\u0026rsquo;t a one style fits all, and the it should be chose based on the competence level of the person for that specific task.\nDelegation Styles # Delegation styles vary depending on the manager\u0026rsquo;s approach and the situation.\nAutocratic # The autocratic style of delegation is characterized by the manager making all decisions and giving orders without any input from the team. This style is often used in emergency situations or when the manager has a great deal of experience and knowledge in the area.\nDirective # The directive style of delegation involves the manager providing clear instructions and guidance to the team. The manager is still in control but allows the team to use their own initiative and make decisions within the framework provided.\nConsultative # The consultative style of delegation involves the manager consulting with the team before making decisions. The team is given the opportunity to provide input and the manager takes their views into account when making decisions.\nParticipative # The participative style of delegation involves the manager and team working together to make decisions. The team is given the opportunity to provide input and the manager takes their views into account when making decisions.\nTips # Explain why you are delegating # How will it improve their skills How does the task fits in the organizations How the task fits in their growth What effect will the completion of the task have on the success of the organization Build Motivation and Commitment # Discuss how the success of the task will impact Financial Rewards Future Opportunities Informal Recognition Other desirable consequences Use the Tell-Show-Try-Do formula # Depending on the expertise level : TELL them how to do it SHOW them how to do it Let them TRY to do it Let them DO it by themselves Reference # These tips are available on Markus Amanto Leadership Training in Udemy, very insigthfull content recommended ","date":"25 July 2023","externalUrl":null,"permalink":"/2023/07/effective-delegation/","section":"Posts","summary":"","title":"Effective Delegation","type":"posts"},{"content":"","date":"27 January 2023","externalUrl":null,"permalink":"/tags/cheatsheet/","section":"Tags","summary":"","title":"CheatSheet","type":"tags"},{"content":"This article is about pacman command in Arch Linux\nIntro # I\u0026rsquo;ve recently started using a Arch Linux distribution Garuda Linux and the package management tool is pacman this article summarizes the most common actions with based on the following article from GeeksforGeeks.\nPacman is a package manager for the arch Linux and arch-based Linux distributions. If you have used Debian-based OS like ubuntu, then the Pacman is similar to the apt command of Debian-based operating systems. Pacman contains the compressed files as a package format and maintains a text-based package database. Pacman keeps the system up to date by synchronizing package lists with the master server. Pacman can install the packages from the official repositories or your own build packages.\nInstalling Packages using the Pacman # When we install any new operating system on our machine, the first task we do is to install the software packages on the system. Now, to install the packages on Arch Linux, use the command Pacman with -S option and mention the package name. The -S option tells the Pacman to synchronize and to continue. Here is one example\nsudo pacman -S cmatrix We can mention the many package names after the -S option, separated by space.\nsudo pacman -S package1 package2 package3 Then Pacman will show the download and install size of the package and ask for to proceed, then simply press the Y key. Pacman categorizes the installed packages into two categories.\nImplicitly Installed: The package that was installed using the -S or -U option. Dependencies: The package is installed because it is required by another package. Removing packages using the Pacman # When we don’t need the package anymore, then we should remove the package from the system. To remove the package with all its dependencies which are not required by other packages, use the following command:\nsudo pacman -Rs \u0026lt;package_name\u0026gt; To remove the package without removing its dependency use the following command:\nsudo pacman -R \u0026lt;package_name\u0026gt; To remove the dependencies which are not required anymore, use the following command:\npacman -Qdtq | pacman -Rs - Upgrading packages # In arch Linux, we can upgrade the full system by only one command using the Pacman. Use the following command to update the system:\nsudo pacman -Suy Let’s understand the meaning, S tell the pacman to synchronize the local database with the main database. u tells the pacman to upgrade the packages and y update the local catch on the system. Basically, this command synchronizes the local pacman database with the main repository database and then updates the system.\nSearching for a Package # Now let’s see how we can search the package into the database of pacman. To search query in the name and description of the package in the database use the following command:\nsudo pacman -Ss \u0026lt;query1\u0026gt; \u0026lt;query2\u0026gt; To search into already installed packages on the system, use the following command:\nsudo pacman -Qs query1\u0026gt; \u0026lt;query2\u0026gt; To search the query into the local database use the following command:\nsudo pacman -F query1\u0026gt; \u0026lt;query2\u0026gt; Cleaning the Package Cache # When pacman downloads the packages it stores the packages into the /var/cache/pacman/pkg/ and while uninstalling the package pacman does not delete these files. Pacman uses these files to downgrade the package or install the package. But it can take a lot of space to store these packages. So to delete the stored packages, use the following command:\nsudo pacman -Sc To remove all stored packages and cache, use the following command:\nsudp pacman -Scc Installing local packages # By using pacman we can install packages other than the main repository of Arch Linux. Use the following command to install the packages\nFor local:\nsudo pacman -U path_to_file.pkg.tar.xz For remote package:\nsudo pacman -U http://www.example.com/repo/example.pkg.tar.xz Troubleshooting # Sometimes installing the packages with pacman we face some errors. Following are the mainly occurred errors with pacman:\nConflicting file error # This error occurs due to some conflict packages present in the repository. To solve this error we can manually rename the file or force the overwrite function. We can use the following command to overwrite the function:\npacman -S --overwrite glob package\nInvalid package # This error can occur due to the partial installation of the packages. We can solve this error by deleting .part files in /var/cache/pacman/pkg/.\nLocking database # This error can occur when pacman is interrupted while updating the database. To solve this error, delete the file /var/lib/pacman/db.lck and update the database. Use the following command to delete the file:\nGaruda # Garuda also brings a useful command update that abstracts all this interactions to keep your rolling system updated, still very important to understand the command line utilities to manage the packages and check the official man page for further details as this is always evolving.\nReferences # https://www.geeksforgeeks.org/pacman-command-in-arch-linux/ https://archlinux.org/pacman/pacman.8.html ","date":"27 January 2023","externalUrl":null,"permalink":"/2023/01/pacman-command-in-arch-linux/","section":"Posts","summary":"","title":"pacman command in Arch Linux","type":"posts"},{"content":"In this article i will describe how to setup a docker environment to have a JFrog Artifactory locally\nIntro # In this article i will describe how to setup a docker environment to have a JFrog Artifactory local solution.\nObjective # The main objective of this walkthrough is to have a local repository to manage packages such as Nugets, jars, rpms, etc.\nCould be useful to test CI/CD pipelines also.\nVersions # Artifactory has several versions. It is recommend to use a Professional edition for productivity workloads. For this article we consider the OSS version\nRequirements # For this setup one should have:\ndocker docker-compose Installation # Obtain the docker-compose image\nhttps://releases.jfrog.io/artifactory/bintray-artifactory/org/artifactory/oss/docker/jfrog-artifactory-oss/7.12.10/jfrog-artifactory-oss-7.12.10-compose.tar.gz NOTE: Check the latest version on the official website.\nExtract the tarball\nRun the config.sh with privileges to setup your folder with the proper permissions.\nIn my case i also installed the PostgreSQL service.\nOne should have an output similar to this one\nBeginning JFrog Artifactory setup Validating System requirements Installation Directory (Default: /root/.jfrog/artifactory): /home/rramos/Development/github/dockers/docker-artifactory/data Running system diagnostics checks, logs can be found at [/home/rramos/Development/github/dockers/docker-artifactory/artifactory-oss-7.12.10/systemDiagnostics.log] Triggering migration script. This will migrate if needed and may take some time. Migration logs will be available at [/home/rramos/Development/github/dockers/docker-artifactory/artifactory-oss-7.12.10/bin/migration.log]. The file will be archived at [/home/rramos/Development/github/dockers/docker-artifactory/data/var/log] after installation For IPv6 address, enclose value within square brackets as follows : [\u0026lt;ipv6_address\u0026gt;] Please specify the IP address of this machine (Default: 127.0.1.1): Are you adding an additional node to an existing product cluster? [y/N]: N The installer can install a PostgreSQL database, or you can connect to an existing compatible PostgreSQL database (https://service.jfrog.org/installer/System+Requirements#SystemRequirements-RequirementsMatrix) If you are upgrading from an existing installation, select N if you have externalized PostgreSQL, select Y if not. Do you want to install PostgreSQL? [Y/n]: Y To setup PostgreSQL, please enter a password database password: confirm database password: Creating third party directories (if necessary) Attempting to seed PostgreSQL. This may take some time. Successfully seeded PostgreSQL Docker setup complete Installation directory: [/home/rramos/Development/github/dockers/docker-artifactory/data] contains data and configurations. Use docker-compose commands to start the application. Once the application has started, it can be accessed at [http://127.0.1.1:8082] Examples: cd /home/rramos/Development/github/dockers/docker-artifactory/artifactory-oss-7.12.10 start postgresql: docker-compose -p rt-postgres -f docker-compose-postgres.yaml up -d start: docker-compose -p rt up -d stop: docker-compose -p rt down NOTE: The compose file uses several environment variables from the .env file. Remember to run from within the [/home/rramos/Development/github/dockers/docker-artifactory/artifactory-oss-7.12.10] folder Done At the end of the process the postgres container is still up. I would suggest killing it as we are going to setup the docker-compose setup for it.\n# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 77bbf3150642 docker.bintray.io/postgres:12.5-alpine \u0026#34;docker-entrypoint.s…\u0026#34; About a minute ago Up About a minute 0.0.0.0:5432-\u0026gt;5432/tcp, :::5432-\u0026gt;5432/tcp postgresql Killing the containers\ndocker kill 77bbf3150642 docker container prune NOTE: Attention don\u0026rsquo;t run the prune command if you want to keep other containers running.\nPost Install # During the setup I\u0026rsquo;ve selected the following path\n/home/rramos/Development/github/dockers/docker-artifactory/data However there are some adjustments required for the services to reach the containers which will be on the same network and need to be adjusted.\nFollowing this setup you should now have on your current dir data and artifactory-oss-7.12.10 folder.\nCreate the following file template docker file that starts all the components\ncp artifactory-oss-7.12.10/templates/docker-compose-all.yaml docker-compose.yaml And replace the line\nPOSTGRES_PASSWORD=${PG_PASS} Let\u0026rsquo;s include that var on the env file.\ncp artifactory-oss-7.12.10/.env . Add the password you defined for postgres on .env file.\nPG_PASS=V3ry$$scurePasssW0rd Let\u0026rsquo;s spin up the application\ndocker-compose up -d If you start getting some issues regarding DB connection issues there is a final step.\nStop all the containers\ndocker-compose down Edit the file ./data/var/data/nginx/conf.d/artifactory.conf and replace localhost references with artifactory\nNOTE: You may need sudo permission to change those files\nAlso edit the file ./data/var/etc/system.yaml\nMaking sure the DB url is like:\nurl: jdbc:postgresql://postgres:5432/artifactory Now let\u0026rsquo;s restart the containers\ndocker-compose up -d And access on the following url: http://localhost:8081/artifactory/\nThe default user/password is admin/password\nNow it\u0026rsquo;s time to explore \u0026hellip;\nConclusion # There are several ways to setup Artictory depending on your environment. If you want to use for local developments it may not payoff due to the limitation of the oss version. One cannot manage Nuget packages for instance and require an pro version. As that was may principal objective i didn\u0026rsquo;t explore more the tool.\nAlso include the docker-compose setting on my dockers repo\nReferences # https://www.jfrog.com/confluence/display/JFROG/Installing+Artifactory https://www.jfrog.com/confluence/display/JFROG/NuGet+Repositories ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/artifactory-setup/","section":"Posts","summary":"","title":"Artifactory Setup","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/code/","section":"Tags","summary":"","title":"Code","type":"tags"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/laws/","section":"Tags","summary":"","title":"Laws","type":"tags"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/nuget/","section":"Tags","summary":"","title":"Nuget","type":"tags"},{"content":"In this article we are going to explore Software Engineering Laws.\nIntro # Latest week a college shared the following article at work that i found very interesting aggregating the info here in order not to forget.\nConway\u0026rsquo;s Law # Also known as: \u0026ldquo;You will ship your org chart.\u0026rdquo;\n\u0026ldquo;Any organization that designs a system will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\u0026rdquo;\nYou may think you can avoid it via cross-functional standup meetings and stakeholder updates and decision matrices, but eventually and inevitably conflicting or diverging priorities will lead to equally conflicting or divergent processes and outcomes.\nBrooks\u0026rsquo;s Law # From The Mythical Man-Month Book:\n\u0026ldquo;Adding manpower to a late software project makes it later.\u0026rdquo;\nWhen you realize you\u0026rsquo;re not making the progress you thought you would and your management tries to reallocate resources from another part of the org, you\u0026rsquo;ll end up not only delaying the project, but you\u0026rsquo;ll likely ship a more brittle, more complex product\nZawinski\u0026rsquo;s Law # \u0026ldquo;Every program attempts to expand until it includes a web server. Those programs which cannot so expand are replaced by ones which can.\u0026rdquo;\nFor web services, it\u0026rsquo;s \u0026ldquo;\u0026hellip;until it requires a user account and collects all users\u0026rsquo; data\u0026rdquo;; for physical devices, it\u0026rsquo;s \u0026ldquo;\u0026hellip;until it includes an insecure wifi access point with a default password you can\u0026rsquo;t change. And a web server\u0026rdquo;.\nParkinson\u0026rsquo;s Law # \u0026ldquo;Work expands so as to fill the time available for its completion.\u0026rdquo;\nThe primary project management lessons here is that if you don\u0026rsquo;t set a deadline, then the project will never be completed. This is why iterating on a minimum viable product within fixed timelines is important.\nAnd, of course, we can and should adjust this law for data, processing power, RAM, \u0026hellip;:\n\u0026ldquo;Data/CPU/memory usage expands to use up all available storage space/bandwidth/cycles/RAM.\u0026rdquo;\nPareto\u0026rsquo;s Fallacy # The Pareto Principle is easy to misinterpret, especially by management. This often leads to Pareto\u0026rsquo;s Fallacy:\n\u0026ldquo;When you\u0026rsquo;re 80% done, you think you only have 20% left.\u0026rdquo;\nThe critical part that\u0026rsquo;s overlooked here is that those 20% will require 80% of your time.\nSturgeon\u0026rsquo;s Revelation # \u0026ldquo;90% of everything is crud.\u0026rdquo;\nYes, that includes your products.\nThe Peter Principle # \u0026ldquo;In a hierarchy, every employee tends to rise to their level of incompetence. Thus, in time, every post tends to be occupied by an employee who is incompetent to carry out its duties.\u0026rdquo;\nDunning and Kruger send their regards\u0026hellip;(ref)\nEagleson\u0026rsquo;s Law # \u0026ldquo;Any code of your own that you haven\u0026rsquo;t looked at for six or more months might as well have been written by someone else.\u0026rdquo;\n6 months is rather optimistic.\nGreenspun\u0026rsquo;s 10th Rule of Programming # This can be generalized to the Universal NIH-Rule: \u0026ldquo;any custom developed system contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of the industry standard you refused to adopt\u0026rdquo;.\nThe Iceberg Fallacy # \u0026ldquo;The cost of development of a new software product is the only ~25% of the total cost of ownership management sees and budgets for.\u0026rdquo;\nThe LGTM Dilemma # \u0026ldquo;If you want to quickly ship a 10 line code change, hide it in a 1500 line pull request.\u0026rdquo;\nAlso known as The Bikeshedders\u0026rsquo; Blind Spot.\n","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/software-engineering-laws/","section":"Posts","summary":"","title":"Software Engineering Laws","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/agile/","section":"Tags","summary":"","title":"Agile","type":"tags"},{"content":"Agile is a highly collaborative approach suited for very complex and competitive projects. In this article, we’ll briefly explore the four values and 12 principles of the Agile Manifesto.\nIntro # Agile is a highly collaborative approach suited for very complex and competitive projects. In this article, we’ll briefly explore the four values and 12 principles of the Agile Manifesto.\nThe history of Agile # Agile as a project management approach was introduced to the world in 2001 in the United States. At a ski resort in the Wasatch mountains of Utah, 17 self-proclaimed organizational anarchists came together and combined several lightweight processes to create what we know today as the Agile Manifesto. The creators of Agile intended it to be a set of values and principles that would improve upon and transform existing software development processes, but companies in various industries quickly saw the value of Agile, too. Soon, Agile was adopted across all fields.\nAgile values and principles # In the last video, you explored the Agile Manifesto—the guiding force behind all Agile teams—in-depth. You learned that Agile is a highly collaborative approach suited for very complex and competitive projects. In this reading, we’ll briefly explore the four values and 12 principles of the Agile Manifesto.\nThe Agile values refer to the following four statements:\nIndividuals and interactions over processes and tools\nWorking software over comprehensive documentation\nCustomer collaboration over contract negotiation\nResponding to change over following a plan\nAgile experts see these values as important underpinnings of the highest performing teams, and every team member should strive to live by these values to apply the full benefits of Agile.\nThe same is true for the 12 principles, which are at the core of every Agile project:\nOur highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nWhether you are working to create a product for your company or for a customer, chances are that someone is awaiting its delivery. If that delivery is delayed, the result is that the customer, user, or organization is left waiting for that added value to their lives and workflows. Agile emphasizes that delivering value to users early and often creates a steady value stream, increasing you and your customer’s success. This will build trust and confidence through continuous feedback as well as early business value realization.\nWelcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage.\nWhen working in Agile, it’s important to be agile. That means being able to move swiftly, shifting direction whenever necessary. That also means that you and your team are constantly scanning your environment to make sure necessary changes are factored into the plans. Acknowledging and embracing that your plans may change (once, twice, or several times) ensures that you and your customers are maximizing your success.\nDeliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.\nDelivering your product in small, frequent increments is important because it allows time and regular opportunities for stakeholders—including customers—to give feedback on its progress. This ensures that the team never spends too much time going down the wrong path.\nBusiness people and developers must work together daily throughout the project.\nRemoving barriers between developers and people focused on the business side of the project, builds trust and understanding and ensures that the developers, or those building the solution, are in tune with the needs of the users.\nBuild projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.\nA successful Agile team includes team members that not only trust each other to get the work done but are also trusted by their sponsors and executives to get the work done. Teams build better solutions when they are empowered and motivated to deliver difficult projects.\nThe most efficient and effective method of information to and within a development is face-to-face conversation.\nThere isn’t anything quite like face-to-face communication. Face-to-face communication allows us to catch certain cues, body language, and facial expressions that are sometimes lost when using forms of communication like email, chat, or the phone. However, we can’t always be face-to-face. Establishing effective communication norms—no matter the format—is essential to effective teams.\nWorking software is the primary measure of progress.\nIn Agile teams, the main way to demonstrate meaningful completion of work is to show a working piece of the solution. In software teams, that might mean a functional piece of software. In non-software teams, that might mean a critical portion of the solution that is ready to be demonstrated to users or their representatives in order to collect feedback. This is in contrast to traditional or Waterfall projects, where the completion of project documents could be used to measure progress. In Agile project management, it is not enough to say that the team is 80% done with an activity if there is no working, demonstrable artifact available to review.\nAgile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.\nMaintaining a steady but careful pace will prevent errors along the way. Also, you never want your team to feel overworked or overwhelmed. On the flip side, a team that is underutilized may become bored and lose the creative spark to innovate. The Agile ideal is to achieve a steady pace of effort for the team that avoids overtime and burnout.\nContinuous attention to technical excellence and good design enhances agility.\nThis principle conveys that just because the team is working fast doesn’t mean they sacrifice quality. By emphasizing quality and design throughout the project development phase, the agility, efficiency, and speed of the team will be increased. When a team delivers a well-built solution, they can quickly respond to user feedback and new information. However, if the product is low quality, implementing changes can become problematic, complex, and slow down the entire team.\nSimplicity—the art of maximizing the amount of work not done—is essential.\nThe team should avoid implementing extra features into the solution that weren’t explicitly requested by the user or product owner. This includes removing procedures that are no longer necessary, and reducing unnecessary documentation.\nThe best architectures, requirements, and designs emerge from self-organizing teams.\nTeam members should be able to get their work done by designing their own work processes and practices, without a manager dictating how they operate. Team members should also feel empowered to speak up with questions, concerns, or feedback.\nAt regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.\nIn Agile, it is important to acknowledge that learning from successes and failures is continuous. No team is perfect. There will be mistakes, challenges, trials, and triumphs. Teams should reflect on all of these different aspects of their activities so that they can make necessary adjustments.\nReferences # https://www.coursera.org/learn/agile-project-management http://agilemanifesto.org/principles.html ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/agile-principals/","section":"Posts","summary":"","title":"Agile Principals","type":"posts"},{"content":"This article is about OpenAPI development in Linux specifically .netcore\nIntro # It as been a while since i\u0026rsquo;ve engaged with C#. In this article I\u0026rsquo;ll describe a step-by-step on how to setup your Arch Linux to implement a .netcore application implementation of OpenAPI.\nInstallation # Let\u0026rsquo;s start by making sure we have the SDK and runtime packages available on the system.\nsudo pacman -S dotnet-runtime dotnet-sdk We can validate the existing environments with the command\ndotnet --list-sdks At the time of this writing\n6.0.100 [/usr/share/dotnet/sdk] Microsoft recommends using Visual Studio Code i will skip that part but here is the url in case of need.\nHello World # Now let\u0026rsquo;s do a quick 5m Hello world just to make sure everything is working properly.\ndotnet new console -o MyApp One should have a similar output\nThe template \u0026#34;Console App\u0026#34; was created successfully. Processing post-creation actions... Running \u0026#39;dotnet restore\u0026#39; on /home/rramos/Development/hello/MyApp/MyApp.csproj... Determining projects to restore... Restored /home/rramos/Development/hello/MyApp/MyApp.csproj (in 2.82 sec). Restore succeeded. Now let\u0026rsquo;s run this amazing helloworld\ncd MyApp dotnet run One should get the desirable outcome\nHello, World! Congratulations, you\u0026rsquo;ve built and run your .NET app. Let\u0026rsquo;s clean the space now\ncd .. rm -rf MyApp OpenAPI # Now let\u0026rsquo;s create our webAPI\ndotnet new webapi -o WebApi1 Next lets add the Swashbuckle.AspNetCore package\ndotnet add package Swashbuckle.AspNetCore At this stage if you try to build it works but fails on the execution with the following error:\nThat is because Arch linux ships ASPNet in a different package. To Solve one should install the extra package using pacman.\nsudo pacman -Sy aspnet-runtime After the installation if we check the exiting runtimes\ndotnet --list-runtimes One should have the following\nMicrosoft.AspNetCore.App 6.0.0 [/usr/share/dotnet/shared/Microsoft.AspNetCore.App] Microsoft.NETCore.App 6.0.0 [/usr/share/dotnet/shared/Microsoft.NETCore.App] Lets try to run it now\ndotnet run You should have a similar output\nBuilding... info: Microsoft.Hosting.Lifetime[14] Now listening on: https://localhost:7177 info: Microsoft.Hosting.Lifetime[14] Now listening on: http://localhost:5169 info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Development info: Microsoft.Hosting.Lifetime[0] Content root path: /home/rramos/Development/WebApi1/ Now that looks promising let\u0026rsquo;s test the the endpoint by using the browser on\nhttps://localhost:7177/swagger/index.html One should see a WeatherForecast service which is the baseline service when created the project.\nLeave this running for now and open a new terminal to create a client.\nClient # Now that we have our service running let\u0026rsquo;c create a client by retrieving the OpenAPI description.\nFor that we will be using the NSwag.ApiDescription.Client package.\nLet\u0026rsquo;s start by creating a new client project and retrieving the OpenAPI description file from swagger.\ncd .. dotnet new console -o console cd console mkdir openapi wget --no-check-certificate https://localhost:7177/swagger/v1/swagger.json -O openapi/weather.json Now lets add the required package\ndotnet add package NSwag.ApiDescription.Client --version 13.15.5 If one checks the console.csproj file we can see the package was added. Although i have added a specific version is not relevant.\nWe are also going to need the following package\ndotnet add package Newtonsoft.Json Add the following block on the console.csproj file\n\u0026lt;ItemGroup\u0026gt; \u0026lt;OpenApiReference Include=\u0026#34;openapi/weather.json\u0026#34; Namespace=\u0026#34;WeatherService\u0026#34;\u0026gt; \u0026lt;ClassName\u0026gt;WeatherClient\u0026lt;/ClassName\u0026gt; \u0026lt;OutputPath\u0026gt;WeatherClient.cs\u0026lt;/OutputPath\u0026gt; \u0026lt;/OpenApiReference\u0026gt; \u0026lt;/ItemGroup\u0026gt; Now execute the build command to generate the WeatherService classes\ndotnet build You should have a similar output\nMicrosoft (R) Build Engine version 17.0.0+c9eb9dd64 for .NET Copyright (C) Microsoft Corporation. All rights reserved. Determining projects to restore... All projects are up-to-date for restore. GenerateNSwagCSharp: dotnet --roll-forward-on-no-candidate-fx 2 \u0026#34;/home/rramos/.nuget/packages/nswag.msbuild/13.14.5/build/../tools/Net60//dotnet-nswag.dll\u0026#34; openapi2csclient /className:WeatherClient /namespace:WeatherService /input:\u0026#34;/home/rramos/Development/console/openapi/weather.json\u0026#34; /output:\u0026#34;obj/WeatherClient.cs\u0026#34; NSwag command line tool for .NET Core Net60, toolchain v13.14.5.0 (NJsonSchema v10.5.2.0 (Newtonsoft.Json v13.0.0.0)) Visit http://NSwag.org for more information. NSwag bin directory: /home/rramos/.nuget/packages/nswag.msbuild/13.14.5/tools/Net60 Code has been successfully written to file. Duration: 00:00:00.6921924 console -\u0026gt; /home/rramos/Development/console/bin/Debug/net6.0/console.dll Build succeeded. 0 Warning(s) 0 Error(s) Time Elapsed 00:00:04.17 And a brand new WeatherClient Class is now available to use on your main Program so let\u0026rsquo;s use the following on Program.cs\n// Configure HttpClientHandler to ignore certificate validation errors. using var httpClientHandler = new HttpClientHandler(); httpClientHandler.ServerCertificateCustomValidationCallback = (message, cert, chain, errors) =\u0026gt; { return true; }; // Create WeatherClient. using var httpClient = new HttpClient(httpClientHandler); var weatherClient = new WeatherService.WeatherClient(\u0026#34;https://localhost:7177\u0026#34;, httpClient); // Call WeatherForecast API. var forecast = await weatherClient.GetWeatherForecastAsync(); foreach (var item in forecast) { Console.WriteLine($\u0026#34;{item.Date} - {item.Summary}\u0026#34;); } If one runs the client now with dotnet run the following output is expected.\n12/20/2021 10:34:23 PM +00:00 - Freezing 12/21/2021 10:34:23 PM +00:00 - Cool 12/22/2021 10:34:23 PM +00:00 - Hot 12/23/2021 10:34:23 PM +00:00 - Scorching 12/24/2021 10:34:23 PM +00:00 - Scorching Hurray our client receives the API data and both client and server seems to be working Archnt to run the service and setup a CI/CD process for it. Also to extend the service or create something more appealing.\nReferences # https://wiki.archlinux.org/title/.NET https://docs.microsoft.com/en-us/aspnet/core/tutorials/web-api-help-pages-using-swagger https://wiki.archlinux.org/title/Visual_Studio_Code https://dotnet.microsoft.com/en-us/learn/dotnet/hello-world-tutorial/create https://github.com/domaindrivendev/Swashbuckle.AspNetCore https://github.com/RicoSuter/NSwag ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/openapi-development-in-arch-linux/","section":"Posts","summary":"","title":"OpenAPI Development in Arch Linux","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/data-quality/","section":"Tags","summary":"","title":"Data Quality","type":"tags"},{"content":"This article is about SODA a tool for Data Quality\nIntro # Bernard Denys kindly share with me the availability of their product for the Open Source Community in Github. SODA provide a Data Monitoring Platform. In this article i will take some time to explore a quick setup on how to use it and final comments around it.\nWhat does Soda SQL do # Soda SQL allows you to\nStop your pipeline when bad data is detected Extract metrics and column profiles through super efficient SQL Full control over metrics and queries through declarative config files\n5m tutorial # There is a 5m tutorial available on their official page let\u0026rsquo;s have a go\u0026hellip;\nInstall # I\u0026rsquo;m using Linux and i\u0026rsquo;m lazy so i\u0026rsquo;ve just installed the required pip\npip3 install soda-sql Let\u0026rsquo;s check if it is working\n$ soda Usage: soda [OPTIONS] COMMAND [ARGS]... Soda CLI version 2.0.0b15 Options: --help Show this message and exit. Commands: analyze Analyzes tables in the warehouse and creates scan YAML files... create Creates a new warehouse.yml file and prepares credentials in your... init Renamed to `soda analyze` scan Computes all measurements and runs all tests on one table. Great now let\u0026rsquo;s create some a dummy warehouse and do some tests:\nCreate dummy Datawarehouse # docker run --name soda_sql_tutorial_db --rm -d \\ -p 5432:5432 \\ -v soda_sql_tutorial_postgres:/var/lib/postgresql/data:rw \\ -e POSTGRES_USER=sodasql \\ -e POSTGRES_DB=sodasql \\ -e POSTGRES_HOST_AUTH_METHOD=trust \\ postgres:9.6.17-alpine And load it with data with the following command\ndocker exec soda_sql_tutorial_db \\ sh -c \u0026#34;wget -qO - https://raw.githubusercontent.com/sodadata/soda-sql/main/tests/demo/demodata.sql | psql -U sodasql -d sodasql\u0026#34; According to the tutorial one can remove the created container and volume with the following command :\ndocker stop soda_sql_tutorial_db docker volume rm soda_sql_tutorial_postgres Create warehouse directory # mkdir soda_sql_tutorial cd soda_sql_tutorial/ soda create -d sodasql -u sodasql -w soda_sql_tutorial postgres This created the warehouse.yml with connection settings which are stored on your homedir ~/.soda/env_vars.yml\nAnalyse table scan YAML files # The following command will analyze the exiting tables and fill the ./tables/ directory with large data warehouse it can be inputted manually.\nWell, i only have 5m so let\u0026rsquo;s give it a go.\nsoda analyze Hum! interesting, queries the information schema and generates a file which i will assume per table with several metrics for validation.\ntable_name: demodata metrics: - row_count - missing_count - missing_percentage - values_count - values_percentage - valid_count - valid_percentage - invalid_count - invalid_percentage - min_length - max_length - avg_length - min - max - avg - sum - variance - stddev tests: - row_count \u0026gt; 0 columns: id: valid_format: uuid tests: - invalid_percentage == 0 feepct: valid_format: number_percentage tests: - invalid_percentage == 0 Run a scan # Each scan requires a warehouse YAML and a scan YAML as input. The scan command will collect the configured metrics and run the defined tests against them.\nsoda scan warehouse.yml tables/demodata.yml And that\u0026rsquo;s it for the tutorial\nOne can then integrate this with a orchestration tool such as Airflow\nIt is recommend to PythonVirtualenvOperator to prevent any dependency conflicts.\nExample dag:\nfrom airflow import DAG from airflow.models.variable import Variable from airflow.operators.python import PythonVirtualenvOperator from airflow.operators.dummy import DummyOperator from airflow.utils.dates import days_ago from datetime import timedelta # Make sure that this variable is set in your Airflow warehouse_yml = Variable.get(\u0026#39;soda_sql_warehouse_yml_path\u0026#39;) scan_yml = Variable.get(\u0026#39;soda_sql_scan_yml_path\u0026#39;) default_args = { \u0026#39;owner\u0026#39;: \u0026#39;soda_sql\u0026#39;, \u0026#39;retries\u0026#39;: 1, \u0026#39;retry_delay\u0026#39;: timedelta(minutes=5), } def run_soda_scan(warehouse_yml_file, scan_yml_file): from sodasql.scan.scan_builder import ScanBuilder scan_builder = ScanBuilder() # Optionally you can directly build the Warehouse dict from Airflow secrets/variables # and set scan_builder.warehouse_dict with values. scan_builder.warehouse_yml_file = warehouse_yml_file scan_builder.scan_yml_file = scan_yml_file scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_failures(): failures = scan_result.failures_count() raise ValueError(f\u0026#34;Soda Scan found {failures} errors in your data!\u0026#34;) dag = DAG( \u0026#39;soda_sql_python_venv_op\u0026#39;, default_args=default_args, description=\u0026#39;A simple Soda SQL scan DAG\u0026#39;, schedule_interval=timedelta(days=1), start_date=days_ago(1), ) ingest_data_op = DummyOperator( task_id=\u0026#39;ingest_data\u0026#39; ) soda_sql_scan_op = PythonVirtualenvOperator( task_id=\u0026#39;soda_sql_scan_demodata\u0026#39;, python_callable=run_soda_scan, requirements=[\u0026#34;soda-sql==2.0.0b10\u0026#34;], system_site_packages=False, op_kwargs={\u0026#39;warehouse_yml_file\u0026#39;: warehouse_yml, \u0026#39;scan_yml_file\u0026#39;: scan_yml}, dag=dag ) publish_data_op = DummyOperator( task_id=\u0026#39;publish_data\u0026#39; ) ingest_data_op \u0026gt;\u0026gt; soda_sql_scan_op \u0026gt;\u0026gt; publish_data_op There is also some work being done to support a SodaScanOperator for Airflow.\nWarehouse types # At the moment SODA is supporting the following techs:\nSnowflake AWS Athena GCP BigQuery PostgreSQL Redshift Spark SQL -\u0026gt; (Coming Soon) Conclusion # This quick tutorial make\u0026rsquo;s it easy to test the tool. But i would like to extended the tests on a larger solution to check how it behaves. Also i would like to see the SparkSQL as a warehouse option as most of my teams work are centered on that technology.\nTopics to do extended testing with:\nSchema evolution impact on the quality test setup Not using a Warehouse solution but a Datalake Providing schemas from a Datalake and using a processing engine on top of it (ex: spark.schemas and Parquet files) Testing with Delta tables Define levels of data-quality analysis Understanding better how the scans works The tool seems to have good potential and seems very simple which would speed the adoption. Also by sharing this to the open source community seems to be as a good choice to increase user quorum and still offer enterprise services solution.\nReferences # https://github.com/sodadata/soda-sql#readme https://docs.soda.io/soda-sql/getting-started/5_min_tutorial.html https://docs.soda.io/soda-sql/documentation/orchestrate_scans.html ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/soda/","section":"Posts","summary":"","title":"SODA","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/bash/","section":"Tags","summary":"","title":"Bash","type":"tags"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/customization/","section":"Tags","summary":"","title":"Customization","type":"tags"},{"content":"This article is about fish a user-friendly command line shell for Linux\nAbout fish # fish is a smart and user-friendly command line shell for Linux, macOS, and the rest of the family.\nConfigurations # To disable the greeting in fish edit the file .config/fish/config.fish and put an empty greeting.\nset fish_greeting \u0026#34;\u0026#34; Now re-open the terminal.\nRemove neofetch # If you don\u0026rsquo;t want the fancy computer stats everything you open a terminal comment out the last 4 lines on the conf file ./config/fish/config.fish\n## Run paleofetch if session is interactive #if status --is-interactive # neofetch #end Now re-open the terminal.\nOh My Fish # Install the package oh-my-fish which would allow to customize you fish configurations running the following command\ncurl https://raw.githubusercontent.com/oh-my-fish/oh-my-fish/master/bin/install | fish\nAfter this install the theme agnoster\nomf install agnoster\nApply and reload\nomf theme agnoster omf reload Disable starship # Comment the following block on the file ./config/fish/config.fish\n## Starship prompt #if status --is-interactive # source (\u0026#34;/usr/bin/starship\u0026#34; init fish --print-full-init | psub) #end Setup the Nord colors in Alacritty # Change the default color scheme for Nord colors\nreplacing on the following file .config/alacritty/alacritty.yml for the coloscheme available on the following repo.\ncolors: primary: background: \u0026#39;#2e3440\u0026#39; foreground: \u0026#39;#d8dee9\u0026#39; dim_foreground: \u0026#39;#a5abb6\u0026#39; cursor: text: \u0026#39;#2e3440\u0026#39; cursor: \u0026#39;#d8dee9\u0026#39; vi_mode_cursor: text: \u0026#39;#2e3440\u0026#39; cursor: \u0026#39;#d8dee9\u0026#39; selection: text: CellForeground background: \u0026#39;#4c566a\u0026#39; search: matches: foreground: CellBackground background: \u0026#39;#88c0d0\u0026#39; bar: background: \u0026#39;#434c5e\u0026#39; foreground: \u0026#39;#d8dee9\u0026#39; normal: black: \u0026#39;#3b4252\u0026#39; red: \u0026#39;#bf616a\u0026#39; green: \u0026#39;#a3be8c\u0026#39; yellow: \u0026#39;#ebcb8b\u0026#39; blue: \u0026#39;#81a1c1\u0026#39; magenta: \u0026#39;#b48ead\u0026#39; cyan: \u0026#39;#88c0d0\u0026#39; white: \u0026#39;#e5e9f0\u0026#39; bright: black: \u0026#39;#4c566a\u0026#39; red: \u0026#39;#bf616a\u0026#39; green: \u0026#39;#a3be8c\u0026#39; yellow: \u0026#39;#ebcb8b\u0026#39; blue: \u0026#39;#81a1c1\u0026#39; magenta: \u0026#39;#b48ead\u0026#39; cyan: \u0026#39;#8fbcbb\u0026#39; white: \u0026#39;#eceff4\u0026#39; dim: black: \u0026#39;#373e4d\u0026#39; red: \u0026#39;#94545d\u0026#39; green: \u0026#39;#809575\u0026#39; yellow: \u0026#39;#b29e75\u0026#39; blue: \u0026#39;#68809a\u0026#39; magenta: \u0026#39;#8c738c\u0026#39; cyan: \u0026#39;#6d96a5\u0026#39; white: \u0026#39;#aeb3bb\u0026#39; Themes # If you don\u0026rsquo; t like the color scheme check other options using the command.\nfish_config\ndonet autocomplete # In case you want dotnet command autocomplete to get something like this.\n╭─rramos@buldozer in ~ via  v17.2.0 ╰─λ dotnet add (Add a package/reference) publish (Publish a .NET project for deployment) build (Build a .NET project) remove (Remove a package/reference) build-server (Interact with build servers) restore (Restore dependencies) clean (Clean build outputs) run (Run the application from source) …and 6 more rows You should have the following on your config.fish file.\ncomplete -f -c dotnet -a \u0026#34;(dotnet complete)\u0026#34; For Garuda this setup seems to be present out-of-the box.\nConclusion # Although Garuda brings a lot of customizations for fish if one wants to go deeper and adjust for it\u0026rsquo;s own taste can take some time. I certainly would take some time to understand where configurations where being override. I probably include some of this configurations in git to prevent future time waist on customizations.\nReferences # https://wiki.archlinux.org/title/fish https://docs.microsoft.com/en-us/dotnet/core/tools/enable-tab-autocomplete#zsh https://github.com/oh-my-fish/oh-my-fish https://techviewleo.com/configure-fish-shell-with-oh-my-fish/ https://github.com/arcticicestudio/nord-alacritty https://fishshell.com/ ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/fish-configs/","section":"Posts","summary":"","title":"fish configs","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/guideline/","section":"Tags","summary":"","title":"Guideline","type":"tags"},{"content":"Article with guidelines for naming variables\nIntro # “There are only two hard things in computer science: cache invalidation and naming things.” — Phil Karlson\nFollow S-I-D # A name must be Short, Intuitive and Descriptive.\n/* Bad */ const a = 5 // \u0026#34;a\u0026#34; could mean anything const isPaginatable = (postsCount \u0026gt; 10) // \u0026#34;Paginatable\u0026#34; sounds extremely unnatural const shouldPaginatize = (postsCount \u0026gt; 10) // Made up verbs are so much fun! Suggested:\n/* Good */ const postsCount = 5 const hasPagination = (postsCount \u0026gt; 10) const shouldDisplayPagination = (postsCount \u0026gt; 10) // alternatively Avoid Contraction # Do not use contractions. They contribute to nothing but decreased readability of the code. Finding a short, descriptive name may be hard, but contraction is not an excuse for not doing so. For instance\n/* Bad */ const onItmClk = () =\u0026gt; {} /* Good */ const onItemClick = () =\u0026gt; {} Avoid Context Duplication # Always remove the context from a name if that doesn’t decrease its readability.\nclass MenuItem { /* Method name duplicates the context (which is \u0026#34;MenuItem\u0026#34;) */ handleMenuItemClick = (event) =\u0026gt; { ... } /* Reads nicely as `MenuItem.handleClick()` */ handleClick = (event) =\u0026gt; { ... } } Should Reflect expected result # /* Bad */ const isEnabled = (itemsCount \u0026gt; 3) return \u0026lt;Button disabled={!isEnabled} /\u0026gt; /* Good */ const isDisabled = (itemsCount \u0026lt;= 3) return \u0026lt;Button disabled={isDisabled} /\u0026gt; Consider Singular/Plurals # Like a prefix, variable names can be made singular or plural depending on whether they hold a single value or multiple values.\n/* Bad */ const friends = \u0026#39;Bob\u0026#39;; const friend = [\u0026#39;Bob\u0026#39;, \u0026#39;Tony\u0026#39;, \u0026#39;Tanya\u0026#39;]; /* Good */ const friend = \u0026#39;Bob\u0026#39;; const friends = [\u0026#39;Bob\u0026#39;, \u0026#39;Tony\u0026#39;, \u0026#39;Tanya\u0026#39;]; Use meaningful and pronounceable variable names # const yyyymmdstr = moment().format(\u0026#34;YYYY/MM/DD\u0026#34;); // simply awful // Instead const currentDate = moment().format(\u0026#34;YYYY/MM/DD\u0026#34;); The original article is more complete and has other suggestion regarding naming good practices would suggest you check it.\nReferences # https://betterprogramming.pub/a-useful-framework-for-naming-your-classes-functions-and-variables-e7d186e3189f ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/guideline-on-naming-variables/","section":"Posts","summary":"","title":"Guideline on Naming Variables","type":"posts"},{"content":"Article about keyboard layouts configuration in Linux\nIntro # Reminder article for as I\u0026rsquo;m constantly forgetting the keyboard layout in my Linux distribution. This started mostly after I have decided to use a 65% keyboard which misses a lot of keys from a traditional 100%.\nStill it has other advantages less space used and linear switches which allow faster input. Being a US traditional Layout there are some portuguese characters missing and having a 65% keyboard doesn\u0026rsquo;t help either. Nevertheless after some weeks of adjustment improvements are noticed.\nThe best layout to choose in Linux would be English (intl, with AltGr dead keys) for this setup.\nI still think that the english layout is the best option to follow. In regards to size a 85% seems a better options specially if you need to deal with spreadsheets.\nReferences # https://en.wikipedia.org/wiki/Dead_key ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/keyboard-layout/","section":"Posts","summary":"","title":"Keyboard Layout","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/keyboards/","section":"Tags","summary":"","title":"Keyboards","type":"tags"},{"content":"This article is about Github ssh keys\nIntro # When using github one of the most recommend ways to get in sync with your repo is by using ssh keys. If you don\u0026rsquo;t use it check the following article from github docs where they explain in detail how to do the setup\nhttps://docs.github.com/en/github/authenticating-to-github/about-ssh On your Profile you can manage your SSH and GPG keys.\nBut take into account the following if you have more than one account the ssh-keys are unique on the system.\nSo if you have for example:\ncorporate account ruiramosff personal account rramos You need to have distinct ssh-keys per profile.\nI would recommend for this cases to have distinct ssh keys and to a proper setup on your repo configurations adding for example\n[user] name = Rui Ramos email = rui.ms.ramos@gmail.com You can also do this globally on your ~./gitconfig but if you are going to work on the same machine but want to use different profiles, pay attention to have the git configuration adapted to it.\nCheers, RR\n","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/github-ssh-keys/","section":"Posts","summary":"","title":"Github ssh keys","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/ssh/","section":"Tags","summary":"","title":"SSH","type":"tags"},{"content":"Article about API versioning\nIntro # A very interesting talk regarding API Lifecycle where Thomas Betts talks with Kin Lane about managing your API lifecycle using standards and specifications, including OpenAPI, AsyncAPI, and JSON Schema. These specifications and the tooling based on them can help reduce communication problems, by creating documentation, generating code, and automating testing.\nInfoQ Article Communication Problems # Almost every problem in software comes down to a communication problem, whether that\u0026rsquo;s the ability to send, receive and understand a message. It can break down whether it\u0026rsquo;s client to server, or product manager to developer trying to understand requirements. Anytime you have two things trying to communicate, explaining the contract is really important. breaking changes aren\u0026rsquo;t bad or good. They are only bad or good if the communication is lacking or the communication is there.\nVersioning API # he most common way for folks to handle change management using versioning like this is with semantic versioning, where you have a 1.0.0 and then that\u0026rsquo;s a major release. And then when you release any changes that are minor changes, feature enhancements, you go 1.1. If you do a patch, if you\u0026rsquo;re needing to fix something it\u0026rsquo;d be 1.1.1. So that last digit is just for a fix a patch, something you\u0026rsquo;re making better. That middle digit is for incremental changes. But the first one, so if we went to version 2.0, that\u0026rsquo;s a major change.\nFull Chapter list from the talk:\nAPI Lifecycle vs. Software Development Lifecycle Swagger and OpenAPI More Than Just Documentation Improving Communication AsyncAPI is OpenAPI For Event-Driven APIs Better Naming Suggestions Relationship Between OpenAPI, AsyncAPI, and JSON Schema Maturity Level of AsyncAPI and OpenAPI API Specifications Are Not a New Idea Versioning an API Validation and Testing API Specifications For Non-Technical Stakeholders What’s Coming For API Lifecycles Closing ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/api-lifecycles/","section":"Posts","summary":"","title":"API Lifecycles","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/contracts/","section":"Tags","summary":"","title":"Contracts","type":"tags"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/crypto/","section":"Tags","summary":"","title":"Crypto","type":"tags"},{"content":"Article about Crypto Art\nIntro # Bumped with this one the last day Crypto Art the following transcript from Mankind explain a bit what\u0026rsquo;s under the movement.\nTranscript # Just like all cats are mammals, but not all mammals are cats. All Crypto Art are NFTs but not all NFTs are Crypto Art. I\u0026rsquo;ve made a video on basics of NFTs or non fungible tokens, you can go check that out. But today I\u0026rsquo;m here to explain the basics of what Crypto Art is, because Crypto Art is a digital artwork that is linked to a non fungible token.\nTraditionally Crypto Art was related to the rise of crypto currencies like bitcoin and ethereum and had symbolism around that, but it\u0026rsquo;s now more loosely accepted as any digital art that has been tokenized on the the blockchain in order to associate digital proof of ownership.\nSo what does that mean? It means what we\u0026rsquo;ve taken for granted in the real world, that a physical painting can be authenticated and owned now has the equivalent in the digital world. This is huge because this has never been possible for digital artists in the past. A token acting as a digital certificate for a digital file securely held on a permanent network of computers now allows digital artists to sell their work like never before.\nCrypto art is not just the digital file. It is the digital file backed by the NFT. So just like you can go online and find M. C. Escher \u0026lsquo;drawing hands\u0026rsquo; and download it and print it, you would never try to convince anyone it\u0026rsquo;s the original, because you don\u0026rsquo;t own the letter of authenticity. The same now goes for digital art, everyone can see or download it, but only one can prove they own it.\nThe question of the value of owning a digital artwork now lies with the market. The market of collectors. Beeple sold this work for $1 and a couple of months later sold this one for $777k. Paris Hilton drew this picture of her cat and it sold for $17k. Justin Roiland the creator of Rick and Morty drew this sketch and it sold for $90k. Of course if you already have a fanbase, anything you sell will be worth more. But even small artists can find collectors that just like the art or want to support their work.\nPop art used to be disregarded as cheap and distasteful, Street art used to just be vandalism, and until recently, digital art was just for Instagram likes.\nWhen you mint an NFT for Crypto Art you\u0026rsquo;re not giving away any copyright or rights of reproduction. You are creating a new thing, a new product. That buyer can then enjoy it, and sell it whenever they please.\nSo where can you sell? There\u0026rsquo;s an ever changing list of places to buy and sell on the primary or secondary market so I\u0026rsquo;ll keep a list below in the description. I\u0026rsquo;ve also made a video on popular ones in the past.\nMost Crypto Art sites currently use Ethereum to do their transactions so you need to be set up with an Ethereum wallet to connect and transfer to. You also need a little Ethereum to start with in your wallet to pay for minting or transaction fees. You can always transfer your Ethereum back out to your exchange of choice to cash out to FIAT or Dollars whenever you want.\nIn the future Crypto Art will just be called art. But at the moment, digital art is going through a movement led by changes in technology. The struggle of the artist will still remain, to create something with meaning and value, to find an audience or collectors. But the success of crypto currency in general has brought a wave of investors with big wallets to make a splash and bring eyeballs to the space with large transactions. The art world is a way to legitimize the space with creative thinkers, emotional ideas, entertainment and connection to culture that is worth investing in.\nEarly creators and collectors alike will be rewarded for their involvement in this movement.\nWhat is NFT # NFT stands for Non Fungible Token.\nFungible means something is able to be exchanged or substituted and will hold the same value. It\u0026rsquo;s interchangeable like the dollar, gold, casino chips, bitcoin, ethereum or frequent flyer loyalty points.\nIf I lend you 10 dollars cash and you return to me two 5 dollar bank notes, I\u0026rsquo;m fine with that because even though they are different, they hold the same total value.\nSo non-fungile obviously means it\u0026rsquo;s an asset that can\u0026rsquo;t be substituted. It has unique attributes that makes it different from something else in the same asset class. Like a painting, a theatre ticket, a house, a video game skin, a trademark or a CryptoKitty which was the first real use case to take off on the Ethereum blockchain in 2017.\nSome of these assets are physical and tangible and others are digital and intangible.\nSo NFTs are non-fungible tokens. \u0026lsquo;Token\u0026rsquo; refers to a digital certificate stored on a secure distributed database called a blockchain.\nNFTs are digital assets. Publicly verifiable intellectual property authenticated on a blockchain.\nA popular place to currently browse a variety of NFTs is on http://OpenSea.io​\nIt\u0026rsquo;s an exciting space, I believe the NFT space will continue to grow in the coming years with some popular use cases being: Digital Art, Virtual Land, Gaming, Collectibles, Finance and much more.\nList of Crypto Art Platforms # https://superrare.co/mankind​ https://makersplace.com/mankind​ https://niftygateway.com/collections https://opensea.io​ https://rarible.com​ https://knownorigin.io​ https://foundation.app​ https://www.bitski.com​ https://zora.co​ https://mintbase.io​ https://mintable.app​ https://niftykit.com​ https://viv3.com Youtube Reference Video # https://youtu.be/DYyW_tTPAhU ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/cryptoart/","section":"Posts","summary":"","title":"CryptoArt","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/google-tag-manager/","section":"Tags","summary":"","title":"Google Tag Manager","type":"tags"},{"content":" Intro # In this article it will be provided a quick walkthrough on Google Tag Manager.\nWhat is Google Tag Manager # Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.\nWhat is a Tag # Tags are segments of code provided by analytics, marketing, and support vendors to help you integrate their products into your websites or mobile apps. With Google Tag Manager, you no longer need to add these tags directly to your projects. Instead, you configure and publish tags and how they fire from within the Tag Manager user interface.\nWhen we speak of tags in an HTML context, we refer to tags such as \u0026lt;body\u0026gt;, \u0026lt;p\u0026gt;, \u0026lt;li\u0026gt;, \u0026lt;blockquote\u0026gt;, and so on. When we refer to tags used in the analytics and marketing industry, we refer to code that an organization provides to install the desired product or functionality on your website or mobile app.\nTag Manager natively supports many Google and 3rd party tag configurations. Custom tags may be used to implement tags that are not yet supported by Tag Manager\u0026rsquo;s native templates.\nHow it works # A snippet of code (container) is required to be installed on each page of your website like it happens for instance in Google Analytics. After that one needs to setup the triggers that would fire the tags using the tagmanager UI. After publishing those tags the events are triggered based on the defined rules in a asynchronous way. NOTE: You need one tag manager account per company or organization, but if you are tracking for several companies, you can connect to multiple tag manager accounts to a single user\u0026rsquo;s Google account. Each Tag Manager account has at least one container. A container includes the tags and “triggers” that determine when those tags should fire or collect data. Typically, you’ll have one container for each website domain, though you can use a single container for cross-domain tracking\nHow to Setup # In order to setup GTM the following steps are required:\nCreate an account, or use an existing account, at tagmanager.google.com. (A new container is created by default, and you can create additional containers within each account.) Install the container in your website or mobile app. For web and AMP pages: Add the container snippet as instructed by Tag Manager, and remove any existing tags. For mobile apps: Use the Firebase SDK for Android or iOS. Add and publish your tags. NOTE: A Tag Manager account represents the topmost level of organization. Typically, only one account is needed per company. A Tag Manager account contains one or more containers, and there are specific container types that may be used for websites, AMP pages, Android apps, and iOS apps.\nInstall the snippet # Place the \u0026lt;script\u0026gt; code snippet in the \u0026lt;head\u0026gt; of your web page HTML output, preferably as close to the opening \u0026lt;head\u0026gt; tag as possible, but below any dataLayer declarations.\nPlace the \u0026lt;noscript\u0026gt; code snippet immediately after the \u0026lt;body\u0026gt; tag in your HTML output.\nNOTE: To ensure that tags do not fire twice, remove from your code any hardcoded tags that have been migrated to your Tag Manager container.\nWhy should one use Google Tag Manager # Tag manager allows to simplify the management of tags and reduce the dependency from engineering teams. Acts as a bridge between Marketers and developers. Allow to have a collaborative space for marketing teams to test and source control their changes, improving the workflow to have this changes deployed. Improves page loads as the tags are fired asynchronously. How to use Google Tag Manager # Manage Tags # Tags are fired based on triggers and these make use of:\nVariable Operators Values Variables # Built-in Variables: ex: {{page path}} or {{click id}} User defined variables: ex: {{purchaseComplete}} Operators # Operators define the relation with variable and values: ex: contains, equals Values # Values like numbers or strings: ex: 1, 2, 3 or a, b , c When the user visit the site the container snippet will trigger tags based on firing instructions which communicates to systems such as Google Analytics.\nDataLayer # The Data Layer in Tag Manager is a Javascript object that holds data such as custom event information or variables passed from your website. Information in the Data Layer is structured into key-value pairs, which may be passed to third-party applications like Analytics or used as a trigger to determine when tags should fire.\nThere are two ways to populate the Data Layer with these key-value pairs.\nPre-populating values in the data layer Javascript push into the Data Layer NOTE: It’s important to note that Data Layer variables don’t persist across pages automatically. If you wish to pass a value to the Data Layer on additional pages, you’ll need to write custom code to do so.\nDefinitions # KPI - Key Performance Indicator (KPI) Definition A Key Performance Indicator is a measurable value that demonstrates how effectively a company is achieving key business objectives. Organizations use KPIs at multiple levels to evaluate their success at reaching targets. References # https://marketingplatform.google.com/about/tag-manager/ https://developers.google.com/tag-manager https://support.google.com/tagmanager/answer/6102821?hl=en ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/google-tag-manager/","section":"Posts","summary":"","title":"Google Tag Manager","type":"posts"},{"content":"","date":"23 December 2022","externalUrl":null,"permalink":"/tags/scrum/","section":"Tags","summary":"","title":"Scrum","type":"tags"},{"content":"This article provides some good practices to write scrum stories\nIntro # This article provides some good practices to write scrum stories\nComponents # When writing user stories, you will need to include the following components:\nUser persona. What is your user like? What is their relation to the project? What goals do they have?\nDefinition of Done. This refers to an agreed upon set of items that must be completed before a user story can be considered complete.\nTasks. What are the key activities needed to complete the user story?\nAny feedback already provided. If you are adding features to an existing product and you have already received feedback from customers on a past iteration, make sure to consider this feedback.\nI.N.V.E.S.T # Your user stories should meet the I.N.V.E.S.T. criteria:\nIndependent: The story’s completion is not dependent on another story.\nNegotiable: There is room for discussion about this item.\nValuable: Completing the user story has to deliver value.\nEstimable: The Definition of Done must be clear so that the team can give each user story an estimate.\nSmall: Each user story needs to be able to fit within a planned Sprint.\nTestable: A test can be conducted to check that it meets the criteria.\nStory Structure # As a \u0026lt;user role\u0026gt;, I want \u0026lt;this action\u0026gt; so that I can \u0026lt;get this value\u0026gt;. References # https://www.coursera.org/learn/agile-project-management https://www.mountaingoatsoftware.com/blog/stories-epics-and-themes ","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/scrum-user-stories/","section":"Posts","summary":"","title":"Scrum User Stories","type":"posts"},{"content":"In this article I\u0026rsquo;ll go through the changes between GA4 and UA\nGoogle Analytics quick History # In the October of 2020, Google rolled out Google Analytics 4 (GA4), the latest iteration of the Google Analytics platform. GA4 replaces Universal Analytics (UA) as the default for digital analytics measurement in GA.\nThe objective of this article is to consolidate the most significative changes, and required operations to migrate to the new data model.\nDifferences in UA and GA4 data models # Events and Hit types # The most significative change from UA to GA4 in my opinion is the fact that all interactions are now reported as events.\nIn UA an event has a Category, Action and Label for that specific hit type. In GA4 all hits are events there is no distinction on the hit types\nHit type in (UA) Measurement in GA4 Page View Event Event Event Social Event Transaction/e-commerce Event User timing Event Exception Event App/screen view Event Events Categories # GA4 events fall into four categories:\nAutomatically collected events Enhanced measurements events Recommended events Custom events Automatically collected events # This events are logged as long you have the base code for GA4 snippet on your site either via gtag.js or Google Tag Manager. No code required\nEnhanced Measurements # Enhanced measurements events are events that are automatically logged no code required and can be enable/disabled on the properties. also they don\u0026rsquo;t count for the named events limit that we will check next. They allow you to measure interactions with your content.\nRecommended Events # This events have predefined names and parameters and are recommend for specific business use cases. This events allow you to have more detail on your reports and enables the benefit from latest features but require you to add code as they are not logged automatically.\nThey are recommended for:\nRetail and e-commerce Jobs/Education, Local Deals, Real Estate Travel Games Custom Events # Custom events are implemented by yourself. You should use as a last resource and identify if the previous ones provide the data you require.\nPageviews # Page view in UA translate to the page_view event . This event is trigger automatically using the gtag.js snippet or by Google Tag manager. For Mobile apps the analog to a pageview a screenview is mapped to a screen_view event\nSessions # Sessions is a group of interactions with your website that takes place within a given time frame. In UA sessions are typically defined as having ended once there has been a 30m period of inactivity or another reset event. In GA4 session metrics are derived from session_start event and the last event in the session.\nActive user calculation: User activity is now detected automatically in GA4 in contrast with UA. Due to the fact that the ID can span to multiple devices it\u0026rsquo;s normal that we find mismatch when comparing both UA and GA4 data. Session counting: One important change is that a new Campaign does not reset the session in GA4 has occurs in UA. Also GA4 processes the sessions with arrives up to 72 hours late while in UA the processing takes place getting data if they arrive 4 hours of close of the preceding day. Another important feature is that the iOS SDK uploads metrics automatically when apps are background, this can also result in different values if compared with UA data. Custom Metrics and Dimensions # Custom Metrics and Dimensions have their own scope. When changing to GA4 the following mapping should be applied.\nScope in UA GA4 property Hit-scoped Events or parameters User-scoped User properties Session-scoped N/A Product-scoped E-commerce parameters Remember that events, event parameters and user properties are subject to limits.\nContent Grouping # This is also big change as in UA you could group content into a logical structure, and then compare metrics by group. (ex: Man/Shirts). In GA4 there is a predefined event content_group that populates data into a \u0026ldquo;Content Group\u0026rdquo; Dimension\nParameters # In GA4 you can send parameters with each event. Parameters are additional pieces of information that can further specify the action the user took, or add more context.Each event can log up to 25 parameters.\nUser property # User properties are attributes about the users who interact with your app or website. They are used to describe segments of your user base.\nData collection settings that can be migrated # The following data collection settings migrate in one-to-one from UA to GA4 as long as you use the gtag.js snippet or Google tag Manager.\ncookie customization ads personalization Data collection settings with no equivalent in GA4 # control over IP anonymization - Enabled by default in GA4 properties get clientID - Not currently available custom task - Not available in GA4 timing - Not available in GA4 Limits # Collections and configurations limits at the date:\nDistinctly named events 500 per app instance. (Can not be deleted if reaching limit) Length of event names is 40 characters Limit of 25 parameters per event Parameters value have the length limit of 100 characters 25 user properties that can be added length of user property names is 24 characters length of user property values is 36 characters 100 audiences can be created by property 30 conversions can be created by property Data retention up to 14 months For a complete list check the following article\nComparing report data in UA and GA4 # As refereed there are several changes that may impact your report when shifting from UA to GA4. Google recommends that one creates a new tracking ID and keep both tracking options working simultaneously, preferably managed in Google Tag manager.\nMake sure of the following:\nYour tracking ID from UA and GA4 are both collecting the same web pages Both properties have equivalent tag implementations All tags are firing successfully Both properties use the same time zone Make sure to compare an unfiltered view in UA with a single web data stream in GA4 Both properties have been collecting data for at least 30m in order to compare on the realtime report Cheers, RR\n","date":"23 December 2022","externalUrl":null,"permalink":"/2022/12/google-analytics-ua-vs-ga4/","section":"Posts","summary":"","title":"Google Analytics - UA vs GA4","type":"posts"},{"content":"","date":"30 August 2022","externalUrl":null,"permalink":"/tags/communication/","section":"Tags","summary":"","title":"Communication","type":"tags"},{"content":"Cheat Sheet with the NATO Alphabet\nIntro # The (International) Radiotelephony Spelling Alphabet, commonly known as the NATO phonetic alphabet, is the most widely used set of clear code words for communicating the letters of the Roman alphabet, technically a radiotelephonic\nA spelling alphabet is used to spell parts of a message containing letters and numbers to avoid confusion, because many letters sound similar, for instance \u0026ldquo;n\u0026rdquo; and \u0026ldquo;m\u0026rdquo; or \u0026ldquo;f\u0026rdquo; and \u0026ldquo;s\u0026rdquo;; the potential for confusion increases if static or other interference is present.\nAlfa Bravo Charlie Delta Echo Foxtrot Golf Hotel India Juliett Kilo Lima Mike November Oscar Papa Quebec Romeo Sierra Tango Uniform Victor Whiskey X-ray Yankee Zulu ","date":"30 August 2022","externalUrl":null,"permalink":"/2022/08/nato-phonetic-alphabet/","section":"Posts","summary":"","title":"NATO phonetic alphabet","type":"posts"},{"content":" Disclaimer # This disclaimer (“Disclaimer”) sets forth the general guidelines, disclosures, and terms of your use of the rramos.github.io website (“Website” or “Service”) and any of its related products and services (collectively, “Services”). This Disclaimer is a legally binding agreement between you (“User”, “you” or “your”) and this Website operator (“Operator”, “we”, “us” or “our”). If you are entering into this agreement on behalf of a business or other legal entity, you represent that you have the authority to bind such entity to this agreement, in which case the terms “User”, “you” or “your” shall refer to such entity. If you do not have such authority, or if you do not agree with the terms of this agreement, you must not accept this agreement and may not access and use the Website and Services. By accessing and using the Website and Services, you acknowledge that you have read, understood, and agree to be bound by the terms of this Disclaimer. You acknowledge that this Disclaimer is a contract between you and the Operator, even though it is electronic and is not physically signed by you, and it governs your use of the Website and Services.\nRepresentation # Any views or opinions represented on the Website are personal and belong solely to the Operator and do not represent those of people, institutions or organizations that the Operator may or may not be associated with in professional or personal capacity unless explicitly stated. Any views or opinions are not intended to malign any religion, ethnic group, club, organization, company, or individual.\nContent and postings # You may print or copy any part of the Website and Services for your personal or non-commercial use.\nReviews and testimonials # Testimonials are received in various forms through a variety of submission methods. The testimonials are not necessarily representative of all of those who will use Website and Services, and the Operator is not responsible for the opinions or comments available on the Website, and does not necessarily share them. All opinions expressed are strictly the views of the reviewers.\nThe testimonials displayed are given verbatim except for grammatical or typing error corrections. Some testimonials may have been edited for clarity, or shortened in cases where the original testimonial included extraneous information of no relevance to the general public. Testimonials may be reviewed for authenticity before they are available for public viewing.\nIndemnification and warranties # The accuracy, reliability and completeness of the information and content, distributed through, linked, downloaded or accessed from the Website and Services are guaranteed by the Operator. However, all information on the Website is provided “as is”, with no guarantee of the results obtained from the use of this information, and without warranty of any kind, express or implied. In no event will the Operator be liable to you or anyone else for any decision made or action taken in reliance on the information on the Website, or for any consequential, special or similar damages, even if advised of the possibility of such damages. Information on the Website is for general information purposes only and is not intended to provide any type of professional advice. Please seek professional assistance should you require it. Information contained on the Website are subject to change at any time and without warning.\nChanges and amendments # We reserve the right to modify this Disclaimer or its terms related to the Website and Services at any time at our discretion. When we do, we will revise the updated date at the bottom of this page. We may also provide notice to you in other ways at our discretion, such as through the contact information you have provided.\nAn updated version of this Disclaimer will be effective immediately upon the posting of the revised Disclaimer unless otherwise specified. Your continued use of the Website and Services after the effective date of the revised Disclaimer (or such other act specified at that time) will constitute your consent to those changes.\nAcceptance of this disclaimer # You acknowledge that you have read this Disclaimer and agree to all its terms and conditions. By accessing and using the Website and Services you agree to be bound by this Disclaimer. If you do not agree to abide by the terms of this Disclaimer, you are not authorized to access or use the Website and Services. This disclaimer was created with the help of the disclaimer generator at https://app.websitepolicies.com/disclaimer-generator\nContacting us # If you have any questions, concerns, or complaints regarding this Disclaimer, we encourage you to contact us using the details below:\nhttps://rramos.github.io/about/\nThis document was last updated on February 13, 2022\n","date":"13 February 2022","externalUrl":null,"permalink":"/disclaimer/","section":"rramos.github.io","summary":"","title":"Disclaimer","type":"page"},{"content":"This article is about Visualcode plugin for Hexo\nVisualCode Hexo extension # This article explain how to activate the visualcode hexo extension which would boost your productivity if you use Hexo for your articles such has i\u0026rsquo;m doing on this blob page.\nSetup # Install VisualCode:\nhttps://code.visualstudio.com/docs/setup/setup-overview Activate the following extension by executing the following command\next install fantasy.vscode-hexo-utils Make sure you open the folder where your Hexo base code is.\nThat\u0026rsquo;s it.\nThe extension allows you to directly manage your posts/tags/categories. It is also very useful if you side lane the articles and preview the Markdowns with [Ctrl] + [Shift] + [V]\nThe extension repo is available on the following github repo:\nhttps://github.com/cwxyz007/vscode-hexo-utils Kudos for the contributors\nCheers, RR\n","date":"21 February 2021","externalUrl":null,"permalink":"/2021/02/visualcode-hexo-addon/","section":"Posts","summary":"","title":"VisualCode Hexo addon","type":"posts"},{"content":"This article is about git and untrack committed files\nIntro # This is something that sometimes bugs me, you keep hitting the .gitinore configuration file but still the file you try to ignore always gets referenced on your changes.\nIf that happens most probably you\u0026rsquo;ve reference it in the past to be track and you need to remove it, so this are the steps.\nUpdate your .gitignore and untrack the file # This example to untrack db.json\nChange the entry in .gitignore ex: *.json\nExecute the following command to untrack\ngit update-index --assume-unchanged db.json Commit the changes ","date":"21 February 2021","externalUrl":null,"permalink":"/2021/02/git-untrack-previous-committed-files/","section":"Posts","summary":"","title":"Git Untrack previous committed files","type":"posts"},{"content":"In this article I share some useful dockers.\nIntro # This article is just to share a list of useful dockers, in case one wants to speed test some feature and required some of this software to be up and running.\nWhy # The main ideia is that i can fire up quickly without having to worry on the ports that are already mapped on my local machine, so the several dockers should have collision ports taken care of.\nOne should just need to execute docker-compose up and that\u0026rsquo;s it.\nYou could also use other repo, but i\u0026rsquo;ll try to make my local configurations in a way that i could reuse several dockers.\nDockers repo # You just need to clone the repo https://github.com/rramos/dockers\ngit clone https://github.com/rramos/dockers.git Requirements # Make sure you have docker and docker compose properly configured\nSetup # Just fire-up docker-compose and that\u0026rsquo;s it\ncd docker-portainer docker-compose up -d Each docker has it\u0026rsquo;s own README file with more instructions.\nDocker List # docker-grafana docker-hdp docker-ksql docker-nifi docker-portainer docker-zeppelin docker-hbase docker-jenkins docker-moodle docker-openproject docker-rundeck This is the available list at the moment of this article, i will add up more as i\u0026rsquo;m using and testing new stuff.\nNOTE: Use this containers at your own risk\nCheers, RR\n","date":"18 October 2017","externalUrl":null,"permalink":"/2017/10/dockers/","section":"Posts","summary":"","title":"Dockers","type":"posts"},{"content":"This article would be about Dr-Elephant\nIntro # This article would be about dr-elephant A Performance and Monitoring tool for Hadoop and Spark.\nDr. Elephant is a performance monitoring and tuning tool for Hadoop and Spark. It automatically gathers all the metrics, runs analysis on them, and presents them in a simple way for easy consumption. Its goal is to improve developer productivity and increase cluster efficiency by making it easier to tune the jobs. It analyzes the Hadoop and Spark jobs using a set of pluggable, configurable, rule-based heuristics that provide insights on how a job performed, and then uses the results to make suggestions about how to tune the job to make it perform more efficiently.\nRequirements # Install mysql-server and create a BD for dr-elephant sudo apt-get install mysql-server MySQl preparation mysql\u0026gt; create database drelephant; Query OK, 1 row affected (0.00 sec) mysql\u0026gt; grant all on drelephant.* to drelephant@localhost identified by \u0026#39;drelephant\u0026#39;; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; flush privileges; Query OK, 0 rows affected (0.00 sec) Install zip command sudo apt-get install zip Install sbt echo \u0026#34;deb https://dl.bintray.com/sbt/debian /\u0026#34; | sudo tee -a /etc/apt/sources.list.d/sbt.list sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823 sudo apt-get update sudo apt-get install sbt Setup # Clone the repo git https://github.com/linkedin/dr-elephant.git Compile sbt package sbt dist cp ./target/universal/dr-elephant-2.0.3-SNAPSHOT.zip . ./compile.sh cd dist unzip dr-elephant-2.0.3-SNAPSHOT.zip Starting the service export ELEPHANT_CONF_DIR=../../app-conf ./bin/start.sh One can now access the web interface at: http://localhost:8080\nConclusion # This tool seems very powerful. At the moment i haven\u0026rsquo;t tested changing the recommendations it provided, but will try them soon. Spark 2.x applications don\u0026rsquo;t seem to be working at the moment\nExtended tests # Test Oozie Scheduler integration Test Airflow integration Define a deployment strategy Test recommended changes References # https://github.com/linkedin/dr-elephant https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark ","date":"13 October 2017","externalUrl":null,"permalink":"/2017/10/dr.-elephant-overview/","section":"Posts","summary":"","title":"Dr. Elephant Overview","type":"posts"},{"content":"","date":"13 October 2017","externalUrl":null,"permalink":"/tags/hadoop/","section":"Tags","summary":"","title":"Hadoop","type":"tags"},{"content":"","date":"13 October 2017","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"Monitoring","type":"tags"},{"content":"","date":"13 October 2017","externalUrl":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance","type":"tags"},{"content":"","date":"13 October 2017","externalUrl":null,"permalink":"/tags/tuning/","section":"Tags","summary":"","title":"Tuning","type":"tags"},{"content":"","date":"9 October 2017","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"This article is about ansible-cmdb a nice solution for gathering your ansible facts into a visual format.\nIntro # This article is about ansible-cmdb a very nice solution for gathering your ansible facts into a visual format.\nAnsible-cmdb takes the output of Ansible\u0026rsquo;s fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information. It supports multiple types of output (html, csv, sql, etc) and extending information gathered by Ansible with custom data. For each host it also shows the groups, host variables, custom variables and machine-local facts.\nSupported output formats / templates # Fancy HTML (\u0026ndash;template html_fancy), as seen in the screenshots above. Fancy HTML Split (\u0026ndash;template html_fancy_split), with each host\u0026rsquo;s details in a separate file (for large number of hosts). CSV (\u0026ndash;template csv), the trustworthy and flexible comma-separated format. JSON (\u0026ndash;template json), a dump of all facts in JSON format. Markdown (\u0026ndash;template markdown), useful for copy-pasting into Wiki\u0026rsquo;s and such. Markdown Split (\u0026rsquo;\u0026ndash;template markdown_split\u0026rsquo;), with each host\u0026rsquo;s details in a separate file (for large number of hosts). SQL (\u0026ndash;template sql), for importing host facts into a (My)SQL database. Plain Text table (\u0026ndash;template txt_table), for the console gurus. and of course, any custom template you\u0026rsquo;re willing to make. Setup # And it\u0026rsquo;s so simple to setup :D\nsudo pip install ansible-cmdb Note: You could also use your distribution preferred method. check the official site.\nCreating Reports # Simple console generation\nansible-cmdb -t txt_table --columns name,os,ip,mem,cpus out The HTML Fancy one\nansible-cmdb --template txt_table out --template html_fancy Official Screenshot\nConclusion # I found this software quite simple and very useful. I found that --template txt_table is quite handy to use on Documentation for small environments. For bigger projects the CSV could be a better choice. Also the fancy HTML templates are very useful on playbook development phases where you can confirm your facts very easily. Ansible-cmdb started as a short Python script, but many people made contributions so far.\nCheers, RR\nReferences # https://github.com/fboender/ansible-cmdb ","date":"9 October 2017","externalUrl":null,"permalink":"/2017/10/ansible-cmdb/","section":"Posts","summary":"","title":"ansible-cmdb","type":"posts"},{"content":"","date":"9 October 2017","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"Just a simple Basic CheatSheet on Spark RDD\u0026rsquo;s\nIntro # Just a simple Basic CheatSheet on Spark RDD\u0026rsquo;s\nBasic Transformations on a RDD containing: {1,2,3,3} Function Name Example Result map() rdd.map(x =\u0026gt; x +1) {2,3,4,4} flatmap() rdd.flatMap(x =\u0026gt; x.to(3)) {1,2,3,2,3,3,3} filter() rdd.filter(x =\u0026gt; x != 1 ) {2,3,3} distinct() rdd.distinct() {1,2,3} sample() rdd.sample(false,0.5) Nondeterministic Basic two-RDD transformations on RDDs: {1,2,3} and {3,4,5} Function Name Example Result union() rdd.union(other) {1,2,3,3,4,5} intersection() rdd.intersection(other) {3} subtract() rdd.subtract(other) {1,2} cartesian() rdd.cartesian(other) {(1,3),(1,4)\u0026hellip;(3,5)} Basic Actions on RDD containing: {1,2,3,3} Function Name Example Result collect() rdd.collect() {1,2,3,3} count() rdd.count() 4 countByValue() rdd.countByValue() {(1,1),(2,1),(3,2)} take() rdd.take(2) {1,2} top() rdd.top(2) {3,3} takeOrdered() rdd.takeOrdered(2)(myOrdering) {3,3} takeSample() rdd.takeSample(false,1) Nondeterministic reduce() rdd.reduce((x,y) =\u0026gt; x + y ) 9 fold() rdd.fold(0)((x,y) =\u0026gt; x + y) 9 foreach() rdd.foreach(func) Nothing ","date":"9 October 2017","externalUrl":null,"permalink":"/2017/10/rdd-basic-transformations-operations/","section":"Posts","summary":"","title":"RDD Basic Transformations Operations","type":"posts"},{"content":"","date":"6 October 2017","externalUrl":null,"permalink":"/tags/powershell/","section":"Tags","summary":"","title":"PowerShell","type":"tags"},{"content":"This article is about the usage of Powershell in Linux\nIntro # This article will present the required steps to install powershell on Linux.\nObjective # The objective is to operate from Linux Windows Azure Pack.\nProcedure # Import the public repository GPG keys curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add - Register the Microsoft Ubuntu repository curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list Update the list of products sudo apt-get update Install PowerShell sudo apt-get install -y powershell Start PowerShell powershell Add Azure module # Install Azure module in powershell running with sudo\nInstall-Module AzureRM Install-Module Azure Check module location\n(gmo -l Azure*).path Uninstall # sudo apt-get remove powershell Conclusion # After some hours i\u0026rsquo;ve given up making this thing work on Linux. The time it takes doesn\u0026rsquo;t payoff, and even if i could make it work, i would still need to make Vagrant work with it for some VM\u0026rsquo;s provisioning in a private cloud. It would take me less time to do it by hand :)\nFrom a standard testing perspective it\u0026rsquo;s nice to have this thing on Linux, because you could centralize the management, but at the moment of this article it doesn\u0026rsquo;t seem to payoff the time one would need to setup this.\nIf you have lot\u0026rsquo;s of Windows machines or Azure infrastructure to manage just get a Windows PC, or wait for Microsoft to fix this, as there are lot\u0026rsquo;s of reports regarding this issue. But seriously i don\u0026rsquo;t see an effort from Microsoft to make this work properly, what\u0026rsquo;s the gain, right !?.\nCheers, RR\nReferences # https://github.com/PowerShell/PowerShell https://github.com/bgelens/WAPTenantPublicAPI http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/ https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/ https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/ ","date":"6 October 2017","externalUrl":null,"permalink":"/2017/10/powershell-on-linux/","section":"Posts","summary":"","title":"PowerShell on Linux","type":"posts"},{"content":"","date":"6 October 2017","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":"In this article you will learn how to convert images from Virtualbox to KVM.\nIntro # In this article i will explain a quick way to convert VirtualBox images (vdi) into qcow2 and load them in KVM.\nWhy # This is something i normally do, because i get better performance running Linux VM\u0026rsquo;s on KVM and saves me some space with qcow2 volume format.\nqcow2 Features # One of the main characteristics of qcow disk images is that files with this format can grow as data is added. This allows for smaller file sizes than raw disk images, which allocate the whole image space to a file, even if parts of it are empty\nRequirements # You should have KVM configured correctly sudo apt-get install qemu-kvm Install qemu-utils sudo apt-get install qemu-utils Let\u0026rsquo;s get an example image. I\u0026rsquo;m fetching one from http://www.osboxes.org/ubuntu. (ex: Ubuntu 17.10 10th-August build (64bit).vdi) NOTE: Procedure executed with qemu-img version 2.5.0\nConverting # Convert the image qemu-img convert -f vdi -O qcow2 Ubuntu_17.10.vdi Ubuntu_17.10.qcow2 You can check the image details by executing qemu-img info Ubuntu_17.10.qcow2 . qemu-img info Ubuntu_17.10.qcow2 image: Ubuntu_17.10.qcow2 file format: qcow2 virtual size: 100G (107374182400 bytes) disk size: 4.1G cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false Now lets Upload this image to our KVM pool and create a new VM from it: sudo virsh vol-create-as default ubuntu-17.10 100G --format qcow2 sudo virsh vol-upload --pool default ubuntu-17.10 Ubuntu_17.10.qcow2 Start virt-manager and select Import existing Disk Image\nThe default password for this images are osboxes.org . You should have something like the following in the console.\nIf you want to create brand new images you could use virt-install, but that one would go to another article. Cheers, RR\nReferences # https://www.virtualbox.org/wiki/Downloads https://www.linux-kvm.org/page/Main_Page http://www.osboxes.org/ubuntu https://docs.openstack.org/image-guide/convert-images.html ","date":"2 October 2017","externalUrl":null,"permalink":"/2017/10/convert-virtualbox-images-to-kvm/","section":"Posts","summary":"","title":"Convert VirtualBox images to KVM","type":"posts"},{"content":"","date":"2 October 2017","externalUrl":null,"permalink":"/tags/kvm/","section":"Tags","summary":"","title":"KVM","type":"tags"},{"content":"","date":"2 October 2017","externalUrl":null,"permalink":"/tags/qemu/","section":"Tags","summary":"","title":"QEMU","type":"tags"},{"content":"","date":"2 October 2017","externalUrl":null,"permalink":"/tags/vdi/","section":"Tags","summary":"","title":"VDI","type":"tags"},{"content":"","date":"2 October 2017","externalUrl":null,"permalink":"/tags/virtualbox/","section":"Tags","summary":"","title":"VirtualBox","type":"tags"},{"content":"","date":"29 September 2017","externalUrl":null,"permalink":"/tags/cloudera/","section":"Tags","summary":"","title":"Cloudera","type":"tags"},{"content":"This article is about Apache Nifi\nIntro # I\u0026rsquo;ve been using Apache Nifi for some time now. But has a Cloudera user it would be nice to have it managed centralized in Cloudera Manager.\nI found this repo which had only one commit for doing exactly this, so i decided to give it a try, to see if it worked.\nRequirements # Virtual machine with Cloudera quickstart pre-installed I had to patch the available Nifi version with NIFI-3466 And parcel repo also with Issue1 So i\u0026rsquo;ll be using my own forks that have this changes already, but will be following the author (prateek) instructions.\nInstructions # Install Requirements # cloudera/cm_ext cd /tmp git clone https://github.com/cloudera/cm_ext cd cm_ext/validator mvn install Create parcel \u0026amp; CSD: cd /tmp git clone git@github.com:rramos/nifi.git cd nifi mvn clean install cd /tmp git clone git@github.com:rramos/nifi-parcel.git cd nifi-parcel POINT_VERSION=5 VALIDATOR_DIR=/tmp/cm_ext ./build-parcel.sh /tmp/nifi/nifi-assembly/target/nifi-*-SNAPSHOT-bin.tar.gz VALIDATOR_DIR=/tmp/cm_ext ./build-csd.sh Test the Parcel # Cloudera quickstart images comes with java version 1.7. But i need version 1.8 for Nifi otherwise i\u0026rsquo;ll get minor major version mismatch issue. So i had to make some changes.\nFirst start your cloudera quickstart VM and copy the folders:\nbuild-csd build-parcel there in my case i had IP 192.168.122.132 (change for your case).\nscp -r build-parcel cloudera@192.168.122.132:/tmp scp -r build-csd cloudera@192.168.122.132:/tmp SSH in the machine and install java8 yum install java-1.8.0-openjdk Run the following command in build-parcel folder cd build-parcel python -m SimpleHTTPServer 14641 Now open your browser at http://192.168.122.132:7180 and access Cloudera Manager (cloudera:cloudera).\nNavigate to -\u0026gt; Parcels -\u0026gt; Edit Settings.\nAdd http://192.168.122.132:14641\nDownload and install the Nifi Parcel.\nCopy NIFI home cp -r /tmp/build-parcel/NIFI-0.0.5.nifi.p0.5 /opt/cloudera/parcels/NIFI/ Correct java path in file /opt/cloudera/parcels/NIFI/bin/nifi-env.sh # The java implementation to use. export JAVA_HOME=/usr/lib/jvm/jre-1.8.0 Copy the csd jars cp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/ mkdir /opt/cloudera/csd/NIFI-1.2.0 cp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/NIFI-1.2.0/ cd /opt/cloudera/csd/NIFI-1.2.0/ jar xvf NIFI-1.2.0.jar rm -f NIFI-1.2.0.jar Correct ownership chown -R cloudera-scm:cloudera-scm /opt/cloudera Apply Configuration changes to Cloudera Manager and restart the service\nMove CSD to Cloudera Manager\u0026rsquo;s CSD Repo\nsudo service cloudera-scm-server restart After login again in CM wait for zookeeper to be running normal, and add a new service (Nifi) After terminating the wizard you should have access to Nifi Interface\nConclusion # I must say that prateek repo with only that commit worked pretty well and the instructions where also clear. There where some minor adjustments because of the java version, but we can start/stop the service via Cloudera Manager.\nThere are some pending items referred in his github page:\nCurrently NiFi runs under the root user Expose config options under Cloudera Manager Conf folder from parcels is used, this needs to be migrated to ConfigWriter Expose metrics from NiFi I haven\u0026rsquo;t tested the configuration in cluster mode as i has using the quickstart VM.\nThe configuration options in CM would be very good improvement. Auto configuring zookeeper and the other nifi options. I\u0026rsquo;ll try to contribute to prateek excellent work if i manage to get some time.\nCheers, RR\nReferences # https://nifi.apache.org https://github.com/prateek/nifi-parcel https://github.com/cloudera/cm_ext https://www.cloudera.com/downloads/quickstart_vms/5-12.html ","date":"29 September 2017","externalUrl":null,"permalink":"/2017/09/nifi-parcel-for-cdh/","section":"Posts","summary":"","title":"Nifi Parcel for CDH","type":"posts"},{"content":"","date":"27 September 2017","externalUrl":null,"permalink":"/tags/big-data/","section":"Tags","summary":"","title":"Big Data","type":"tags"},{"content":"In this article I will go through the process of configuring a UDF function for Hive.\nIntro # Today i was going to use a simple sha256 function in Hive in order to mask a column and apparently in the latest Cloudera distribution the Shipped hive version doesn\u0026rsquo;t have that native function.\nThis article will explain how you can build a sha256 or other udfs function and add it in Hive.\nChecking Cloudera Packages Version # Check the following URL in order to see the latest shipped package versions in Cloudera.\nCDH 5.12 -\u0026gt; hive-1.1.0+cdh5.12.1+1197\nReturn Type Name(Signature) Description string sha2(string/binary, int) Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2(\u0026lsquo;ABC\u0026rsquo;, 256) = \u0026lsquo;b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78\u0026rsquo;. Full Version\nImplement UDFS # this will server more as an exercise, one could create a more complex udf function. For the time being let\u0026rsquo;s create a GenericUDFSha2 based on existing hive 1.3.0 version\nYou code clone my repo with some udfs-utils here\nThe original code for hive version 1.3.0 is available in the repo\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java\nLet\u0026rsquo;s create the building structure\nmkdir -p GenericUDFSha2/org/apache/hadoop/hive/ql/udf/generic cd GenericUDFSha2 Let\u0026rsquo;s create a pom.xml file\n\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.rramos.bigdata.utils\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;GenericUDFSha2\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;GenericUDFSha2\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.apache.org\u0026lt;/url\u0026gt; \u0026lt;!-- properties --\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;!-- prerequisitesprerequisites --\u0026gt; \u0026lt;prerequisites\u0026gt; \u0026lt;maven\u0026gt;3.0\u0026lt;/maven\u0026gt; \u0026lt;/prerequisites\u0026gt; \u0026lt;!-- Dependencies --\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;joda-time\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;joda-time\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!-- Build options --\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;addClasspath\u0026gt;true\u0026lt;/addClasspath\u0026gt; \u0026lt;mainClass\u0026gt;com.rramos.bigdata.utils.GenericUDFSha2\u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;mainClass\u0026gt; com.rramos.bigdata.utils.GenericUDFSha2 \u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; You should obviously change for you packaging namespace, i\u0026rsquo;m just using com.rramos.bigdata.utils to be simpler.\nNext, let\u0026rsquo;s create the following file\norg/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java\nwith the content\npackage com.rramos.bigdata.utils; import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.BINARY_GROUP; import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.NUMERIC_GROUP; import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.STRING_GROUP; import java.security.MessageDigest; import java.security.NoSuchAlgorithmException; import org.apache.commons.codec.binary.Hex; import org.apache.hadoop.hive.ql.exec.Description; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter; import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils; import org.apache.hadoop.io.BytesWritable; import org.apache.hadoop.io.Text; /** * GenericUDFSha2. * */ @Description(name = \u0026#34;sha2\u0026#34;, value = \u0026#34;_FUNC_(string/binary, len) - Calculates the SHA-2 family of hash functions \u0026#34; + \u0026#34;(SHA-224, SHA-256, SHA-384, and SHA-512).\u0026#34;, extended = \u0026#34;The first argument is the string or binary to be hashed. \u0026#34; + \u0026#34;The second argument indicates the desired bit length of the result, \u0026#34; + \u0026#34;which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). \u0026#34; + \u0026#34;SHA-224 is supported starting from Java 8. \u0026#34; + \u0026#34;If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL.\\n\u0026#34; + \u0026#34;Example: \u0026gt; SELECT _FUNC_(\u0026#39;ABC\u0026#39;, 256);\\n \u0026#39;b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78\u0026#39;\u0026#34;) public class GenericUDFSha2 extends GenericUDF { private transient Converter[] converters = new Converter[2]; private transient PrimitiveCategory[] inputTypes = new PrimitiveCategory[2]; private final Text output = new Text(); private transient boolean isStr; private transient MessageDigest digest; @Override public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { checkArgsSize(arguments, 2, 2); checkArgPrimitive(arguments, 0); checkArgPrimitive(arguments, 1); // the function should support both string and binary input types checkArgGroups(arguments, 0, inputTypes, STRING_GROUP, BINARY_GROUP); checkArgGroups(arguments, 1, inputTypes, NUMERIC_GROUP); if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(inputTypes[0]) == STRING_GROUP) { obtainStringConverter(arguments, 0, inputTypes, converters); isStr = true; } else { GenericUDFParamUtils.obtainBinaryConverter(arguments, 0, inputTypes, converters); isStr = false; } if (arguments[1] instanceof ConstantObjectInspector) { Integer lenObj = getConstantIntValue(arguments, 1); if (lenObj != null) { int len = lenObj.intValue(); if (len == 0) { len = 256; } try { digest = MessageDigest.getInstance(\u0026#34;SHA-\u0026#34; + len); } catch (NoSuchAlgorithmException e) { // ignore } } } else { throw new UDFArgumentTypeException(1, getFuncName() + \u0026#34; only takes constant as \u0026#34; + getArgOrder(1) + \u0026#34; argument\u0026#34;); } ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector; return outputOI; } @Override public Object evaluate(DeferredObject[] arguments) throws HiveException { if (digest == null) { return null; } digest.reset(); if (isStr) { Text n = GenericUDFParamUtils.getTextValue(arguments, 0, converters); if (n == null) { return null; } digest.update(n.getBytes(), 0, n.getLength()); } else { BytesWritable bWr = GenericUDFParamUtils.getBinaryValue(arguments, 0, converters); if (bWr == null) { return null; } digest.update(bWr.getBytes(), 0, bWr.getLength()); } byte[] resBin = digest.digest(); String resStr = Hex.encodeHexString(resBin); output.set(resStr); return output; } @Override public String getDisplayString(String[] children) { return getStandardDisplayString(getFuncName(), children); } @Override protected String getFuncName() { return \u0026#34;sha2\u0026#34;; } } And build the package.\nmvn package After compile you find in target dir the require jar (GenericUDFSha2-1.0-SNAPSHOT.jar) you need to add in Hive.\nYou should use your Hadoop Distribution instructions for deploying new jars.\nHere are Cloudera Instructions.\nNext on your Hive session you need to ADD JAR and create a FUNCTION or TEMPORARY FUNCTION\nADD JAR ./target/GenericUDFSha2-1.0-SNAPSHOT.jar CREATE TEMPORARY FUNCTION sha2 AS \u0026#39;com.rramos.bigdata.utils.GenericUDFSha2\u0026#39;; SELECT sha2(foo) from bar LIMIT1; Matthew Rathbone Blog has some great tutorial on Hive Functions. Take a look if you want to go deep with it.\nCheers, RR\nReferences # https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html ","date":"27 September 2017","externalUrl":null,"permalink":"/2017/09/hive-udfs-functions/","section":"Posts","summary":"","title":"Hive UDFS functions","type":"posts"},{"content":"","date":"27 September 2017","externalUrl":null,"permalink":"/tags/udfs/","section":"Tags","summary":"","title":"UDFS","type":"tags"},{"content":"","date":"26 September 2017","externalUrl":null,"permalink":"/tags/libvirt/","section":"Tags","summary":"","title":"Libvirt","type":"tags"},{"content":"","date":"26 September 2017","externalUrl":null,"permalink":"/tags/mesos/","section":"Tags","summary":"","title":"Mesos","type":"tags"},{"content":"This article is a quick guide for vagrant and libvirt\nIntro # This article is going to be about a quick way to setup a PoC environment using libvirt (QEMU) and vagrant, and auto configure the environment using ansible.\nWhy # One might go a bit further in the testing environments, instead of dockers using virtualized environment. This kind of approcah would be suitable when you like to try infrastructure services, or simple don\u0026rsquo;t want to use dockers.\nIf you check Internet Trends we can see docker interest have grown quite a lot on the last 5 years.\nStill i\u0026rsquo;m OldSchool and i like my plain-simple-VMs some times :D\nA later article would be about configuring a Mesos Cluster so this one makes sense as a preparation guide for it.\nRequirements # Make sure you have vagrant installed Make sure your machine supports virtualization :) Make sure you have QEMU installed Make sure you have libvirt up and running Make sure you have ansible (Optional) Note: This guide was tested on Ubuntu 16. Use at your own risk\nOne can test libvirt version suing the command\n$ virsh version Compiled against library: libvirt 1.3.1 Using library: libvirt 1.3.1 Using API: QEMU 1.3.1 Running hypervisor: QEMU 2.5.0 NOTE: Before you start using Vagrant-libvirt, please make sure your libvirt and qemu installation is working correctly and you are able to create qemu or kvm type virtual machines with virsh or virt-manager.\nsetup # Install dependencies\nsudo apt-get install qemu libvirt-bin ebtables dnsmasq sudo apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev sudo apt-get install vagrant ruby-libvirt Install vagrant-libvirt plugin:\nvagrant plugin install vagrant-libvirt Now let\u0026rsquo;s test by creating a startup Box. There are several available here\nExample: (Ubuntu 16)\nmkdir Vagrant cd Vagrant vagrant init marczis/ubuntu_16_04 Let\u0026rsquo;s start the VM\nvagrant up --provider=libvirt After executing the command you should have some output similar\n==\u0026gt; default: Successfully added box \u0026#39;marczis/ubuntu_16_04\u0026#39; (v0.1.2) for \u0026#39;libvirt\u0026#39;! ==\u0026gt; default: Uploading base box image as volume into libvirt storage... ==\u0026gt; default: Creating image (snapshot of base box volume). ==\u0026gt; default: Creating domain with the following settings... ==\u0026gt; default: -- Name: Vagrant_default ==\u0026gt; default: -- Domain type: kvm ==\u0026gt; default: -- Cpus: 1 ==\u0026gt; default: -- Feature: acpi ==\u0026gt; default: -- Feature: apic ==\u0026gt; default: -- Feature: pae ==\u0026gt; default: -- Memory: 512M ==\u0026gt; default: -- Management MAC: ==\u0026gt; default: -- Loader: ==\u0026gt; default: -- Base box: marczis/ubuntu_16_04 ==\u0026gt; default: -- Storage pool: default ==\u0026gt; default: -- Image: /var/lib/libvirt/images/Vagrant_default.img (10G) ==\u0026gt; default: -- Volume Cache: default ==\u0026gt; default: -- Kernel: ==\u0026gt; default: -- Initrd: ==\u0026gt; default: -- Graphics Type: vnc ==\u0026gt; default: -- Graphics Port: 5900 ==\u0026gt; default: -- Graphics IP: 127.0.0.1 ==\u0026gt; default: -- Graphics Password: Not defined ==\u0026gt; default: -- Video Type: cirrus ==\u0026gt; default: -- Keymap: en-us ==\u0026gt; default: -- TPM Path: ==\u0026gt; default: -- INPUT: type=mouse, bus=ps2 ==\u0026gt; default: Creating shared folders metadata... ==\u0026gt; default: Starting domain. ==\u0026gt; default: Waiting for domain to get an IP address... ==\u0026gt; default: Waiting for SSH to become available... default: default: Vagrant insecure key detected. Vagrant will automatically replace default: this with a newly generated keypair for better security. default: default: Inserting generated public key within guest... default: Removing insecure key from the guest if it\u0026#39;s present... default: Key inserted! Disconnecting and reconnecting using new SSH key... ==\u0026gt; default: Configuring and enabling network interfaces... ==\u0026gt; default: Rsyncing folder: /home/rramos/Development/Vagrant/ =\u0026gt; /vagrant ==\u0026gt; default: Check for insecure vagrant key: OK (not present) If you execute virt-manager you can see the virtual machine specification and adapt the Vagrant file to you needs.\nYou can then ssh to the VM using the command vagrant ssh. You should have a prompt like:\nWelcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-77-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage vagrant@ubuntu:~$ You can add the ssh key to your ssh configuration executing the following command. ( Check my previous {% post_link ssh-config [article] %} regarding this topic)\nvagrant ssh-config \u0026gt;\u0026gt; ~/.ssh/config then try the command ssh default . Default if the default name of the VM defined in the vagrant file.\nLet\u0026rsquo;s extend a bit # The next step is to set Ansible as our provisioning provider for the Vagrant Box. Add the following lines before the end statement in your Vagrantfile to set Ansible as the provisioning provider:\nconfig.vm.provision :ansible do |ansible| ansible.playbook = \u0026#34;playbook.yml\u0026#34; end Let\u0026rsquo;s create the playbook.yml file with the following content\n--- - hosts: all become: yes become_user: root tasks: - name: install apache2 apt: name=apache2 update_cache=yes state=latest handlers: - name: restart apache2 service: name=apache2 state=restarted This example playbook will install apache service on the all hosts given by Vagrantfile and start the service.\nThen we can run the command to just run the provision stuff\nvagrant provision Let\u0026rsquo;s add the following configuration on our Vagrant file\nconfig.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 80, host: 8080 And execute vagrant reload\nIf one access http://localhost:8080 we are now accessing the VM installed service.\nIf you recreate the VM it would run the ansible part at the end.\nSo it\u0026rsquo;s quite easy to recreate your environment.\nCluster Configuration # One could have Vagrant more complex configuration like the following for several VMs.\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026#34;echo Hello\u0026#34; config.vm.define \u0026#34;web\u0026#34; do |web| web.vm.box = \u0026#34;apache\u0026#34; end config.vm.define \u0026#34;db\u0026#34; do |db| db.vm.box = \u0026#34;mysql\u0026#34; end end Conclusion # In this article is described a simple way to integrate Vangrant with libvirt for quickly spinning VM\u0026rsquo;s and running ansible playbooks on them.\nThis could be useful for testing ansible playbooks before shipping them to LIVE environments or testing purposes.\nAlso to quickly configure a more complex infrastructure, where you don\u0026rsquo;t want to use dockers.\nOn the next article i will write a step-by-step guide to spin up a Mesos Cluster using this approach, so this article could be used as prep-guide.\nReferences # https://libvirt.org/ https://www.qemu.org/ https://www.vagrantup.com/ https://www.ansible.com/ https://github.com/vagrant-libvirt/vagrant-libvirt https://app.vagrantup.com/boxes/search?provider=libvirt https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/ https://www.vagrantup.com/docs/multi-machine/ ","date":"26 September 2017","externalUrl":null,"permalink":"/2017/09/quick-guide-for-vagrant-and-libvirt/","section":"Posts","summary":"","title":"Quick Guide for Vagrant and Libvirt","type":"posts"},{"content":"","date":"26 September 2017","externalUrl":null,"permalink":"/tags/vagrant/","section":"Tags","summary":"","title":"Vagrant","type":"tags"},{"content":"","date":"24 September 2017","externalUrl":null,"permalink":"/tags/ksql/","section":"Tags","summary":"","title":"KSQL","type":"tags"},{"content":" Excertpt # In this article i\u0026rsquo;ll explore Apache Kafka KSQL\nIntro # In this article i\u0026rsquo;ll explore Apache Kafka KSQL\nRequirements # Docker docker-compose Description # {% blockquote Official Website Definition%}\nKSQL is an open source streaming SQL engine that implements continuous, interactive queries against Apache Kafka™. It allows you to query, read, write, and process data in Apache Kafka in real-time, at scale using SQL commands. KSQL interacts directly with the Kafka Streams API, removing the requirement of building a Java app.\n{% endblockquote %}\nPoC Setup # Create the following file docker-compose.yml\n--- version: \u0026#39;2\u0026#39; services: zookeeper: image: \u0026#34;confluentinc/cp-zookeeper:latest\u0026#34; hostname: zookeeper ports: - \u0026#39;32181:32181\u0026#39; environment: ZOOKEEPER_CLIENT_PORT: 32181 ZOOKEEPER_TICK_TIME: 2000 extra_hosts: - \u0026#34;moby:127.0.0.1\u0026#34; kafka: image: \u0026#34;confluentinc/cp-enterprise-kafka:latest\u0026#34; hostname: kafka ports: - \u0026#39;9092:9092\u0026#39; - \u0026#39;29092:29092\u0026#39; depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092 KAFKA_AUTO_CREATE_TOPICS_ENABLE: \u0026#34;true\u0026#34; KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092 CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181 CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1 CONFLUENT_METRICS_ENABLE: \u0026#39;true\u0026#39; CONFLUENT_SUPPORT_CUSTOMER_ID: \u0026#39;anonymous\u0026#39; extra_hosts: - \u0026#34;moby:127.0.0.1\u0026#34; schema-registry: image: \u0026#34;confluentinc/cp-schema-registry:latest\u0026#34; hostname: schema-registry depends_on: - zookeeper - kafka ports: - \u0026#39;8081:8081\u0026#39; environment: SCHEMA_REGISTRY_HOST_NAME: schema-registry SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181 extra_hosts: - \u0026#34;moby:127.0.0.1\u0026#34; # Runs the Kafka KSQL data generator for topic called \u0026#34;pageviews\u0026#34; ksql-datagen-pageviews: image: \u0026#34;confluentinc/ksql-examples:latest\u0026#34; hostname: ksql-datagen-pageviews depends_on: - kafka - schema-registry # Note: The container\u0026#39;s `run` script will perform the same readiness checks # for Kafka and Confluent Schema Registry, but that\u0026#39;s ok because they complete fast. # The reason we check for readiness here is that we can insert a sleep time # for topic creation before we start the application. command: \u0026#34;bash -c \u0026#39;echo Waiting for Kafka to be ready... \u0026amp;\u0026amp; \\ cub kafka-ready -b kafka:29092 1 20 \u0026amp;\u0026amp; \\ echo Waiting for Confluent Schema Registry to be ready... \u0026amp;\u0026amp; \\ cub sr-ready schema-registry 8081 20 \u0026amp;\u0026amp; \\ echo Waiting a few seconds for topic creation to finish... \u0026amp;\u0026amp; \\ sleep 2 \u0026amp;\u0026amp; \\ java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar quickstart=pageviews format=delimited topic=pageviews bootstrap-server=kafka:29092 maxInterval=100 iterations=1000 \u0026amp;\u0026amp; \\ java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar quickstart=pageviews format=delimited topic=pageviews bootstrap-server=kafka:29092 maxInterval=1000\u0026#39;\u0026#34; environment: KSQL_CONFIG_DIR: \u0026#34;/etc/ksql\u0026#34; KSQL_LOG4J_OPTS: \u0026#34;-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\u0026#34; STREAMS_BOOTSTRAP_SERVERS: kafka:29092 STREAMS_SCHEMA_REGISTRY_HOST: schema-registry STREAMS_SCHEMA_REGISTRY_PORT: 8081 extra_hosts: - \u0026#34;moby:127.0.0.1\u0026#34; # Runs the Kafka KSQL data generator for topic called \u0026#34;users\u0026#34; ksql-datagen-users: image: \u0026#34;confluentinc/ksql-examples:latest\u0026#34; hostname: ksql-datagen-users depends_on: - kafka - schema-registry # Note: The container\u0026#39;s `run` script will perform the same readiness checks # for Kafka and Confluent Schema Registry, but that\u0026#39;s ok because they complete fast. # The reason we check for readiness here is that we can insert a sleep time # for topic creation before we start the application. command: \u0026#34;bash -c \u0026#39;echo Waiting for Kafka to be ready... \u0026amp;\u0026amp; \\ cub kafka-ready -b kafka:29092 1 20 \u0026amp;\u0026amp; \\ echo Waiting for Confluent Schema Registry to be ready... \u0026amp;\u0026amp; \\ cub sr-ready schema-registry 8081 20 \u0026amp;\u0026amp; \\ echo Waiting a few seconds for topic creation to finish... \u0026amp;\u0026amp; \\ sleep 2 \u0026amp;\u0026amp; \\ java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar quickstart=users format=json topic=users bootstrap-server=kafka:29092 maxInterval=100 iterations=1000 \u0026amp;\u0026amp; \\ java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar quickstart=users format=json topic=users bootstrap-server=kafka:29092 maxInterval=1000\u0026#39;\u0026#34; environment: KSQL_CONFIG_DIR: \u0026#34;/etc/ksql\u0026#34; KSQL_LOG4J_OPTS: \u0026#34;-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\u0026#34; STREAMS_BOOTSTRAP_SERVERS: kafka:29092 STREAMS_SCHEMA_REGISTRY_HOST: schema-registry STREAMS_SCHEMA_REGISTRY_PORT: 8081 extra_hosts: - \u0026#34;moby:127.0.0.1\u0026#34; # Runs the Kafka KSQL application ksql-cli: image: \u0026#34;confluentinc/ksql-cli:latest\u0026#34; hostname: ksql-cli depends_on: - kafka - schema-registry - ksql-datagen-pageviews - ksql-datagen-users command: \u0026#34;perl -e \u0026#39;while(1){ sleep 99999 }\u0026#39;\u0026#34; environment: KSQL_CONFIG_DIR: \u0026#34;/etc/ksql\u0026#34; KSQL_LOG4J_OPTS: \u0026#34;-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\u0026#34; STREAMS_BOOTSTRAP_SERVERS: kafka:29092 STREAMS_SCHEMA_REGISTRY_HOST: schema-registry STREAMS_SCHEMA_REGISTRY_PORT: 8081 extra_hosts: - \u0026#34;moby:127.0.0.1\u0026#34; Prepare the environment with the following command\nsudo docker-compose up -d This will create the minimal services to test ksql\nzookeeper schema-registry kafka ksql-cli It also will create 2 dockers which will generate data in order to test:\nksql-datagen-pageviews ksql-datagen-users Those dockers are kafka producers, which we will use to test KSQL.\nKSQL # Execute the CLI in order to test\nsudo docker-compose exec ksql-cli ksql-cli local --bootstrap-server kafka:29092 You will see something like this\n====================================== = _ __ _____ ____ _ = = | |/ // ____|/ __ \\| | = = | \u0026#39; /| (___ | | | | | = = | \u0026lt; \\___ \\| | | | | = = | . \\ ____) | |__| | |____ = = |_|\\_\\_____/ \\___\\_\\______| = = = = Streaming SQL Engine for Kafka = Copyright 2017 Confluent Inc. CLI v0.1, Server v0.1 located at http://localhost:9098 Having trouble? Type \u0026#39;help\u0026#39; (case-insensitive) for a rundown of how things work! ksql\u0026gt; Now let\u0026rsquo;s create a new STREAM\nCREATE STREAM pageviews_original (viewtime bigint, userid varchar, pageid varchar) WITH (kafka_topic=\u0026#39;pageviews\u0026#39;, value_format=\u0026#39;DELIMITED\u0026#39;); Let\u0026rsquo;s describe the created object\nksql\u0026gt; DESCRIBE pageviews_original; Field | Type ---------------------------- ROWTIME | BIGINT ROWKEY | VARCHAR(STRING) VIEWTIME | BIGINT USERID | VARCHAR(STRING) PAGEID | VARCHAR(STRING) Now lets create a users TABLE\nCREATE TABLE users_original (registertime bigint, gender varchar, regionid varchar, userid varchar) WITH (kafka_topic=\u0026#39;users\u0026#39;, value_format=\u0026#39;JSON\u0026#39;); And confirm the DDL\nksql\u0026gt; DESCRIBE users_original; Field | Type -------------------------------- ROWTIME | BIGINT ROWKEY | VARCHAR(STRING) REGISTERTIME | BIGINT GENDER | VARCHAR(STRING) REGIONID | VARCHAR(STRING) USERID | VARCHAR(STRING) So there are two main objects TABLES and STREAMS. To see the existing ones, execute:\nksql\u0026gt; SHOW STREAMS; Stream Name | Kafka Topic | Format ---------------------------------------------- PAGEVIEWS_ORIGINAL | pageviews | DELIMITED and\nksql\u0026gt; SHOW TABLES; Table Name | Kafka Topic | Format | Windowed -------------------------------------------------- USERS_ORIGINAL | users | JSON | false Differences between STREAMS and TABLES # Stream # A stream is an unbounded sequence of structured data (\u0026ldquo;facts\u0026rdquo;). For example, we could have a stream of financial transactions such as \u0026ldquo;Alice sent $100 to Bob, then Charlie sent $50 to Bob\u0026rdquo;. Facts in a stream are immutable, which means new facts can be inserted to a stream, but existing facts can never be updated or deleted. Streams can be created from a Kafka topic or derived from existing streams and tables.\nTable # A table is a view of a stream, or another table, and represents a collection of evolving facts. For example, we could have a table that contains the latest financial information such as \u0026ldquo;Bob’s current account balance is $150\u0026rdquo;. It is the equivalent of a traditional database table but enriched by streaming semantics such as windowing. Facts in a table are mutable, which means new facts can be inserted to the table, and existing facts can be updated or deleted. Tables can be created from a Kafka topic or derived from existing streams and tables.\nExtending the Model # Lets create a new pageviews stream with key pageid and joi it with table users.\nFirst create the new stream\nCREATE STREAM pageviews \\ (viewtime BIGINT, \\ userid VARCHAR, \\ pageid VARCHAR) \\ WITH (kafka_topic=\u0026#39;pageviews\u0026#39;, \\ value_format=\u0026#39;DELIMITED\u0026#39;, \\ key=\u0026#39;pageid\u0026#39;, \\ timestamp=\u0026#39;viewtime\u0026#39;); And users table\nCREATE TABLE users \\ (registertime BIGINT, \\ gender VARCHAR, \\ regionid VARCHAR, \\ userid VARCHAR, \\ interests array\u0026lt;VARCHAR\u0026gt;, \\ contact_info map\u0026lt;VARCHAR, VARCHAR\u0026gt;) \\ WITH (kafka_topic=\u0026#39;users\u0026#39;, \\ value_format=\u0026#39;JSON\u0026#39;); Aggregations # Let\u0026rsquo;s test some aggregations\nSELECT count(*),userid FROM pageviews GROUP BY userid; As data arrives in stream this is constantly updating , but we can verify that the count values begins from the start of the query, this could be useful to dump to another topic with transformed data.\nLet\u0026rsquo;s do that.\nCREATE TABLE user_counts AS select count(*),userid from pageviews group by userid; We can use the statement SHOW TOPICS which is useful and confirmed that the USER_COUNTS TOPICS is created.\nThe following aggregations are available\nFunction Example Description COUNT COUNT(col1) Count the number of rows MAX MAX(col1) Return the maximum value for a given column and window MIN MIN(col1) Return the minimum value for a given column and window SUM SUM(col1) Sums the column values Window # The WINDOW clause lets you control how to group input records that have the same key into so-called windows for operations such as aggregations or joins. Windows are tracked per record key. KSQL supports the following WINDOW types:\nTUMBLING HOPPING SESSION Window Tumbling # TUMBLING: Tumbling windows group input records into fixed-sized, non-overlapping windows based on the records\u0026rsquo; timestamps. You must specify the window size for tumbling windows. Note: Tumbling windows are a special case of hopping windows where the window size is equal to the advance interval.\nExample:\nSELECT item_id, SUM(quantity) FROM orders WINDOW TUMBLING (SIZE 20 SECONDS) GROUP BY item_id; Window HOPPING # HOPPING: Hopping windows group input records into fixed-sized, (possibly) overlapping windows based on the records\u0026rsquo; timestamps. You must specify the window size and the advance interval for hopping windows.\nExample:\nSELECT item_id, SUM(quantity) FROM orders WINDOW HOPPING (SIZE 20 SECONDS, ADVANCE BY 5 SECONDS) GROUP BY item_id; Window SESSION # SESSION: Session windows group input records into so-called sessions. You must specify the session inactivity gap parameter for session windows. For example, imagine you set the inactivity gap to 5 minutes. If, for a given record key such as \u0026ldquo;alice\u0026rdquo;, no new input data arrives for more than 5 minutes, then the current session for \u0026ldquo;alice\u0026rdquo; is closed, and any newly arriving data for \u0026ldquo;alice\u0026rdquo; in the future will mark the beginning of a new session.\nExample:\nSELECT item_id, SUM(quantity) FROM orders WINDOW SESSION (20 SECONDS) GROUP BY item_id; Transformations # Let\u0026rsquo;s create a new stream with a column transformation\nCREATE STREAM pageviews_transformed \\ WITH (timestamp=\u0026#39;viewtime\u0026#39;, \\ partitions=5, \\ value_format=\u0026#39;JSON\u0026#39;) AS \\ SELECT viewtime, \\ userid, \\ pageid, \\ TIMESTAMPTOSTRING(viewtime, \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;) AS timestring \\ FROM pageviews \\ PARTITION BY userid; Joining # And joining the STREAM with enriched data from TABLE\nCREATE STREAM pageviews_enriched AS \\ SELECT pv.viewtime, \\ pv.userid AS userid, \\ pv.pageid, \\ pv.timestring, \\ u.gender, \\ u.regionid, \\ u.interests, \\ u.contact_info \\ FROM pageviews_transformed pv \\ LEFT JOIN users u ON pv.userid = users.userid; Use Cases # Common KSQL use cases are:\nFraud detection - identify and act on out of the ordinary data to provide real-time awareness. Personalization - create real-time experiences and insight for end users driven by data. Notifications - build custom alerts and messages based on real-time data. Real-time Analytics - power real-time dashboards to understand what’s happening as it does. Sensor data and IoT - understand and deliver sensor data how and where it needs to be. Customer 360 - provide a clear, real-time understanding of your customers across every interaction. Streaming ETL # KSQL makes it simple to transform data within the pipeline, readying messages to cleanly land in another system\nAnomaly Detection # KSQL is a good fit for identifying patterns or anomalies on real-time data. By processing the stream as data arrives you can identify and properly surface out of the ordinary events with millisecond latency.\nMonitoring # Kafka\u0026rsquo;s ability to provide scalable ordered messages with stream processing make it a common solution for log data monitoring and alerting. KSQL lends a familiar syntax for tracking, understanding, and managing alerts.\nConclusion # On a first glance KSQL seems pretty easy to start using it. It\u0026rsquo;s still pretty new, i would give it a time to mature before start using it on production.\nFor small transformations, data-quality control and analytical queries that don\u0026rsquo;t take into account a large windows this seems a good solution. For more complex queries i still prefer to keep Kafka to it\u0026rsquo;s core business and let the computation work to Spark.\nMore Information # KSQL is available as a developer preview on Github Read the documentation for KSQL Join the conversation in the #ksql channel in the Confluent Community Slack. References # https://github.com/confluentinc/ksql https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis ","date":"24 September 2017","externalUrl":null,"permalink":"/2017/09/streaming-sql-for-kafka-ksql/","section":"Posts","summary":"","title":"Streaming SQL for Kafka (KSQL)","type":"posts"},{"content":"This article is about Portainer a docker UI manager solution\nIntro # Portainer is:\nPORTAINER IS AN OPEN-SOURCE LIGHTWEIGHT MANAGEMENT UI WHICH ALLOWS YOU TO EASILY MANAGE YOUR DOCKER HOSTS OR SWARM CLUSTERS\nIn order to manage the dockers where it is running one should pass the following option\nsudo docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer Snapshot # Setup # Create the following docker-compose.yml file\nversion: \u0026#39;2\u0026#39; services: portainer: restart: always ports: - \u0026#34;9000:9000\u0026#34; volumes: - ./data:/opt/data - /var/run/docker.sock:/var/run/docker.sock image: \u0026#34;portainer/portainer\u0026#34; and execute docker-compose up -d\nYou can then access the interface at: http://localhost:9000\nCheers, RR\n","date":"23 September 2017","externalUrl":null,"permalink":"/2017/09/portainer/","section":"Posts","summary":"","title":"Portainer","type":"posts"},{"content":"Installation guide for Sublime\nIntro # Text Editing, Done Right\nInstall # For me Sublime, this is one of the best smart editors in Linux. Here are the quick instructions to install for Ubuntu.\nwget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add - echo \u0026#34;deb https://download.sublimetext.com/ apt/stable/\u0026#34; | sudo tee /etc/apt/sources.list.d/sublime-text.list sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install sublime-text ","date":"23 September 2017","externalUrl":null,"permalink":"/2017/09/sublime-install-guide/","section":"Posts","summary":"","title":"Sublime Install Guide","type":"posts"},{"content":"","date":"19 September 2017","externalUrl":null,"permalink":"/tags/avro/","section":"Tags","summary":"","title":"Avro","type":"tags"},{"content":"This article describes one way to create tables in BigQuery From HDFS parquet Data.\nIntro # This article describes one way to create tables in BigQuery From HDFS parquet Data.\nRequirements # For this article assumes the following requisites are meet:\nYou have a Google Cloud Platform account You have created a Google Cloud Storage bucket You have HDFS configured You have Parquet data that you could materialize in Hive Must have bq util install Definitions # Hive Parquet Table: parquet_table HDFS Parquet Location: /user/hive/warehouse/test.db/parquet_table Hive Avro Table Name: avro_table HDFS Avro Location: /user/hive/warehouse/test.db/avro_table Process # Create a avro table from parquet data Copy avro files to GCS Create Bigquery Table from avro in gs bucket Setup # Create the Avro Table # Let\u0026rsquo;s start by creating the new table based on the existing parquet data\nSET hive.exec.compress.output=true; SET avro.output.codec=snappy; CREATE TABLE avro_table STORED AS AVRO AS (SELECT * FROM parquet_table); One could specify in the SELECT statement the columns we would like that could be obtain using parquet-tools command ex:\nparquet-tools meta \u0026lt;parquet_file\u0026gt; You would still need to get the parquet file to obtain that.\nWhy Avro File and that format # You could update data to BigQuery by streaming or from Google Cloud Storage as a batch process. A bulk import from HDFS seems logical to use a batch process so why avro ? According to the latest info on the google BigQuery Site it\u0026rsquo;s possible to:\nLoad from Google Cloud Storage, including CSV, JSON (newline-delimited), and Avro files, as well as Google Cloud Datastore backups. Load directly from a readable data source. Insert individual records using streaming inserts. Compressed Avro files are not supported, but compressed data blocks are. BigQuery supports the DEFLATE and Snappy codecs.\nAlso there is the following Avro mapping that could be useful\nAvro data type BigQuery data type null - ignored - boolean BOOLEAN int INTEGER long INTEGER float FLOAT double FLOAT bytes BYTES string STRING record RECORD enum STRING array - repeated fields - map RECORD union RECORD fixed BYTES Check the full spec on GCP Page\nThe other advantage of using avro is that BigQuery infers the schema so you don\u0026rsquo;t have to describe the columns of you table.\nCopy Avro file from HDFS to GCS # The best approach for this is to add the GCS connector to your HDFS config\nFollow the instructions in the following link or download the jar for Hadoop 2.x here\nAdd that jar on a valid location for you cluster HADOOP_CLASSPATH Generate a service account in the GCP console and get JSON key (follow this instructions) Copy that JSON file to a location in your cluster Add the following properties to your cluster core-site.xml \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.gs.project.id\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;your-project-name\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; Required. Google Cloud Project ID with access to configured GCS buckets. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;google.cloud.auth.service.account.json.keyfile\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/path/to/your/JSON-keyfile\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; The JSON key file of the service account used for GCS access when google.cloud.auth.service.account.enable is true. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.gs.working.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; The directory relative gs: uris resolve in inside of the default bucket. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; Extended options are available in gcs-core-default example\nCreate a new bucket on GCP and make sure you can access to with via hdfs command. hdfs dfs -ls gs://my-bucket-name/ If that works, you can now execute a distcp to sync the avro files directly to GCS.\nhdfs mkdir gs://my-bucket-name/my_table hdfs distcp /user/hive/warehouse/test.db/avro_table/* gs://my-bucket-name/my_table/ Load GCS avro data to a BigQuery table # Execute\nbq load ds.table gs://my-bucket-name/my_table/* --autodetect And that\u0026rsquo;s it you now have a table with data in BigQuery.\nIt is recommended to have this process managed by some type of orchestrator. There are several solutions for this. The next article i\u0026rsquo;ll be writing about one of those Airflow\nCheers, RR\nReferences # https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html https://github.com/apache/parquet-mr/tree/master/parquet-tools https://cloud.google.com/bigquery/loading-data https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md https://cloud.google.com/bigquery/docs/loading-data-cloud-storage ","date":"19 September 2017","externalUrl":null,"permalink":"/2017/09/create-avro-tables-for-google-bigquery/","section":"Posts","summary":"","title":"Create Avro Tables For Google BigQuery","type":"posts"},{"content":"","date":"19 September 2017","externalUrl":null,"permalink":"/tags/gcs/","section":"Tags","summary":"","title":"GCS","type":"tags"},{"content":"","date":"19 September 2017","externalUrl":null,"permalink":"/tags/parquet/","section":"Tags","summary":"","title":"Parquet","type":"tags"},{"content":"In this article i describe a quick way to have zeepelin running locally\nIntro # In this article i describe a quick way to have zeepelin running so that you could quickly testing some Spark application.\nNOTE: This procedure shouldn\u0026rsquo;t be used in production environments has you should setup the Notebook with auth and connected to your local infrastructure.\nRequirements # One should have a docker environment setup. Check my previous {% post_link dockerclean article %} if you need some help with that Docker-compose Setup # Create a folder named zeepelin mkdir docker-zeepelin Create a data where you could put some data to analyse. mkdir -p docker-zeepelin/data Create the following docker-compose.yml file in dir docker-zeepelin : version: \u0026#39;2\u0026#39; services: zeppelin: ports: - \u0026#34;8080:8080\u0026#34; volumes: - ./data:/opt/data image: \u0026#34;dylanmei/zeppelin\u0026#34; Launch docker-compose sudo docker-compose up -d That\u0026rsquo;s it you should now be able to access http://localhost:8080 Test it # Lets download a demo file to our data dir. curl -s https://api.opendota.com/api/publicMatches -o ./data/OpenDotaPublic.json Yeah! I kinda like Dota2 so this makes sense :D\nCreate a new NoteBook in the web Interface and use the following code %spark val df = sqlContext.read.json(\u0026#34;file:///opt/data/OpenDotaPublic.json\u0026#34;) df.show Hit: Shift-Enter\nLet\u0026rsquo;s register this dataframe as temp table and create some visuals %spark df.registerTempTable(\u0026#34;publicmatches\u0026#34;) Create the following to generate visualizations %sql select radiant_win,match_id from publicmatches Guess i need to start playing on Radiant side :D\nWell and that\u0026rsquo;s it.\nCheers, RR\nReferences # https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/ https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04 https://docs.docker.com/compose https://zeppelin.apache.org/ ","date":"19 September 2017","externalUrl":null,"permalink":"/2017/09/quick-setup-zeppelin-notebook/","section":"Posts","summary":"","title":"Quick Setup Zeppelin Notebook","type":"posts"},{"content":"","date":"19 September 2017","externalUrl":null,"permalink":"/tags/zeepelin/","section":"Tags","summary":"","title":"Zeepelin","type":"tags"},{"content":"","date":"19 September 2017","externalUrl":null,"permalink":"/tags/btrfs/","section":"Tags","summary":"","title":"BTRFS","type":"tags"},{"content":"Article about docker using btrfs filesystem\nIntro # So dockers tend to start growing like weeds. And cost me space i don\u0026rsquo;t have. But one could change the storage driver. You could choose LVM and take advantage of the snapshots or btrfs which i already have.\nBtrfs # Btrfs is a next generation copy-on-write filesystem that supports many advanced storage technologies that make it a good fit for Docker. Btrfs is included in the mainline Linux kernel.\nDocker’s btrfs storage driver leverages many Btrfs features for image and container management. Among these features are block-level operations, thin provisioning, copy-on-write snapshots, and ease of administration. You can easily combine multiple physical block devices into a single Btrfs filesystem.\nRequirements # Install btrfs Make sure you have a volume formatted as btrfs (not part of the article) Clean all dockers and images # # Delete all containers sudo docker rm $(sudo docker ps -a -q) # Delete all images sudo docker rmi $(sudo docker images -q) Setup # Make sure you have btrfs on your system\nsudo cat /proc/filesystems | grep btrfs Stop docker sudo service docker stop Create a backup of you docker settings sudo mv /var/lib/docker/ /var/lib/docker.bak create a subvolume from an existing btrfs FS btrfs subvolume create /archive/dockers NOTE: It\u0026rsquo;s better to use a dedicate disk for it\nCreate the symlink ln -s /archive/dockers /var/lib/docker Copy the backup data to the new location cp -rp /var/lib/docker.bak/* /var/lib/docker/ Configure Docker to use the btrfs storage driver. This is required even though /var/lib/docker/ is now using a Btrfs filesystem. Edit or create the file /etc/docker/daemon.json. { \u0026#34;storage-driver\u0026#34;: \u0026#34;btrfs\u0026#34; } Start docker service sudo service docker start Check if docker is running with docker support sudo docker info|grep \u0026#34;Storage Driver\u0026#34; You could check the volumes being created with the command\nsudo btrfs subvolume list /var/liv/docker |grep dockers References # https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/ ","date":"19 September 2017","externalUrl":null,"permalink":"/2017/09/clean-all-dockers-and-configure-with-btrfs/","section":"Posts","summary":"","title":"Clean all Dockers and configure with btrfs","type":"posts"},{"content":"This article is about optimization of SSH connections with your .ssh/config file\nIntro # If most of your work is done remotely though ssh, and you have to access several environments, there will come a time where you need to organize our connection settings. Which user you need to access server X or which port is configured or the case you work in consulting and have several ssh_keys.\nThere are some tools that might help on this, but i\u0026rsquo;m old school and still stick with plain ssh command.\nssh config file helps quite a lot, here are some tips unknown to some:\nAlias # Let\u0026rsquo;s say you want to take advantage of tab auto-completion when using your connections for a environment like\n├── Client_A │ ├── server1 │ └── server2 ├── Client_B │ └── server1 ├── DEV │ ├── BackOffice │ │ ├── server1 │ │ └── server2 │ └── FrontOffice │ ├── server1 │ ├── server2 │ └── server3 ├── LIVE │ ├── Cluster │ │ ├── node01 │ │ ├── node02 │ │ └── node03 │ └── Servers │ ├── server01 │ └── server02 └── QA └── server1 this would be quicker to do something like ssh DEV TAB Back TAB server1\nthat\u0026rsquo;s actually possible with ssh_config alias.\nAdd the following to ~/.ssh/config to see this in action\nHost Client_A.server1 Hostname localhost Host Client_A.server2 Hostname localhost Host Client_B.server1 Hostname localhost Host Client_A.server1 Hostname localhost Host DEV.BackOffice.server1 Hostname localhost Host DEV.BackOffice.server2 Hostname localhost Host DEV.FrontOffice.server1 Hostname localhost Host DEV.FrontOffice.server2 Hostname localhost Host DEV.FrontOffice.server3 Hostname localhost Host LIVE.Cluster.node01 Hostname localhost Host LIVE.Cluster.node02 Hostname localhost Host LIVE.Cluster.node03 Hostname localhost Host LIVE.Servers.server01 Hostname localhost Host LIVE.Servers.server02 Hostname localhost Host QA.server01 Hostname localhost Now try out the power of Tab-autocompletion. this is just an example of a type of structure you could use.\nYou could also add alias like\nHost LIVE.Servers.server02 server01.mydomain.com Hostname localhost So that both ssh attempts to LIVE.Servers.server02 and server01.mydomain.com would use the same configuration.\nAccess customizations # No let\u0026rsquo;s say for accessing LIVE.Servers.server01 you require account admin and ssh listens on port 2228 . one could setup the following\nHost LIVE.Servers.server01 Hostname localhost Port 2228 User admin With this configuration one could simple execute ssh LIVE.Servers.server01 and it will use the configured user and port in the connection.\nOr if you have a specific ssh_key for it in QA\nHost QA.server01 Hostname localhost Port 2227 User qa_user01 IdentityFile ~/.ssh/id_rsa_qa Tunnels # one could also setup tunnels directly in ssh_config like\nHost tunnel HostName database.example.com IdentityFile ~/.ssh/id_rsa_dev LocalForward 9906 127.0.0.1:3306 User dba You can simple execute ssh -f -N tunnel\nOr if you have access to server3 only from server1\nHost DEV.FrontOffice.server3 Hostname 10.0.0.1 Port 22 User admin IdentityFile ~/.ssh/id_rsa_dev ProxyCommand ssh -q -W %h:%p DEV.FrontOffice.server1 One configuration i normally use in development with containers or virtual machines which are deprovision with regularity is the following:\nHost 192.168.77.* mesos-* StrictHostKeyChecking no User rramos IdentityFile ~/.ssh/id_rsa UserKnownHostsFile=/dev/null LogLevel QUIET This means any ssh connection to local network 192.168.77.* or hosts with name mesos-* won\u0026rsquo;t get registered in KnownHosts.\nYou could also use this to change your settings for TCPKeepAlive or any other specific connections settings you may need the man page as the full list of options.\nCheers, RR\nReferences # https://linux.die.net/man/5/ssh_config ","date":"13 September 2017","externalUrl":null,"permalink":"/2017/09/optimize-your-ssh-connections-with-ssh-config-file/","section":"Posts","summary":"","title":"Optimize your SSH Connections with SSH config File","type":"posts"},{"content":"","date":"13 September 2017","externalUrl":null,"permalink":"/tags/internet/","section":"Tags","summary":"","title":"Internet","type":"tags"},{"content":"","date":"13 September 2017","externalUrl":null,"permalink":"/tags/proxy/","section":"Tags","summary":"","title":"Proxy","type":"tags"},{"content":"Article about setting proxy environments with Chrome\nIntro # Something that keeps bugging me is the need to access different environments in the browser through proxy tunnels, due to security restrictions or lack of routing between your local network and the environment you are working. Very common on Cloud environments or if you are working has a consultant for several clients at the same time.\nSo this guide is intended to help on the setup of Google Chrome to use your configured tunnels.\nRequirements # Configure your local tunnel # You require a tunnel configuration setup. For Linux i recommend setting this up in your .ssh/config file. I\u0026rsquo;ll give some examples, if you have interest on expanding you ./ssh/config check the next article ill go in depth with it. There are also other tools such as gSTM tool.\nSo let\u0026rsquo;s begin to create our reverse tunnel\nssh -R 19999:localhost:22 sourceuser@remoteip You could use any other available localport\nInstall plugin in Google Chrome # Go into your google chrome settings and search for Extensions . I\u0026rsquo;m using Proxy SwitchySharp But you could use any other.\nGo to your Extension Options and let\u0026rsquo;s create a profile\nName: mylocaldomain.dev SOCKS HOST: localhost Port: 19999 SOCKS v5: (enable) Now let\u0026rsquo;s create a rule for our local domain in Switch Rules\nAdd the rule for our domain:\nRule Name: MyLocalDomain URL Pattern: *.mylocaldomain.dev Pattern Type: Wildcard ProxyProfile: mylocaldomain.dev NOTE: Don\u0026rsquo;t forget to active the option Enable Switch Rules\nAlso don\u0026rsquo;t forget to activate the Auto Switch Mode in the extension.\nAnd that\u0026rsquo;s it now when you access the browser to domain http://something.mylocaldomain.dev you be using your defined ssh tunnel for it.\nTest # Lets do a quick test :)\nsudo sh -c \u0026#39;echo \u0026#34;127.0.0.1 test.mylocaldomain.dev\u0026#34; \u0026gt;\u0026gt; /etc/hosts\u0026#39; ssh -R 19999:localhost:22 localhost python -m SimpleHTTPServer 8000 Now access on the browser to http://test.mylocaldomain.dev:8000\nYou could now have different profiles configured and autoswitch will do the job for you of choosing the appropriate tunnel.\nCheers, RR\n","date":"13 September 2017","externalUrl":null,"permalink":"/2017/09/setting-up-proxy-environments-with-chrome/","section":"Posts","summary":"","title":"Setting up Proxy Environments with Chrome","type":"posts"},{"content":"This article is about Polybase and its configuration with Cloudera 5\nIntro # In this article i\u0026rsquo;ll try to describe the required configuration steps to setup Polybase in SQLServer 2016 for Cloudera CDH5.\nBefore starting the the configuration steps, let\u0026rsquo;s just try to understand why this is being done.\nSqoop # Sqoop is one of the most used tools to transfer data from the Relational World to BigData infrastructures. It relies on JDBC connectors to transfer data between SQLServer and HDFS.\nPerformance # The transfer process to SQLServer via sqoop is taking quite a lot, so the objective of this PoC is to verify if PolyBase alternative for dumping data in/out the cluster and understand if there is a improvement on existing processes.\nSetup Process # Obtain the cluster configuration files # In order to configure Polybase for you Cloudera cluster one should first gather from CM the client configurations for HDFS and YARN.\nAfter downloading the configs you need to update the following files:\nyarn-site.xml mapred-site.xml hdfs-site.xml Copy this files to your SQLServer instance where Polybase will be installed.\nThe usual path is C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\Binn\\Polybase\\Hadoop\\conf\nNote: Please note, that when PolyBase authenticates to a Kerberos secured cluster, we require the hadoop.rpc.protection setting to be set to authentication. This will leave the data communication between Hadoop nodes unencrypted.\nActivate the required Feature # On the SQLServer you are going to activate Polybase make sure you have the required pre-requisites\n64-bit SQL Server Evaluation edition Microsoft .NET Framework 4.5. Oracle Java SE RunTime Environment (JRE) version 7.51 or higher (64-bit) Minimum memory: 4GB Minimum hard disk space: 2GB TCP/IP must be enabled for Polybase to function correctly. Follow Microsoft guide to activate the feature PolyBase Install Guide\nOne could test if Polybase is correctly installed by running the following command\n{% codeblock lang:SQL %} SELECT SERVERPROPERTY (\u0026lsquo;IsPolybaseInstalled\u0026rsquo;) AS IsPolybaseInstalled;\n{% endcodeblock %}\nConfigure External data source # Execute the following T-SQL to create Hadoop connectivity to CDH5\n{% codeblock lang:SQL %} \u0026ndash; Values map to various external data sources.\n\u0026ndash; Option 6: Cloudera 5.1, 5.2, 5.3, 5.4, 5.5, 5.9, 5.10, 5.11, and 5.12 on Linux sp_configure @configname = \u0026lsquo;hadoop connectivity\u0026rsquo;, @configvalue = 6; GO\nRECONFIGURE GO\n{% endcodeblock %}\nYou could change the option in case you use a different Hadoop Cluster check the Option Mapping\nNote: After running RECONFIGURE, you must stop and restart the SQL Server service.\nCreate the T-SQL objects # Follow the example configuration described in Getting Started with Polybase\n-- 1: Create a database scoped credential. -- Create a master key on the database. This is required to encrypt the credential secret. CREATE MASTER KEY ENCRYPTION BY PASSWORD = \u0026#39;S0me!nfo\u0026#39;; -- 2: Create a database scoped credential for Kerberos-secured Hadoop clusters. -- IDENTITY: the Kerberos user name. -- SECRET: the Kerberos password CREATE DATABASE SCOPED CREDENTIAL HadoopUser1 WITH IDENTITY = \u0026#39;\u0026lt;hadoop_user_name\u0026gt;\u0026#39;, Secret = \u0026#39;\u0026lt;hadoop_password\u0026gt;\u0026#39;; -- 3: Create an external data source. -- LOCATION (Required) : Hadoop Name Node IP address and port. -- RESOURCE MANAGER LOCATION (Optional): Hadoop Resource Manager location to enable pushdown computation. -- CREDENTIAL (Optional): the database scoped credential, created above. CREATE EXTERNAL DATA SOURCE MyHadoopCluster WITH ( TYPE = HADOOP, LOCATION =\u0026#39;hdfs://10.xxx.xx.xxx:xxxx\u0026#39;, RESOURCE_MANAGER_LOCATION = \u0026#39;10.xxx.xx.xxx:xxxx\u0026#39;, CREDENTIAL = HadoopUser1 ); -- 4: Create an external file format. -- FORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT, RCFILE, ORC, PARQUET). CREATE EXTERNAL FILE FORMAT TextFileFormat WITH ( FORMAT_TYPE = DELIMITEDTEXT, FORMAT_OPTIONS (FIELD_TERMINATOR =\u0026#39;|\u0026#39;, USE_TYPE_DEFAULT = TRUE) -- 5: Create an external table pointing to data stored in Hadoop. -- LOCATION: path to file or directory that contains the data (relative to HDFS root). CREATE EXTERNAL TABLE [dbo].[CarSensor_Data] ( [SensorKey] int NOT NULL, [CustomerKey] int NOT NULL, [GeographyKey] int NULL, [Speed] float NOT NULL, [YearMeasured] int NOT NULL ) WITH (LOCATION=\u0026#39;/Demo/\u0026#39;, DATA_SOURCE = MyHadoopCluster, FILE_FORMAT = TextFileFormat ); -- 6: Create statistics on an external table. CREATE STATISTICS StatsForSensors on CarSensor_Data(CustomerKey, Speed) Example Queries # Import external Data -- PolyBase Scenario 2: Import external data into SQL Server. -- Import data for fast drivers into SQL Server to do more in-depth analysis and -- leverage Columnstore technology. SELECT DISTINCT Insured_Customers.FirstName, Insured_Customers.LastName, Insured_Customers.YearlyIncome, Insured_Customers.MaritalStatus INTO Fast_Customers from Insured_Customers INNER JOIN ( SELECT * FROM CarSensor_Data where Speed \u0026gt; 35 ) AS SensorD ON Insured_Customers.CustomerKey = SensorD.CustomerKey ORDER BY YearlyIncome CREATE CLUSTERED COLUMNSTORE INDEX CCI_FastCustomers ON Fast_Customers; Export External Data -- PolyBase Scenario 3: Export data from SQL Server to Hadoop. -- Enable INSERT into external table sp_configure ‘allow polybase export’, 1; reconfigure -- Create an external table. CREATE EXTERNAL TABLE [dbo].[FastCustomers2009] ( [FirstName] char(25) NOT NULL, [LastName] char(25) NOT NULL, [YearlyIncome] float NULL, [MaritalStatus] char(1) NOT NULL ) WITH ( LOCATION=\u0026#39;/old_data/2009/customerdata\u0026#39;, DATA_SOURCE = HadoopHDP2, FILE_FORMAT = TextFileFormat, REJECT_TYPE = VALUE, REJECT_VALUE = 0 ); -- Export data: Move old data to Hadoop while keeping it query-able via an external table. INSERT INTO dbo.FastCustomer2009 SELECT T.* FROM Insured_Customers T1 JOIN CarSensor_Data T2 ON (T1.CustomerKey = T2.CustomerKey) WHERE T2.YearMeasured = 2009 and T2.Speed \u0026gt; 40; Tests # Initial tests are quite good actually, even with the identified issues. Polybase seems quite limited but for the objective in hands might present like a very viable solution.\nSome more tests would be required.\nIssues # It seems one cannot truncate external tables so an extra process would be required if you plan to use this as part of an ETL process that should support re-runs It seems that hadoop_user_name is being ignored and polybase still uses pwc_user account in cluster. Take care on the compression levels you choose as they consume quite a lot CPU on your SQLServer The metadata of the tables is always stored on SQLServer. And when you choose parquet files has source format it stores in parquet meta the columns as col-0,col-1,col-3,... if you map those files to a Hive table would require a view with the respective column name mapping. Not sure if this can be change but the dumped files to HDFS are split in 8, for the initial tests, only bad for small tables. Conclusion # Work in progress References # https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation ","date":"9 September 2017","externalUrl":null,"permalink":"/2017/09/polybase-configuration-with-cloudera-5/","section":"Posts","summary":"","title":"Polybase Configuration with Cloudera 5","type":"posts"},{"content":"","date":"9 September 2017","externalUrl":null,"permalink":"/tags/sqlserver/","section":"Tags","summary":"","title":"SQLServer","type":"tags"},{"content":"This article goes through Alluxio a distributed storage layer\nIntro # Something that i have in mind for some time is to test Alluxio as a distributed storage layer for Spark.\nIn this article i\u0026rsquo;ll describe my PoC on this.\nObjective # Test Alluxio as a Storage layer for Spark Create a docker environment that could be quick to setup Document the procedure and test the solution Infrastructure # Create several dockers to run the tests\n1 dockers for HDFS (TBD) 3 dockers for aluxio cluster (TBD) Pre-Setup # Let\u0026rsquo;s start by creating a docker with CDH which will simulate our HDFS Infrastructure.\nWe could use the provide docker image from Cloudera\ndocker pull cloudera/quickstart:latest Let\u0026rsquo;s create the image with docker compose\nTODO: \u0026hellip;\nTo fireup a Cloudera Quickstart Container\ndocker run --hostname=quickstart.cloudera --privileged=true -t -i cloudera/quickstart /usr/bin/docker-quickstart -d Setup # Setup Alluxio # TODO\nSetup HDFS # TODO\nSetup Spark # TODO\nTests # WIP\nReferences # https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html http://www.alluxio.org/ http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html ","date":"7 September 2017","externalUrl":null,"permalink":"/drafts/alluxio/","section":"Drafts","summary":"","title":"Alluxio PoC","type":"drafts"},{"content":"","date":"7 September 2017","externalUrl":null,"permalink":"/tags/data-backend/","section":"Tags","summary":"","title":"Data Backend","type":"tags"},{"content":"","date":"7 September 2017","externalUrl":null,"permalink":"/tags/distributed-system/","section":"Tags","summary":"","title":"Distributed System","type":"tags"},{"content":"","date":"7 September 2017","externalUrl":null,"permalink":"/tags/dimensions/","section":"Tags","summary":"","title":"Dimensions","type":"tags"},{"content":"This article explains how to create GA custom dimensions\nIntro # In order to add a custom dimension in Google Analytics follow this steps:\nClick admin and navigate to the property you which to add a custom dimension Click new Custom Dimension\nGive it a Name and a Scope and you\u0026rsquo;l get the javascript to add on your website\nAfter you should modify your tracking code adding for example ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;, { \u0026#39;dimension1\u0026#39;: \u0026#39;My Custom Dimension\u0026#39; }); When we should use this # Let\u0026rsquo;s say your website does some sort of user classification, you can create a CustomDimension ex: UserCategory and send enrich the data you track.\n","date":"7 September 2017","externalUrl":null,"permalink":"/2017/09/how-to-create-a-google-analytics-custom-dimension/","section":"Posts","summary":"","title":"How to Create a Google Analytics Custom Dimension","type":"posts"},{"content":"This article is about how to Unpivot tables in Hive to avoid multiple joins\nIntro # In Hive, when a table has multiple similar columns, such as:\nproduct_id image_name1 image_name2 \u0026hellip; image_name15 1 a.jpg b.jpg \u0026hellip; c.jpg 2 d.jpg e.jpg \u0026hellip; f.jpg and we want to join based on all image_names, the naive approach would be to perform 15 joins (in the running example).\nHive does not have an unpivot functionality, but it is possible to use the Hive builtin UDF explode to accomplish this: Query example\nselect x.product_id, x.image_name from ( select product_id, map(\u0026#34;image_name1\u0026#34;, image_name1, ..., \u0026#34;image_name15\u0026#34;, image_name15) as image_map from table ) x lateral view explode(image_map) expl as image_nr, image_name This returns an unpivoted table like below, which allows us to perform a single join:\nproduct_id image_name 1 a.jpg 1 b.jpg 1 c.jpg 2 d.jpg 2 e.jpg 2 f.jpg Big thanks to Diogo Franco, for this hint ;) check also is Blog Page diogoalexandrefranco.github.io\n","date":"7 September 2017","externalUrl":null,"permalink":"/2017/09/hive-unpivot-to-avoid-multiple-joins/","section":"Posts","summary":"","title":"Hive - Unpivot to avoid multiple joins","type":"posts"},{"content":"","date":"7 September 2017","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"This article is about mdv a Terminal Markdown Viewer and MarkDown SpellChecker\nIntro # It\u0026rsquo;s been a while that i\u0026rsquo;ve updated this posts. So let me start for something i\u0026rsquo;ve found out the other day.\nTerminal Markdown Viewer # I though it would be cool to have some tool that could parse me Markdown files and present them nicely formatted, like cat command.\nAnd it seems i\u0026rsquo;m not the only one :)\nhttps://github.com/axiros/terminal_markdown_viewer\nIn order to install just follow the instruction on the site. Pip worked well for me:\npip install mdv I\u0026rsquo;ve updated my PATH environment var on .bashrc with:\nexport PYTHONPATH=\u0026#34;${PYTHONPATH}:/${HOME}/.local/lib/python2.7/site-packages\u0026#34; export PATH=\u0026#34;$PATH:~/.local/bin\u0026#34; I can now execute mdv command try it out it some .md files :D\nA Markdown spell checking tool # The other thing is regarding spell-check, as you can probably notice i really do a lot of errors, but i don\u0026rsquo;t want to use a high resource editor just because of this how about a MD spell checking tool ?\nWell i\u0026rsquo;m trying this one ad i\u0026rsquo;m using hexo for the page generation and it uses already npm modules just added a new one\ninstall # npm i markdown-spellcheck -g One can run interactive like\nmdspell \u0026#34;**/*.md\u0026#34; It\u0026rsquo;s a bit slow but the database would grow in terms, i\u0026rsquo;m giving a try to it.\nI also add to add .spelling no to keep the dictionary database on git. I probably would add it later.\nCheers, RR\n","date":"7 September 2017","externalUrl":null,"permalink":"/2017/09/terminal-markdown-viewer-and-markdown-spellchecker/","section":"Posts","summary":"","title":"Terminal Markdown Viewer and MarkDown SpellChecker","type":"posts"},{"content":"","date":"3 November 2016","externalUrl":null,"permalink":"/tags/conferences/","section":"Tags","summary":"","title":"Conferences","type":"tags"},{"content":"This article provides an overview of topics from the Spark Summit 2016\nIntro # So i\u0026rsquo;ve attended 25-27 OCT in Brussels to SparkSummit 2016 in Europe. So it kind makes sense to write down some impressions regarding the event.\nThe Event it self was pretty well organized, and the distribution of spaces was well planned.\nRegarding Spark it-self, is one of the ecosystem players (https://hadoopecosystemtable.github.io/) that\u0026rsquo;s been getting more traction from the industry, partially because of the Enterprise support from Databricks and other major BigData contributors.\nThis Summit, had huge increase of participants, compared to last year, what guaranties that this is really here to stay.\nI always try to make of this conferences with two key points:\nWhat\u0026rsquo;s new ? What are the other Companies doing, that\u0026rsquo;s working for them ? What new # AMPLab, the creators of Spark from UC Berkeley presented some interesting projects for the future that would increase SparkEngine to a new level. They presented some comparison charts with execution times of this new engine compared with project Flink which uses real streaming and not micro-batching, which may come as a very good news, because there are more and more Streaming applications being implemented. Also some new layer of secure, the projects are called Drizzle and Opaque, let\u0026rsquo;s hope they get this ready soon :)\nAlso one of the big announcements, was the new TensorFrames object. this output come\u0026rsquo;s from a join work of TensorFlow and Databricks in order to bring DeepLearning to Spark. What this means is that in the Future one could simple use the Spark Framework in order to take advantage of GPU\u0026rsquo;s, also that comes into alignment as you could expect with the new feature in Databricks Notebooks to support this.\nIt is still in Beta, but if you are using both TensorFlow and Spark, this is obviously good news.\nIBM is also entering the race with a SaaS solution (https://console.ng.bluemix.net/catalog/services/apache-spark) similar to Databricks, in terms of Notebook offering but it runs on their own infrastructure, which could be good and have good stability and all, but when dealing with bigdata and moving all that data from provider to provider, you better to the maths.\nFor that matter still prefer the Databricks model, and they have good news that until the end of the year they are planning to support a new CloudProvider.\nUse Cases # There where lot\u0026rsquo;s of use cases, in the several industries, even in the Milk Industry that\u0026rsquo;s right :)\nBut looking on the overall presented cases, is clear that more and more companies are adopting Streaming solutions for they business, as Jacek Laskowski referred in his presentation. (Really recommend that one, a really appealing one)\nKafka is becoming a key-role figure in the several presented architectures.\nAnd it seems HDFS is becoming mostly used as a DataLake only solution, and starting using more high-speed memory layer as DataGrid, Alluxio solutions.\nThey also presented some comparisons regarding direct access to Cloud Blob Storage compared with traditional FS Steve Loughran. Interesting values presented regarding Microsoft Cloud Solution vs AWS.\nOn other aspect that call my attention, was the lack of monitoring solutions for Spark workloads, and there where in fact several presentations regarding this subject, mostly use cases of in-house developments that fill this gap. Simon Whitear was one of the best in my opinion.\nLambda,Gamma and Omega. With all these Streaming high-speed architectures showing on, one has to play it safe right. That\u0026rsquo;s also one of the tendencies for the Industry to have these types of architectures with a speed layer and slow-layer. William Benton made a very enjoyable presentation regarding this topic.\nSlides and Videos # It seems they updated the site and the videos and slides seem available, which is good news.\nCheck them if you like on the site Schedule page:\nhttps://spark-summit.org/eu-2016/schedule/\nCheers and keep Sparking\n","date":"3 November 2016","externalUrl":null,"permalink":"/2016/11/spark-summit-2016/","section":"Posts","summary":"","title":"Spark Summit 2016","type":"posts"},{"content":"Article about GC Issue when Hive crashes trying to insert data into a table\nIntro # When running some queries with hive sometimes we get a very nice java exception of overhead limit\nException in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: GC overhead limit exceeded I didn\u0026rsquo;t know but it turns out you can supply directly on the jdbc connection of beeline the parameters to increase this values.\njdbc:hive2://localhost:10000/default?mapreduce.map.memory.mb=3809;mapreduce.map.java.opts=-Xmx3428m;mapreduce.reduce.memory.mb=2560;mapreduce.reduce.java.opts=-Xmx2304m; It is important to understand the size of the containers in your cluster and this is useful for some adhoc procedures.\nThe best way is to configure the containers memory size in hadoop, but this kind of quick solutions are useful, especially for testing parameters of new workflows.\nThere is some good info regarding this configuration in altiscale documentation:\nhttps://documentation.altiscale.com/heapsize-for-mappers-and-reducers\nCheers, RR\n","date":"19 October 2016","externalUrl":null,"permalink":"/2016/10/hive-queries-crash-when-inserting-gc-exception/","section":"Posts","summary":"","title":"hive Queries crash when inserting GC Exception","type":"posts"},{"content":"","date":"23 September 2016","externalUrl":null,"permalink":"/tags/events/","section":"Tags","summary":"","title":"Events","type":"tags"},{"content":"","date":"23 September 2016","externalUrl":null,"permalink":"/tags/meetups/","section":"Tags","summary":"","title":"Meetups","type":"tags"},{"content":"Article about Porto Tech HUB\nIntro # I\u0026rsquo;ve attend today to the second edition of Porto Tech HUB 2016. It was good to see old friends and the raising tech potential in the City.\nThere where some good presentations, good organization and the location was appealing also.\nI especially liked Christoffer Noring presentation about NativeScript and Angular and Miguel Veiga about Storm and Kafka.\nThis last one have more to do with my actual work.\nhttp://www.portotechhub.com https://twitter.com/PortoTechHub ","date":"23 September 2016","externalUrl":null,"permalink":"/2016/09/porto-tech-hub-2016/","section":"Posts","summary":"","title":"Porto Tech HUB 2016","type":"posts"},{"content":"Git sheet sheet commands\nIntro # As I\u0026rsquo;m always forgetting how to reset my local changes from origin HEAD. This cheat sheet might help.\nCreate # Clone an existing repository\ngit clone ssh://user@domain.com/repo.git Create a new local repository\ngit init Local Changes # Changed files in your working directory\ngit status Changes to tracked files\ngit diff Add all current changes to the next commit\ngit add . Add some changes in \u0026lt;file\u0026gt; to the next commit\ngit add -p \u0026lt;file\u0026gt; Commit all local changes in tracked files\ngit commit -a Commit previously staged changes\ngit commit Change the last commit (Don\u0026rsquo;t amend publish commits)\ngit commit --amend Branches \u0026amp; Tags # List all existing branches\ngit branch -av Switch HEAD branch\ngit checkout \u0026lt;branch\u0026gt; Create new branch based on your current HEAD\ngit branch \u0026lt;new-branch\u0026gt; Create new tracking branch based on a remote branch\ngit checkout --track \u0026lt;remote/branch\u0026gt; Delete a local branch\ngit branch -d \u0026lt;branch\u0026gt; Mark the current commit with a tag\ngit tag \u0026lt;tag-name\u0026gt; Update \u0026amp; Publish # List all currently configured remotes\ngit remote -v Show information about a remote\ngit remote show \u0026lt;remote\u0026gt; Add new remote repository, named \u0026lt;remote\u0026gt;\ngit remote add \u0026lt;shortname\u0026gt; \u0026lt;url\u0026gt; Download all changes from \u0026lt;remote\u0026gt;, but don\u0026rsquo;t integrate into HEAD\ngit fetch \u0026lt;remote\u0026gt; Download changes and directly merge/integrate into HEAD\ngit pull \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; Publish local changes on a remote\ngit push \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; Delete a branch on the remote\ngit branch -dr \u0026lt;remote/branch\u0026gt; Publish your tags\ngit push --tags Merge \u0026amp; Rebase # Merge \u0026lt;branch\u0026gt; into your current HEAD, without fast-forward\ngit merge --no-ff \u0026lt;branch\u0026gt; Rebase your current HEAD onto \u0026lt;branch\u0026gt;\ngit rebase \u0026lt;branch\u0026gt; Abort a rebase\ngit rebase --abort Continue a rebase after resolving conflicts\ngit rebase --continue Use your configured merge tool to solve conflicts\ngit mergetool Use your editor to manually solve conflicts and (after resolving) mark file as resolved\ngit add \u0026lt;resolved-file\u0026gt; git rm \u0026lt;resolved-file\u0026gt; Undo # Discard all local changes in your working directory\ngit reset --hard HEAD Discard local changes in a specific file\ngit checkout HEAD \u0026lt;file\u0026gt; Revert a commit (by producing a new commit with contrary changes)\ngit revert \u0026lt;commit\u0026gt; Reset your HEAD pointer to a previous commit, and discard all changes since then\ngit reset --hard \u0026lt;commit\u0026gt; \u0026hellip;and preserve all changes as upstaged changes\ngit reset \u0026lt;commit\u0026gt; \u0026hellip;and preserve uncommitted local changes\ngit reset --keep \u0026lt;commit\u0026gt; Commit History # Show all commits, starting with newest\ngit log Show changes over time for a specific file\ngit log -p \u0026lt;file\u0026gt; Who changed what and when in \u0026lt;file\u0026gt;\ngit blame \u0026lt;file\u0026gt; Git plugins # Store credential in manager\ngit config credential.helper \u0026#39;store\u0026#39; Activate gpg signature\ngit config commit.gpgsign true Reference # https://www.git-tower.com/blog/git-cheat-sheet/ ","date":"18 September 2016","externalUrl":null,"permalink":"/2016/09/git-cheat-sheet/","section":"Posts","summary":"","title":"Git cheat sheet","type":"posts"},{"content":"","date":"18 September 2016","externalUrl":null,"permalink":"/tags/encryption/","section":"Tags","summary":"","title":"Encryption","type":"tags"},{"content":"","date":"18 September 2016","externalUrl":null,"permalink":"/tags/keybase/","section":"Tags","summary":"","title":"Keybase","type":"tags"},{"content":"Article about keybase.io\nKeybase # Some time back i used to have my PGP key registered on MIT pgp server http://pgp.mit.edu/. It\u0026rsquo;s one of the largest public directory for PGP keys, but it\u0026rsquo;s kind of old fashioned and attracts mostly the academic world.\nI\u0026rsquo;m not entirely sure how i come to find http://keybase.io but it seems a new interesting way to get PGP to the general audience.\nIt allows, as the slogan say:\nKeybase maps your identity to your public keys, and vice versa.\nIn order to be a truly trusted PKI the registration is done by invitation only which seems interesting in order to maintain some sort of quality assurance, but will make the evolution a lot slower.\nSo basically this allows you to associate your domain, twitter account, github account,etc with you keys.\nIt also has it\u0026rsquo;s own distributed filesystem KBFS.\nhttps://keybase.io/docs/kbfs/understanding_kbfs\nNow this, seems to me, like the most important factor on this solution. Because you can quickly access other people keys, trust them, and follow you know friends.\nI still have some invites left, in case you are interested drop me an email :)\nCheers, RR@keybase https://keybase.io/ruimsramos\n","date":"18 September 2016","externalUrl":null,"permalink":"/2016/09/keybase.io/","section":"Posts","summary":"","title":"keybase.io","type":"posts"},{"content":"","date":"18 September 2016","externalUrl":null,"permalink":"/tags/pgp/","section":"Tags","summary":"","title":"PGP","type":"tags"},{"content":"","date":"18 September 2016","externalUrl":null,"permalink":"/tags/pki/","section":"Tags","summary":"","title":"PKI","type":"tags"},{"content":"Started Github pages Blog entries\nStarted Github pages # I had several blogs in the past that i normally tend to ignore and stop writing, so this one will be a simple tech/personal blog with entries that i might find useful in the future to search quicker.\nAnd as any normal blog i would start by writing the procedure of deploying the blog itself.\nThe only way i would keep this thing updated is if i have very quick way to edit via terminal a Markdown file and do a command like post-it and that\u0026rsquo;s it.\nSo hexo seems like a good solution for me.\nGonna try this one out.\nSo first things first, hosting. I\u0026rsquo;m cheap bastard so i don\u0026rsquo;t have cool domain yet, and i just need some git repo for the blog.\nLets stick with GitHub Pages there git service is pretty awesome so lets try this one out.\nMental-Note-To-Self: Get some nice english corrector ;)\nSo i\u0026rsquo;ve just created a new repo in github and from the settings i\u0026rsquo;ve selected to build a standard GitHub Page.\ncloned the dam thing to my computer\ngit clone git@github.com:rramos/rramos.github.io.git\nInstalling Hexo # So first things first, let\u0026rsquo;s install the requirements.\nIt requires:\nNode Git Well, git i already have since a cloned the repo. Node must be installed, but since i\u0026rsquo;m using Ubuntu Xenial i need to install the legacy one.\nsudo apt install nodejs-legacy Now starting to install hexo via npm.\nsudo npm install -g hexo-cli sudo npm install hexo-deployer-git --save Ok that\u0026rsquo;s it.\nStart writing # So i\u0026rsquo;ve run the following command on the repo, this initiates the hexo page structure.\nhexo init I\u0026rsquo;ve added the deploy configuration in _config.yml\ndeploy: type: git repo: git@github.com:rramos/rramos.github.io.git branch: master I also downloaded a very simple theme, cas\u0026rsquo;m a simple guy.\ncd themes git clone https://github.com/lotabout/very-simple Thanks @lotabout for the theme by the way.\nAnd installed the theme requirements\ngit clone https://github.com/lotabout/very-simple themes/very-simple sudo npm install hexo-renderer-sass --save sudo npm install hexo-renderer-jade --save Writing # So \u0026hellip; now i suppose i can start creating blog entries, let\u0026rsquo;s start.\nhexo new post \u0026#34;Created rramos.github.io\u0026#34; I get the output where the md file is and start writing there.\nAfter i have made sure my ssh keys where registered on github i simply deployed with\nhexo deploy And voilá: https://rramos.github.io is up and running.\nFinal touches # So hexo only dumps the public part of the blog which makes sense to the git repo. But i want to include all the source and not have separated repositories for that.\nI\u0026rsquo;ve created a src dir and copy all the source data there, now i can edit directly that repo, let\u0026rsquo;s take that advantage and remove the default hello world page from the structure guess there might exist a command for that.\n$ hexo list post INFO Start processing Date Title Path Category Tags 2016-09-17 Created rramos.github.io _posts/Created-rramos-github-io.md 2016-09-18 Hello World _posts/hello-world.md There you are you bastard hello-world, lets get ride of you.\nhexo clean rm source/_posts/hello-world.md hexo generate Well guess that\u0026rsquo;s it. There is a lot to explore in hexo from what i can tell, need to check the documentation and understand the community envolvment. Also a quick way to add imagens and other objects and define the quick way to deploy.\nSeparate public from source # It turns out that have the source and public data in the same repo causes some issues on the updates. The best approach seems to have a separate repo for the source data and the official Github Pages or other hosting service with git for the public part.\nI\u0026rsquo;ve also added in this source repo a .gitignore with\npublic db.json .deploy_git So now i just have to edit MarkDown hexo generate and hexo deploy, seems quick enough let\u0026rsquo;s see if this time i can keep this updated.\nCheers, RR\n","date":"17 September 2016","externalUrl":null,"permalink":"/2016/09/created-rramos.github.io/","section":"Posts","summary":"","title":"Created rramos.github.io","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]